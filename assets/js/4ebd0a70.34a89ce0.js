"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[7166],{93490:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var n=s(85893),o=s(11151);const i={layout:"posts",title:"Using flow control to handle uncontrolled process loops",date:new Date("2024-07-25T00:00:00.000Z"),categories:["performance"],tags:["availability"],authors:"rodrigo"},r="Using flow control to handle uncontrolled process loops",a={permalink:"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2024-07-25-Using-flow-control-to-handle-uncontrolled-process-loops/index.md",source:"@site/blog/2024-07-25-Using-flow-control-to-handle-uncontrolled-process-loops/index.md",title:"Using flow control to handle uncontrolled process loops",description:"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).",date:"2024-07-25T00:00:00.000Z",formattedDate:"July 25, 2024",tags:[{label:"availability",permalink:"/zeebe-chaos/tags/availability"}],readingTime:5.16,hasTruncateMarker:!1,authors:[{name:"Rodrigo Lopes",title:"Associate Software Engineer @ Zeebe",url:"https://github.com/rodrigo-lourenco-lopes",imageURL:"https://github.com/rodrigo-lourenco-lopes.png",key:"rodrigo"}],frontMatter:{layout:"posts",title:"Using flow control to handle uncontrolled process loops",date:"2024-07-25T00:00:00.000Z",categories:["performance"],tags:["availability"],authors:"rodrigo"},unlisted:!1,prevItem:{title:"Using flow control to handle bottleneck on exporting",permalink:"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting"},nextItem:{title:"Reducing the job activation delay",permalink:"/zeebe-chaos/2024/01/19/Job-Activation-Latency"}},l={authorsImageUrls:[void 0]},c=[{value:"Mitigating the performance impacts of deployed loops:",id:"mitigating-the-performance-impacts-of-deployed-loops",level:2},{value:"Single loop processing:",id:"single-loop-processing",level:2},{value:"Expected results:",id:"expected-results",level:3},{value:"Actual",id:"actual",level:3},{value:"Dual loop processing:",id:"dual-loop-processing",level:2},{value:"Expected",id:"expected",level:2},{value:"Actual",id:"actual-1",level:2}];function p(e){const t={code:"code",em:"em",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",strong:"strong",...(0,o.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default)."}),"\n",(0,n.jsx)(t.p,{children:"Limiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog."}),"\n",(0,n.jsx)(t.p,{children:"In these experiments we will test what happens with the deployment of endless\nloops that result in high processing load, and how we can use the new\nflow control to keep the cluster stable."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"TL;DR;"})}),"\n",(0,n.jsx)(t.p,{children:"Enabling the write rate limiting can help mitigate the effects caused by\nprocess instances that contain uncontrolled loops by preventing building up an\nexcessive exporting backlog."}),"\n",(0,n.jsx)(t.h2,{id:"mitigating-the-performance-impacts-of-deployed-loops",children:"Mitigating the performance impacts of deployed loops:"}),"\n",(0,n.jsx)(t.p,{children:"When an uncontrolled loop is accidentally deployed this tends to use of\nmost of the\nprocessing resources of the partitions where instances are running."}),"\n",(0,n.jsx)(t.p,{children:"Such instances completely occupies its partition, starves other instances and results in slow response times."}),"\n",(0,n.jsx)(t.p,{children:"Usually, these problems should be addressed before other issues arise, such as full disk due to a large backlog of not exported records (max exporting speed tends to be slower than max processing speed)."}),"\n",(0,n.jsx)(t.p,{children:"Using the write rate limiter, we can slow down the processing speed and\ngive us more time to address the issue, while at the same time enabling us to reduce or maintain the backlog size and reduce risks of side effects."}),"\n",(0,n.jsx)(t.p,{children:"To reduce the rate write limit we will use the unified control endpoint and configure write limit to be significantly lower than the processing speed."}),"\n",(0,n.jsx)(t.p,{children:"To fetch the current configuration we can port forward to one of the zeebe pods and use the command:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-Shell",children:"GET /actuator/flowControl\n"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"original-configuration",src:s(1285).Z+"",width:"586",height:"532"})}),"\n",(0,n.jsx)(t.p,{children:"To configure the write rate limit we use the same endpoint:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'POST /actuator/flowControl\n{\n   "write": {\n        "enabled": true,\n        "limit": 3000,\n  }\n}\n'})}),"\n",(0,n.jsx)(t.p,{children:"For this experiment we will test the impact of write rate limits both in\nsingle loops and dual loops."}),"\n",(0,n.jsx)(t.h2,{id:"single-loop-processing",children:"Single loop processing:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"single-loop",src:s(14265).Z+"",width:"1090",height:"634"})}),"\n",(0,n.jsx)(t.p,{children:"This single-loop process will hoard the processing resources and never complete but will append to the processing queue only the next step in the process."}),"\n",(0,n.jsx)(t.p,{children:"This means that the number of records not processed will only grow if many other processes or requests are arriving at the same time, at a faster rate than the cluster can process."}),"\n",(0,n.jsx)(t.h3,{id:"expected-results",children:"Expected results:"}),"\n",(0,n.jsx)(t.p,{children:"When deploying a process instance with a single loop we should see the\nprocessing rate in the partition increases significantly."}),"\n",(0,n.jsx)(t.p,{children:"This can lead to processing speed to surpass the exporting speed, which\nresults in increase in the backlog of exported records."}),"\n",(0,n.jsx)(t.p,{children:"Using the rate write limits to restrict the processing speed enables us to\nreduce the backlog size and give more time for the user to fix the\nunderlying issues with the cluster."}),"\n",(0,n.jsx)(t.h3,{id:"actual",children:"Actual"}),"\n",(0,n.jsx)(t.p,{children:"By deploying a single loop model we can see that the processing and writing\nincreases in the same partition and stabilizes around 5 000, later at\naround 17:55 we apply the write rate limit of 3 000, and the processing\ngets limited accordingly."}),"\n",(0,n.jsx)(t.p,{children:"This leads to some of the requests being\nredirected to\nthe other partitions which cause the processing in these to increase."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"single-loop-processing-per-partition",src:s(59954).Z+"",width:"1999",height:"290"})}),"\n",(0,n.jsx)(t.p,{children:"When observing the backpressure, we can draw the same conclusions as from\nthe processing per partition graph, after the model gets deployed, we see an\nincrease in the backpressure to around 7% in the partition where the loop\ninstance was deployed."}),"\n",(0,n.jsx)(t.p,{children:"Once the limit gets set at around 17:55 the backpressure in this partition\nincreases even more, to around 22% with the backpressure in the other partitions also increasing significantly."}),"\n",(0,n.jsx)(t.p,{children:"This follows the expected results since with the limiting processing, the after partition will reject even more commands, which get redirected to the remaining partitions which also cause their load to increase and therefore their backpressure as well."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"single-loop-backpressure",src:s(2262).Z+"",width:"1972",height:"456"})}),"\n",(0,n.jsx)(t.p,{children:"Observing the exporting per partition panel we can see that the exporting also increases in the affected partition, and this gets reduced after the limit gets imposed."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"single-loop-exporting-per-partition",src:s(36339).Z+"",width:"1999",height:"293"})}),"\n",(0,n.jsx)(t.h2,{id:"dual-loop-processing",children:"Dual loop processing:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"double-loop",src:s(65160).Z+"",width:"976",height:"612"})}),"\n",(0,n.jsx)(t.p,{children:"On the other hand, this dual loop process during its run will always create more records than can be processed since it doubles in the last step."}),"\n",(0,n.jsx)(t.p,{children:"This will create a steady increase in records not processed even if no other processes or requests are competing for processing time."}),"\n",(0,n.jsx)(t.h2,{id:"expected",children:"Expected"}),"\n",(0,n.jsx)(t.p,{children:"When deploying a process instance with a dual loop we should expect to see\na rapid increase in the processing speed and also in the number of records\nnot processed."}),"\n",(0,n.jsx)(t.p,{children:"In this case, restricting the processing speed should not decrease the\nbacklog of processed records since on each run of the loop more records are\ncreated than\nprocessed."}),"\n",(0,n.jsx)(t.p,{children:"However, it should help us at least in reducing the pace of the increase in\nthe backlog and therefore give us more time to address the\nunderlying problem."}),"\n",(0,n.jsx)(t.h2,{id:"actual-1",children:"Actual"}),"\n",(0,n.jsx)(t.p,{children:"After deploying the dual loop we can see that the processing quickly jumps to its peak, at around 18:11 we configure the write rate limit at 3000."}),"\n",(0,n.jsx)(t.p,{children:"Unlike the previous experience here we can observe that the processing speed in the other partitions was already increasing before the configuration gets applied."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"dual-loop-processing-per-partition",src:s(50552).Z+"",width:"1999",height:"288"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"dual-loop-exporting-per-partition",src:s(97459).Z+"",width:"1999",height:"295"})}),"\n",(0,n.jsx)(t.p,{children:"Observing the backpressure we get the answer as to why the processing in the other partitions was already increasing before the configuration gets applied."}),"\n",(0,n.jsx)(t.p,{children:"The backpressure had already reached at 100% which means that the dual loop process by itself hoarded completely the processing resources of the partition."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"dual-loop-backpressure",src:s(64560).Z+"",width:"1966",height:"462"})}),"\n",(0,n.jsx)(t.p,{children:"Observing the number of records not processed we conclude as expected that\nlimiting the write rate cannot stop the records backlog from continuing to increase, but we can see that the slope of the curve is smaller after configuring the limit."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"dual-loop-records-not-exported",src:s(46221).Z+"",width:"1106",height:"462"})}),"\n",(0,n.jsx)(t.p,{children:"Overall the results match our expectations that the flow control configuration can be leveraged to give us more control of the cluster, which in the case of acting on deployed loop instances can give us more tools to address these issues."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.em,{children:"Footnote:\n(As of the latest release, it is no longer possible to deploy processes that\ncontain a straight-through processing loops such as the ones used in this\nexperience)."})})]})}function d(e={}){const{wrapper:t}={...(0,o.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},64560:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/dual-loop-backpressure-824800471f8562faea35e72be8d9be05.png"},97459:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/dual-loop-exporting-per-partition-65937890949ec823d82338a744ad7ffe.png"},46221:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/dual-loop-number-of-records-not-processed-7e4a14070299c6842e16c7496e6b99cd.png"},50552:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/dual-loop-processing-per-partition-89041e42051a2123a5c442c197ad08fe.png"},65160:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/dual-loop-629f9c9afa1f36b5bab529656d32cf42.png"},1285:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/original-configuration-568ece5f5f513848f06d772b821f7ba6.png"},2262:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/single-loop-backpressure-3a91001378b847e1184ccb2e1e36d13a.png"},36339:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/single-loop-exporting-per-partition-c72cfee7a14b98c2a7e45945c63f2e99.png"},59954:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/single-loop-processing-per-partition-5a64caa8af538a2e888591a0d00f7ab1.png"},14265:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/single-loop-2c6d288753c26789a55ac011536dcf6a.png"},11151:(e,t,s)=>{s.d(t,{Z:()=>a,a:()=>r});var n=s(67294);const o={},i=n.createContext(o);function r(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);