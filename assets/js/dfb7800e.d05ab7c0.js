"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[6753],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(n),h=o,m=d["".concat(s,".").concat(h)]||d[h]||u[h]||i;return n?a.createElement(m,r(r({ref:t},p),{},{components:n})):a.createElement(m,r({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:o,r[1]=l;for(var c=2;c<i;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},5763:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var a=n(7462),o=(n(7294),n(3905));const i={layout:"posts",title:"Deployment Distribution",date:new Date("2021-01-26T00:00:00.000Z"),categories:["chaos_experiment","broker","network"],tags:["availability","data"],authors:"zell"},r="Chaos Day Summary",l={permalink:"/zeebe-chaos/2021/01/26/deployments",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-01-26-deployments/index.md",source:"@site/blog/2021-01-26-deployments/index.md",title:"Deployment Distribution",description:"On this chaos day we wanted to experiment a bit with deployment's and there distribution.",date:"2021-01-26T00:00:00.000Z",formattedDate:"January 26, 2021",tags:[{label:"availability",permalink:"/zeebe-chaos/tags/availability"},{label:"data",permalink:"/zeebe-chaos/tags/data"}],readingTime:10.855,hasTruncateMarker:!0,authors:[{name:"Christopher Zell",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],frontMatter:{layout:"posts",title:"Deployment Distribution",date:"2021-01-26T00:00:00.000Z",categories:["chaos_experiment","broker","network"],tags:["availability","data"],authors:"zell"},prevItem:{title:"Automating Deployment Distribution Chaos Experiment",permalink:"/zeebe-chaos/2021/02/23/automate-deployments-dist"},nextItem:{title:"Network partitions",permalink:"/zeebe-chaos/2021/01/19/network-partition"}},s={authorsImageUrls:[void 0]},c=[{value:"Deployments",id:"deployments",level:2},{value:"Deployment Distribution",id:"deployment-distribution",level:3},{value:"Reasoning",id:"reasoning",level:4},{value:"Why isn&#39;t the gateway in charge of distributing the deployment?",id:"why-isnt-the-gateway-in-charge-of-distributing-the-deployment",level:5},{value:"Why the response isn&#39;t send after the distribution is done. Why it is build in an asynchronous way?",id:"why-the-response-isnt-send-after-the-distribution-is-done-why-it-is-build-in-an-asynchronous-way",level:5},{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Steady State",id:"steady-state",level:4},{value:"Chaos Injection (Method)",id:"chaos-injection-method",level:4},{value:"Result",id:"result",level:3},{value:"Further work",id:"further-work",level:4}],p={toc:c},d="wrapper";function u(e){let{components:t,...i}=e;return(0,o.kt)(d,(0,a.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"On this chaos day we wanted to experiment a bit with deployment's and there distribution."),(0,o.kt)("p",null,"We run a chaos experiment with deploying multiple workflows and disconnecting two leaders. We verified whether deployments are distributed later. The chaos experiment was successful and showed a bit how fault tolerant deployment distribution is. \ud83d\udcaa"),(0,o.kt)("h2",{id:"deployments"},"Deployments"),(0,o.kt)("p",null,"In order to continue here we need to explain how the workflow deployment and distribution actually works, if you know it then you can skip this section \ud83d\ude09."),(0,o.kt)("h3",{id:"deployment-distribution"},"Deployment Distribution"),(0,o.kt)("p",null,"If you deploy a workflow and you have multiple partitions, Zeebe needs to make sure that all partitions eventually have the same version of the deployment. This is done with via the deployment distribution."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"distribution",src:n(7886).Z,width:"1911",height:"982"})),(0,o.kt)("p",null,'On deploying a workflow via a client, like the java client, we send a deployment command to the gateway. The gateway sends the received deployment to the "deployment partition", which is partition one. The partition one is in charge of distributing the deployment. When the client receives a response for the deployment command, this means that the deployment is written/created on partition one. It doesn\'t mean that it is distributed to all other partitions. The distribution is done asynchronously. '),(0,o.kt)("p",null,"This can cause issues, if you want to create workflow instances immediately after the deployment. If you try to create a workflow instance on a partition which hasn't received the deployment yet, then this creation will fail. The gateway sends commands, like workflow instance creation, in a round-robin fashion and if you have multiple partitions, then the chance that you hit another partition is quite high."),(0,o.kt)("h4",{id:"reasoning"},"Reasoning"),(0,o.kt)("p",null,"You may ask why we build it like that. Let me explain this a bit more."),(0,o.kt)("h5",{id:"why-isnt-the-gateway-in-charge-of-distributing-the-deployment"},"Why isn't the gateway in charge of distributing the deployment?"),(0,o.kt)("p",null,"Because the gateway is stateless. If the gateway restarts it has no state it can replay, so it is not able to retry the deployment distributions. If the gateway failed during deployment distribution some partition might lose the deployment update. In the broker we replay the state, which means we can detect whether we distributed the deployment already to a certain partition if not we can retry it."),(0,o.kt)("h5",{id:"why-the-response-isnt-send-after-the-distribution-is-done-why-it-is-build-in-an-asynchronous-way"},"Why the response isn't send after the distribution is done. Why it is build in an asynchronous way?"),(0,o.kt)("p",null,"The simple answer would be, because it can take a long time until the deployment is distributed to all partitions. In a distributed system it is likely that a service fail, which means if one partition is not available the distribution is not complete. With the current approach you can already start creating instances at least at partition one and you can retry the requests if you get an rejection."),(0,o.kt)("p",null,"After the small excursion of how deployment distribution look like and why, we can start with our experiment to verify that it works as expected."),(0,o.kt)("h2",{id:"chaos-experiment"},"Chaos Experiment"),(0,o.kt)("p",null,"We have a standard setup of three nodes, three partitions and replication factor three."),(0,o.kt)("h3",{id:"expected"},"Expected"),(0,o.kt)("p",null,"We deploy multiple versions (1000+) of a deployment and assume that at some point all deployments are distributed on all partitions and that we are able to create a workflow instance with the latest version on all partitions. The system should remain stable during distributing the deployments. This can be seen as the steady state."),(0,o.kt)("p",null,"If we now disconnect a leader of a different partition (different from partition one) with the leader of partition one, then the deployments can't be distributed. If we try to create workflow instances on that partition we should receive rejection's. After we connect them again the deployments should be distributed and we should be able to create workflow instances on that specific partition."),(0,o.kt)("h3",{id:"actual"},"Actual"),(0,o.kt)("h4",{id:"steady-state"},"Steady State"),(0,o.kt)("p",null,"Following Java code was used to verify the steady state. In order to find out on which partition the workflow instances was created I used the ",(0,o.kt)("inlineCode",{parentName:"p"},"Protocol#decodePartitionId")," method, which is available in the zeebe protocol module. This functionality and the property of the key's was already quite useful in the past on doing chaos experiments."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},'import io.zeebe.client.ZeebeClient;\nimport io.zeebe.model.bpmn.Bpmn;\nimport io.zeebe.protocol.Protocol;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class ChaosDayMain {\n\n\n  private static final Logger LOG = LoggerFactory.getLogger(ChaosDayMain.class.getName());\n\n  public static void main(String[] args) {\n    final var zeebeClient = ZeebeClient.newClientBuilder().usePlaintext().gatewayAddress("localhost:26500").build();\n\n    final var topology = zeebeClient.newTopologyRequest().send().join();\n    LOG.info("{}", topology);\n\n    var lastVersion = 0;\n    for (int i = 0; i < 1_000; i++) {\n\n      final var modelInstance = Bpmn.createExecutableProcess("workflow").startEvent().endEvent().done();\n\n      final var workflow = zeebeClient.newDeployCommand()\n          .addWorkflowModel(modelInstance, "workflow").send().join();\n      lastVersion = workflow.getWorkflows().get(0).getVersion();\n    }\n    LOG.info("Last version deployed: {}", lastVersion);\n\n    final var partitions = new ArrayList<>(List.of(1, 2, 3));\n\n    while (!partitions.isEmpty()) {\n      try {\n        final var workflowInstanceEvent = zeebeClient.newCreateInstanceCommand()\n            .bpmnProcessId("workflow")\n            .version(lastVersion).send().join();\n        final var workflowInstanceKey = workflowInstanceEvent.getWorkflowInstanceKey();\n        final var partitionId = Protocol.decodePartitionId(workflowInstanceKey);\n\n        partitions.remove(Integer.valueOf(partitionId));\n        LOG.info("Created workflow instance on partition {}, {} partitions left ({}).", partitionId, partitions.size(), partitions);\n      } catch (Exception e) {\n        // retry\n        LOG.info("Failed to create workflow instance", e);\n      }\n\n    }\n  }\n}\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Small Note")," the line: ",(0,o.kt)("inlineCode",{parentName:"p"},'Bpmn.createExecutableProcess("workflow").startEvent().endEvent().done()')," will always create a new version of a workflow, since internally new id's are generated."),(0,o.kt)("p",null,"After running the code above we can see following output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"13:45:00.606 [] [main] INFO  ChaosDayMain - TopologyImpl{brokers=[BrokerInfoImpl{nodeId=0, host='zell-chaos-zeebe-0.zell-chaos-zeebe.zell-chaos.svc.cluster.local', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=LEADER, health=HEALTHY}]}, BrokerInfoImpl{nodeId=2, host='zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc.cluster.local', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=LEADER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=LEADER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=FOLLOWER, health=HEALTHY}]}, BrokerInfoImpl{nodeId=1, host='zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc.cluster.local', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=FOLLOWER, health=HEALTHY}]}], clusterSize=3, partitionsCount=3, replicationFactor=3, gatewayVersion=0.27.0-SNAPSHOT}\n13:46:04.384 [] [main] INFO  ChaosDayMain - Last version deployed: 6914\n13:46:04.434 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 2 partitions left ([2, 3]).\n13:46:04.505 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\n13:46:04.571 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 3, 0 partitions left ([]).\n")),(0,o.kt)("p",null,"As you can see in the version count I run it already multiple times :). With this we are able to verify our steady state, that the deployments are distributed to all partitions and that we are able to create workflow instances with the specific (last) version on all partitions."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Side note, I needed multiple runs because there was a leader change (of partition one) in between and I had to adjust the code etc. Needs to be investigated whether the deployment distribution caused that.")),(0,o.kt)("h4",{id:"chaos-injection-method"},"Chaos Injection (Method)"),(0,o.kt)("p",null,"The following topology we used to determined who to disconnect:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"Cluster size: 3\nPartitions count: 3\nReplication factor: 3\nGateway version: 0.27.0-SNAPSHOT\nBrokers:\n  Broker 0 - zell-chaos-zeebe-0.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\n    Version: 0.27.0-SNAPSHOT\n    Partition 1 : Leader, Healthy\n    Partition 2 : Leader, Healthy\n    Partition 3 : Follower, Healthy\n  Broker 1 - zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\n    Version: 0.27.0-SNAPSHOT\n    Partition 1 : Follower, Healthy\n    Partition 2 : Follower, Healthy\n    Partition 3 : Follower, Healthy\n  Broker 2 - zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\n    Version: 0.27.0-SNAPSHOT\n    Partition 1 : Follower, Healthy\n    Partition 2 : Follower, Healthy\n    Partition 3 : Leader, Healthy\n")),(0,o.kt)("p",null,"Based on the work of the last chaos days we are able to disconnect brokers easily."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'#!/bin/bash\nset -exuo pipefail\n\nsource utils.sh\n\npartition=1\nnamespace=$(getNamespace)\ngateway=$(getGateway)\n\n# determine leader for partition one\nindex=$(getIndexOfPodForPartitionInState "$partition" "LEADER")\nleader=$(getBroker "$index")\nleaderIp=$(kubectl get pod "$leader" -n "$namespace" --template="{{.status.podIP}}")\n\n# determine leader for partition three\n\nindex=$(getIndexOfPodForPartitionInState "3" "FOLLOWER")\nleaderTwo=$(getBroker "$index")\nleaderTwoIp=$(kubectl get pod "$leaderTwo" -n "$namespace" --template="{{.status.podIP}}")\n\n# To print the topology in the journal\nretryUntilSuccess kubectl exec "$gateway" -n "$namespace" -- zbctl status --insecure\n\n# we put all into one function because we need to make sure that even after preemption the \n# dependency is installed\nfunction disconnect() {\n toChangedPod="$1"\n targetIp="$2"\n\n # update to have access to ip\n kubectl exec -n "$namespace" "$toChangedPod" -- apt update\n kubectl exec -n "$namespace" "$toChangedPod" -- apt install -y iproute2\n kubectl exec "$toChangedPod" -n "$namespace" -- ip route add unreachable "$targetIp"\n\n}\n\nretryUntilSuccess disconnect "$leader" "$leaderTwoIp"\nretryUntilSuccess disconnect "$leaderTwo" "$leaderIp" \n\n')),(0,o.kt)("p",null,"We used partition three here, since the leader of partition one and two are the same node. After disconnecting and running the Java code from above, we get the following output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"14:11:56.655 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\n14:11:56.713 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\n14:11:56.777 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\nio.zeebe.client.api.command.ClientStatusException: Command 'CREATE' rejected with code 'NOT_FOUND': Expected to find workflow definition with process ID 'workflow' and version '8916', but none found\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    at ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Command 'CREATE' rejected with code 'NOT_FOUND': Expected to find workflow definition with process ID 'workflow' and version '8916', but none found\n    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\n    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    ... 1 more\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Command 'CREATE' rejected with code 'NOT_FOUND': Expected to find workflow definition with process ID 'workflow' and version '8916', but none found\n    at io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\n    at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n    at java.lang.Thread.run(Thread.java:834) ~[?:?]\n14:11:56.846 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\n14:11:56.907 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\n14:11:56.971 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\nio.zeebe.client.api.command.ClientStatusException: Command 'CREATE' rejected with code 'NOT_FOUND': Expected to find workflow definition with process ID 'workflow' and version '8916', but none found\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    at ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Command 'CREATE' rejected with code 'NOT_FOUND': Expected to find workflow definition with process ID 'workflow' and version '8916', but none found\n    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\n    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    ... 1 more\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Command 'CREATE' rejected with code 'NOT_FOUND': Expected to find workflow definition with process ID 'workflow' and version '8916', but none found\n    at io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\n    at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n    at java.lang.Thread.run(Thread.java:834) ~[?:?]\n")),(0,o.kt)("p",null,"Which is of course expected, since the deployment is not available at the third partition."),(0,o.kt)("p",null,"In the stackdriver we see also warnings regarding the deployment distribution:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"2021-01-26 14:11:49.439 CET\nFailed to push deployment to node 2 for partition 3\n2021-01-26 14:11:49.439 CET\nFailed to push deployment to node 2 for partition 3\n2021-01-26 14:11:49.439 CET\nFailed to push deployment to node 2 for partition 3\n")),(0,o.kt)("p",null,"After we connected the leaders again (deleting the ip route), we can see in the application log that the exception changed to deadline exceeded and at some point we are able to create a workflow instance on partition three."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"14:16:21.958 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\n14:16:22.020 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\n14:16:32.032 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\nio.zeebe.client.api.command.ClientStatusException: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    at ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\n    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\n    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\n    at io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\n    ... 1 more\nCaused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\n    at io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\n    at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n    at java.lang.Thread.run(Thread.java:834) ~[?:?]\n14:16:32.062 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\n14:16:32.125 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\n14:16:37.596 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 3, 0 partitions left ([]).\n")),(0,o.kt)("p",null,"We see the deadline exceeded, because the processor is not able to send the response's in time. The reason for that is probably because we have so many deployments to process, which have been pushed by the partition one."),(0,o.kt)("p",null,"The resource consumption around that time look ok. We can see that the memory spikes a bit, but it recovers later. On the CPU graph we can see when we connected the nodes again."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"res",src:n(7203).Z,width:"1833",height:"646"}),")"),(0,o.kt)("h3",{id:"result"},"Result"),(0,o.kt)("p",null,"The chaos experiment was successful. The deployment was distributed even after a network disconnect and we were able to create workflow instance of the latest version on all partitions at the end."),(0,o.kt)("p",null,"With this experiment we were able to show that the deployment distribution is fault tolerant in way that it can handle unavailability of other partitions. This means eventually all partitions will receive there deployment's and we are able to create workflow instances on these partitions. "),(0,o.kt)("h4",{id:"further-work"},"Further work"),(0,o.kt)("p",null,"Further possible experiments would be to restart the leader of partition one to see that even after restart we re-distribute the deployments. It is probably also interesting to see how the distribution behaves on more partitions than three."),(0,o.kt)("p",null,"During the experiment we have observed some leader changes. It needs to be investigated further, whether this was related to the deployments or something different. It is probably also interesting to see how it behaves with larger deployments, also resource consumption wise."))}u.isMDXComponent=!0},7886:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/distribution-472ccace62249d74b06e5d0514215dc7.png"},7203:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/res-bfc342434d3419d6a6338dba219cbaad.png"}}]);