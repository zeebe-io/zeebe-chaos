"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[3186],{43085:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>l,frontMatter:()=>i,metadata:()=>o,toc:()=>h});var n=s(85893),a=s(11151);const i={layout:"posts",title:"Job push overloading",date:new Date("2023-11-30T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},r="Chaos Day Summary",o={permalink:"/zeebe-chaos/2023/11/30/Job-push-overloading",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-11-30-Job-push-overloading/index.md",source:"@site/blog/2023-11-30-Job-push-overloading/index.md",title:"Job push overloading",description:"In today's chaos day we (Nicolas and I) want to verify how job push behaves and in general, the Zeebe system when we have slow workers.",date:"2023-11-30T00:00:00.000Z",formattedDate:"November 30, 2023",tags:[{label:"availability",permalink:"/zeebe-chaos/tags/availability"}],readingTime:5.585,hasTruncateMarker:!0,authors:[{name:"Christopher Kujawa",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],frontMatter:{layout:"posts",title:"Job push overloading",date:"2023-11-30T00:00:00.000Z",categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},unlisted:!1,prevItem:{title:"Job push resiliency",permalink:"/zeebe-chaos/2023/12/06/Job-Push-resiliency"},nextItem:{title:"Hot backups impact on processing",permalink:"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing"}},c={authorsImageUrls:[void 0]},h=[{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Second Chaos Experiment",id:"second-chaos-experiment",level:2},{value:"Expected",id:"expected-1",level:3},{value:"Actual",id:"actual-1",level:3},{value:"Third Chaos Experiment",id:"third-chaos-experiment",level:2},{value:"Expected",id:"expected-2",level:3},{value:"Actual",id:"actual-2",level:3},{value:"Several further experiments",id:"several-further-experiments",level:2},{value:"Experiment with worker impact",id:"experiment-with-worker-impact",level:2},{value:"Expected",id:"expected-3",level:3},{value:"Actual",id:"actual-3",level:3},{value:"Reasoning",id:"reasoning",level:3},{value:"Result",id:"result",level:2}];function d(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"In today's chaos day we (Nicolas and I) want to verify how job push behaves and in general, the Zeebe system when we have slow workers."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"TL;DR;"})," Right now it seems that even if we have a slow worker it doesn't impact the general system, and only affects the corresponding process instance, not other instances. We found no unexpected issues, everything performed pretty well."]}),"\n",(0,n.jsx)(t.h2,{id:"chaos-experiment",children:"Chaos Experiment"}),"\n",(0,n.jsx)(t.p,{children:"Firstly we want to verify that job push will not overload a worker or gateway when workers are slow."}),"\n",(0,n.jsx)(t.h3,{id:"expected",children:"Expected"}),"\n",(0,n.jsx)(t.p,{children:"We expect that if the workers are slowing down, the load is distributed to other workers (if available), and it is expected that the general performance (of the affected process instance) should be slowed down. We wouldn't expect any restarts/failures on the gateway or workers."}),"\n",(0,n.jsx)(t.h3,{id:"actual",children:"Actual"}),"\n",(0,n.jsxs)(t.p,{children:["We deployed a normal benchmark, with ",(0,n.jsx)(t.a,{href:"https://github.com/camunda/zeebe/blob/main/benchmarks/setup/default/values.yaml",children:"default configurations"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["We slowed the workers down, in the sense that we changed ",(0,n.jsx)(t.a,{href:"https://github.com/zeebe-io/benchmark-helm/blob/main/charts/zeebe-benchmark/templates/worker.yaml#L30",children:"the completionDelay to 1250 ms"})]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(88506).Z+"",width:"1249",height:"654"})}),"\n",(0,n.jsx)(t.p,{children:"The throughput is lower than normal, as expected."}),"\n",(0,n.jsx)(t.p,{children:"We see no significant increase in memory usage on the gateway, nor any outages because of this."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(69150).Z+"",width:"628",height:"307"})}),"\n",(0,n.jsx)(t.p,{children:"We see that a high amount of job pushes fail (due to capacity constraints now in the workers).\n!Jobs are yielded back to the engine."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(67085).Z+"",width:"1249",height:"344"})}),"\n",(0,n.jsxs)(t.p,{children:["So far so good, first experiment worked as expected ","\u2705"]}),"\n",(0,n.jsx)(t.h2,{id:"second-chaos-experiment",children:"Second Chaos Experiment"}),"\n",(0,n.jsx)(t.p,{children:"The normal scenario when something is slow is for a user to scale up. This is what we did in the next experiment, we scaled the workers to 10 replicas (from 3), to verify how the system behaves in this case."}),"\n",(0,n.jsxs)(t.p,{children:["Something to keep in mind when the completion delay is 1250ms, we ",(0,n.jsx)(t.a,{href:"https://github.com/camunda/zeebe/blob/7002d53a079c06ab3a94f5485f022681a41dc9ed/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L113",children:"multiply the activation timeout by 6 in our workers"}),". This means completionDelay: 1250 -> job timeout 7.5s"]}),"\n",(0,n.jsx)(t.h3,{id:"expected-1",children:"Expected"}),"\n",(0,n.jsx)(t.p,{children:"We expect that we can reach a higher throughput."}),"\n",(0,n.jsx)(t.h3,{id:"actual-1",children:"Actual"}),"\n",(0,n.jsx)(t.p,{children:"Scaling the workers to 10:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-sh",children:"k scale deployment worker --replicas=10\n"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(28401).Z+"",width:"1240",height:"647"})}),"\n",(0,n.jsx)(t.p,{children:"We can see that after scaling we can complete more jobs."}),"\n",(0,n.jsxs)(t.p,{children:["The gateway memory seems to be not really affected.\n",(0,n.jsx)(t.img,{src:s(6906).Z+"",width:"633",height:"307"})]}),"\n",(0,n.jsx)(t.p,{children:"In the job push metrics we see less job push failures."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(34509).Z+"",width:"1247",height:"638"})}),"\n",(0,n.jsx)(t.p,{children:"When we check the written records we can see a decrease in yield, but an increase in timeouts. The reason is that we have to try several workers before giving it back."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.img,{src:s(86126).Z+"",width:"1254",height:"342"}),"\n",(0,n.jsx)(t.img,{src:s(31347).Z+"",width:"1244",height:"344"})]}),"\n",(0,n.jsxs)(t.p,{children:["Experiment two worked as expected. ","\u2705"]}),"\n",(0,n.jsx)(t.h2,{id:"third-chaos-experiment",children:"Third Chaos Experiment"}),"\n",(0,n.jsx)(t.p,{children:"In a real-world scenario, it will not happen if you have a slow dependency, for which for example a worker waits that you can scale and this will solve your problems. Likely you will get even slower because more pressure is put on the dependency. To mimic this scenario we experimented with increasing the completion time again. A completion delay set to 2500ms, means a job timeout of 15s."}),"\n",(0,n.jsx)(t.h3,{id:"expected-2",children:"Expected"}),"\n",(0,n.jsx)(t.p,{children:"We expect after slowing down all workers again, that our throughput goes down again, but we should see no general error. Potentially a slight memory increase because of buffering of jobs."}),"\n",(0,n.jsx)(t.p,{children:"This also means more yields and fewer timeouts."}),"\n",(0,n.jsx)(t.h3,{id:"actual-2",children:"Actual"}),"\n",(0,n.jsx)(t.p,{children:"As expected again we see a drop in throughput, but it is still a bit higher than at the first experiment."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(60348).Z+"",width:"1253",height:"648"})}),"\n",(0,n.jsx)(t.p,{children:"No difference at all in the memory consumption, by the gateway."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(25366).Z+"",width:"628",height:"306"})}),"\n",(0,n.jsx)(t.p,{children:"In the records we can also again see that yield increase, and timeouts have been decreased."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(355).Z+"",width:"1257",height:"686"})}),"\n",(0,n.jsxs)(t.p,{children:["Experiment three worked as expected. ","\u2705"]}),"\n",(0,n.jsx)(t.h2,{id:"several-further-experiments",children:"Several further experiments"}),"\n",(0,n.jsx)(t.p,{children:"We did several further experiments where we scaled the workers, played with the completion delay, reduced the starter load etc. At some point, we reached a state size that was too big (~2 Gig) such that this impacted our processing. We had to drain the cluster and stop the starters completely."}),"\n",(0,n.jsx)(t.p,{children:"Interestingly was that when we reduced the completion delay, we just had a slight increase in completion, when we scaled down the workers (marked with the annotation in the graph), to reduce activations, we saw no difference."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(62504).Z+"",width:"1258",height:"646"})}),"\n",(0,n.jsx)(t.p,{children:"Only when we hit a certain threshold in RocksDb (it seems to be at least), the completion went up by a lot."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(65308).Z+"",width:"615",height:"304"})}),"\n",(0,n.jsx)(t.p,{children:"This is because the record processing latency was heavily reduced (likely the commit latency or iteration latency in RocksDb)."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(80838).Z+"",width:"1259",height:"276"})}),"\n",(0,n.jsx)(t.h2,{id:"experiment-with-worker-impact",children:"Experiment with worker impact"}),"\n",(0,n.jsx)(t.p,{children:"We wanted to understand and experiment with the impact of a slow worker on different process instances."}),"\n",(0,n.jsx)(t.p,{children:"To see such an impact in our metrics we had to patch our current execution metrics, such that includes the BPMN processId, so we can differentiate between execution times of different processes."}),"\n",(0,n.jsxs)(t.p,{children:["See the related branch for more details ",(0,n.jsx)(t.a,{href:"https://github.com/camunda/zeebe/tree/ck-latency-metrics",children:"ck-latency-metrics"})]}),"\n",(0,n.jsxs)(t.p,{children:["Furthermore, a new process model was added ",(0,n.jsx)(t.code,{children:"slow-task.bpm"})," and new deployments to create such instances and work on them. The process model was similar to the benchmark model, only the job type has been changed."]}),"\n",(0,n.jsx)(t.h3,{id:"expected-3",children:"Expected"}),"\n",(0,n.jsx)(t.p,{children:"To verify was that whether a slow worker would impact other instances, this was uncertain territory we were hitting."}),"\n",(0,n.jsx)(t.p,{children:"To be honest we expected it would affect them."}),"\n",(0,n.jsx)(t.h3,{id:"actual-3",children:"Actual"}),"\n",(0,n.jsx)(t.p,{children:"We started the benchmark (default configs for broker and gateway), with additional configurations:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"benchmark starter with 75 PI/s rate"}),"\n",(0,n.jsx)(t.li,{children:"3 benchmark worker (60 capacity) and completion delay of 50 ms"}),"\n",(0,n.jsx)(t.li,{children:"slow-task starter with 75 PI/s rate"}),"\n",(0,n.jsx)(t.li,{children:"3 slow-worker (60 capacity) and a completion delay of 50 ms (at the begin)"}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(2490).Z+"",width:"2539",height:"871"})}),"\n",(0,n.jsx)(t.p,{children:"We can see based on the metrics that the execution latency is the same for both process instances, and we are able to complete our 150 PI/s."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(49313).Z+"",width:"1894",height:"644"})}),"\n",(0,n.jsxs)(t.p,{children:["We slowed now the worker for the type ",(0,n.jsx)(t.code,{children:"slow-task"})," down to a completion delay of 2500ms."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(73081).Z+"",width:"1694",height:"682"})}),"\n",(0,n.jsxs)(t.p,{children:["We can see that we start to get ",(0,n.jsx)(t.code,{children:"Job.YIELD"})," commands from the gateway, and we can see that the process instance execution is slowed down."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:s(99636).Z+"",width:"2544",height:"875"})}),"\n",(0,n.jsx)(t.p,{children:"Interestingly that is only for the affected process instance, which we wanted to validate/verify."}),"\n",(0,n.jsx)(t.h3,{id:"reasoning",children:"Reasoning"}),"\n",(0,n.jsx)(t.p,{children:"Our first assumption was that both instance latencies would be impacted, because are writing YIELD commands, instead of being able to complete them."}),"\n",(0,n.jsx)(t.p,{children:"But another consequence comes into play. If fewer jobs are worked on, there are also fewer jobs completed, this means fewer process instances have to be continued (with batch processing until the end)."}),"\n",(0,n.jsx)(t.p,{children:"This means a load of yield underweights the normal load of job completions, with additional process instance continuation. That was an interesting insight for us."}),"\n",(0,n.jsx)(t.h2,{id:"result",children:"Result"}),"\n",(0,n.jsx)(t.p,{children:"Right now it seems that even if we have a slow worker it doesn't impact badly the general system, and only affects the corresponding process instance, not other instances."}),"\n",(0,n.jsx)(t.p,{children:"What we should or need to investigate further what if the job completion delay is much larger than the timeout. This is something we might want to test soon."})]})}function l(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},88506:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp1-general-de6a6f7f1d6b52ae218f3c55ae8b33ce.png"},69150:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp1-gw-memory-220a9ab49a649d351bb58040bde2c4a0.png"},67085:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp1-records-26fd0549438faf9475280e80678bc2ce.png"},28401:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp2-general-fcdc544e61dee39561dd634ff2259c8e.png"},6906:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp2-gw-mem-1c710a1cc021fd9a369efee7871a6eef.png"},34509:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp2-job-push-56d12fe3469c8287fdc4b6fca3d7175f.png"},86126:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp2-records-37ead57c1e5b9bf03bb812ee7d58abf0.png"},31347:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp2-records2-b3733ab69b894a659d211e164c939e9e.png"},60348:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp3-general-88403398292f5c5cdbac3a1b9b8ee63d.png"},25366:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp3-memory-769d10a88790ea58ca00a995effcfa7c.png"},355:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp3-records-bf6c88f09631f951d4d272c346ee7768.png"},62504:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp6-general-f35222d6e1d66526ef6eb07c53f10923.png"},80838:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp6-latency-0b30edf17db48cf5d056bafe677cced4.png"},65308:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/exp6-state-f33212be78503a223b0741b9d7a340e1.png"},49313:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/slow-exp-base-general-25e62c9332f1ecad2df0cb8253bb188d.png"},2490:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/slow-exp-base-ea8adfd10aa658d81ca76a4c342f5df7.png"},73081:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/slow-exp-slow-worker-2500ms-records-388a523821cc584224d95d43b30c8c6d.png"},99636:(e,t,s)=>{s.d(t,{Z:()=>n});const n=s.p+"assets/images/slow-exp-slow-worker-2500ms-9c93bc66fc1f410535f45a170ba6b96a.png"},11151:(e,t,s)=>{s.d(t,{Z:()=>o,a:()=>r});var n=s(67294);const a={},i=n.createContext(a);function r(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);