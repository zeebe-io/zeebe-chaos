"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[9227],{78422:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>r,toc:()=>h});var n=a(85893),i=a(11151);const s={layout:"posts",title:"Multiple Leader Changes",date:new Date("2020-10-13T00:00:00.000Z"),categories:["chaos_experiment","broker"],authors:"zell"},o="Chaos Day Summary",r={permalink:"/zeebe-chaos/2020/10/13/multiple-leader-changes",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-10-13-multiple-leader-changes/index.md",source:"@site/blog/2020-10-13-multiple-leader-changes/index.md",title:"Multiple Leader Changes",description:"Today I wanted to add new chaostoolkit experiment, which we can automate.",date:"2020-10-13T00:00:00.000Z",formattedDate:"October 13, 2020",tags:[],readingTime:3.39,hasTruncateMarker:!0,authors:[{name:"Christopher Kujawa",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",imageURL:"https://github.com/zelldon.png",key:"zell"}],frontMatter:{layout:"posts",title:"Multiple Leader Changes",date:"2020-10-13T00:00:00.000Z",categories:["chaos_experiment","broker"],authors:"zell"},unlisted:!1,prevItem:{title:"Gateway memory consumption",permalink:"/zeebe-chaos/2020/10/27/standalone-gw-memory"},nextItem:{title:"Play around with ToxiProxy",permalink:"/zeebe-chaos/2020/10/06/toxi-proxy"}},l={authorsImageUrls:[void 0]},h=[{value:"Chaos Experiment: Multiple Leader Elections",id:"chaos-experiment-multiple-leader-elections",level:2},{value:"Steady State",id:"steady-state",level:3},{value:"Hypothesis",id:"hypothesis",level:3},{value:"Method",id:"method",level:3},{value:"Result",id:"result",level:3},{value:"Metrics",id:"metrics",level:4},{value:"Chaostoolkit",id:"chaostoolkit",level:4},{value:"Chaos Experiment: High Load",id:"chaos-experiment-high-load",level:2},{value:"Steady State",id:"steady-state-1",level:3},{value:"Hypothesis",id:"hypothesis-1",level:3},{value:"Method",id:"method-1",level:3},{value:"Result",id:"result-1",level:3},{value:"Metrics",id:"metrics-1",level:4},{value:"Participants",id:"participants",level:2}];function d(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(t.p,{children:["Today I wanted to add new chaostoolkit experiment, which we can automate.\nWe already have experiments like restarting followers and leaders for a partition, but in the past what also caused issues was multiple restarts/leader changes\nin a short period of time. This is the reason why I created ",(0,n.jsx)(t.a,{href:"https://github.com/zeebe-io/zeebe-chaos/issues/39",children:"#39"}),"."]}),"\n",(0,n.jsx)(t.h2,{id:"chaos-experiment-multiple-leader-elections",children:"Chaos Experiment: Multiple Leader Elections"}),"\n",(0,n.jsx)(t.p,{children:"In order to reduce the blast radius I setup an new Zeebe cluster with one partition (clusterplan: production-s). This makes it possible that we exactly restart the leader for that one partition.\nLater we can also try it out with multiple partitions. In our automated environment it is anyway executed with multiple partitions."}),"\n",(0,n.jsx)(t.h3,{id:"steady-state",children:"Steady State"}),"\n",(0,n.jsx)(t.p,{children:"All Brokers are ready and we are able to create new workflow instances on the partition one."}),"\n",(0,n.jsx)(t.h3,{id:"hypothesis",children:"Hypothesis"}),"\n",(0,n.jsx)(t.p,{children:"Even if we cause multiple leader changes due to broker restarts we should still be able to start new workflow instances on the corresponding partition."}),"\n",(0,n.jsx)(t.h3,{id:"method",children:"Method"}),"\n",(0,n.jsx)(t.p,{children:"We requesting the Topology, determine the leader for partition one restart that corresponding node and wait until it is up again. We repeat that multiple times (three times)."}),"\n",(0,n.jsx)(t.h3,{id:"result",children:"Result"}),"\n",(0,n.jsxs)(t.p,{children:["The corresponding experiment was added via this ",(0,n.jsx)(t.a,{href:"https://github.com/zeebe-io/zeebe-chaos/commit/11c3a96fc87991f649fb1559363ba335b2bf42a1",children:"commit"}),".\nWe were able to prove that our hypothesis is true. we are able to handle multiple leader changes even in a short period of time."]}),"\n",(0,n.jsx)(t.h4,{id:"metrics",children:"Metrics"}),"\n",(0,n.jsx)(t.p,{children:"In the metrics we can see the behavior during the experiment and also we can see that it becomes healthy at the end."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"general.png",src:a(60883).Z+"",width:"1835",height:"648"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"atomix.png",src:a(96999).Z+"",width:"1823",height:"615"})}),"\n",(0,n.jsx)(t.p,{children:"I also run this with a cluster plan M cluster with the same results:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"multiple.png",src:a(63610).Z+"",width:"1821",height:"842"})}),"\n",(0,n.jsx)(t.h4,{id:"chaostoolkit",children:"Chaostoolkit"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"(chaostk) [zell kubernetes/ ns:4ac065c1-a67e-4f47-8782-38a10d67515d-zeebe]$ chaos run multiple-leader-restart/experiment.json \n[2020-10-13 14:01:30 INFO] Validating the experiment's syntax\n[2020-10-13 14:01:30 INFO] Experiment looks valid\n[2020-10-13 14:01:30 INFO] Running experiment: Zeebe Leader restart multiple times experiment\n[2020-10-13 14:01:30 INFO] Steady-state strategy: default\n[2020-10-13 14:01:30 INFO] Rollbacks strategy: default\n[2020-10-13 14:01:30 INFO] Steady state hypothesis: Zeebe is alive\n[2020-10-13 14:01:30 INFO] Probe: All pods should be ready\n[2020-10-13 14:01:30 INFO] Probe: Should be able to create workflow instances on partition one\n[2020-10-13 14:01:32 INFO] Steady state hypothesis is met!\n[2020-10-13 14:01:32 INFO] Playing your experiment's method now...\n[2020-10-13 14:01:32 INFO] Action: Terminate leader of partition one\n[2020-10-13 14:01:42 INFO] Pausing after activity for 5s...\n[2020-10-13 14:01:47 INFO] Probe: All pods should be ready\n[2020-10-13 14:02:32 INFO] Action: Terminate leader of partition one\n[2020-10-13 14:02:41 INFO] Pausing after activity for 5s...\n[2020-10-13 14:02:46 INFO] Probe: All pods should be ready\n[2020-10-13 14:03:23 INFO] Action: Terminate leader of partition one\n[2020-10-13 14:03:33 INFO] Pausing after activity for 5s...\n[2020-10-13 14:03:38 INFO] Steady state hypothesis: Zeebe is alive\n[2020-10-13 14:03:38 INFO] Probe: All pods should be ready\n[2020-10-13 14:04:12 INFO] Probe: Should be able to create workflow instances on partition one\n[2020-10-13 14:04:16 INFO] Steady state hypothesis is met!\n[2020-10-13 14:04:16 INFO] Let's rollback...\n[2020-10-13 14:04:16 INFO] No declared rollbacks, let's move on.\n[2020-10-13 14:04:16 INFO] Experiment ended with status: completed\n"})}),"\n",(0,n.jsx)(t.h2,{id:"chaos-experiment-high-load",children:"Chaos Experiment: High Load"}),"\n",(0,n.jsx)(t.p,{children:"As mentioned last week @pihme has reported voluntarily that he wants to implement another chaos experiment.\nHe worked on #7, where we expect that even we put high load on the Zeebe cluster we will cause no leader changes. This was in the past an failure case, where high load disrupted the cluster."}),"\n",(0,n.jsx)(t.h3,{id:"steady-state-1",children:"Steady State"}),"\n",(0,n.jsx)(t.p,{children:"All Brokers are ready and we can create workflow instances on all partitions. We store the begining topology for later."}),"\n",(0,n.jsx)(t.h3,{id:"hypothesis-1",children:"Hypothesis"}),"\n",(0,n.jsx)(t.p,{children:"We expect that even on high load we are not able to disrupt the cluster and that this will not cause any leader changes."}),"\n",(0,n.jsx)(t.h3,{id:"method-1",children:"Method"}),"\n",(0,n.jsx)(t.p,{children:"Put high load on the cluster for several minutes, via creating workflow instances"}),"\n",(0,n.jsx)(t.h3,{id:"result-1",children:"Result"}),"\n",(0,n.jsxs)(t.p,{children:["@pihme create a new PR to add the experiment ",(0,n.jsx)(t.a,{href:"https://github.com/zeebe-io/zeebe-chaos/pull/41",children:"#41"})]}),"\n",(0,n.jsx)(t.h4,{id:"metrics-1",children:"Metrics"}),"\n",(0,n.jsx)(t.p,{children:"We see that we already put some load on the cluster but it is not enough to exhaust the request limits and reach back pressure."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"highload",src:a(41610).Z+"",width:"1828",height:"656"})}),"\n",(0,n.jsx)(t.p,{children:"We neeed to find a good way how put high load on the Zeebe cluster. We will continue on this."}),"\n",(0,n.jsx)(t.h2,{id:"participants",children:"Participants"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"@pihme"}),"\n",(0,n.jsx)(t.li,{children:"@zelldon"}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},96999:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/atomix-21a9647bff5ea46a48d314b015a95687.png"},60883:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/general-c5d51de85e74d262744025b00dcbb6f8.png"},41610:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/highload-d7d6a7b22d9314ca056bb4b669dfe8fd.png"},63610:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/multiple-6ca96f4dfef92ecc711e92eb2f739444.png"},11151:(e,t,a)=>{a.d(t,{Z:()=>r,a:()=>o});var n=a(67294);const i={},s=n.createContext(i);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);