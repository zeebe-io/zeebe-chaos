"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[9696],{74881:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2024/01/19/Job-Activation-Latency","metadata":{"permalink":"/zeebe-chaos/2024/01/19/Job-Activation-Latency","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2024-01-19-Job-Activation-Latency/index.md","source":"@site/blog/2024-01-19-Job-Activation-Latency/index.md","title":"","description":"With the addition of end-to-end job streaming capabilities in Zeebe, we wanted to measure the improvements in job activation latency:","date":"2024-01-19T00:00:00.000Z","formattedDate":"January 19, 2024","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":11.045,"hasTruncateMarker":true,"authors":[{"name":"Nicolas Pepin-Perreault","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/npepinpe","imageURL":"https://github.com/npepinpe.png","key":"nicolas"}],"frontMatter":{"layout":"posts","title":"","date":"2024-01-19T00:00:00.000Z","categories":["performance","bpmn"],"tags":["availability"],"authors":"nicolas"},"unlisted":false,"nextItem":{"title":"Broker Scaling and Performance","permalink":"/zeebe-chaos/2023/12/20/Broker-scaling-performance"}},"content":"With the addition of end-to-end job streaming capabilities in Zeebe, we wanted to measure the improvements in job activation latency:\\n\\n- How much is a single job activation latency reduced?\\n- How much is the activation latency reduced between each task of the same process instance?\\n- How much is the activation latency reduced on large clusters with a high broker and partition count?\\n\\nAdditionally, we wanted to guarantee that every component involved in streaming, including clients, would remain resilient in the face of load surges.\\n\\n**TL;DR;** Job activation latency is greatly reduced, with task based workloads seeing up to 50% reduced overall execution latency. Completing a task now immediately triggers pushing out the next one, meaning the latency to activate the next task in a sequence is bounded by how much time it takes to process its completion in Zeebe. Activation latency is unaffected by how many partitions or brokers there in a cluster, as opposed to job polling, thus ensuring scalability of the system. Finally, reuse of gRPC\'s flow control mechanism ensure clients cannot be overloaded even in the face of load surges, without impacting other workloads in the cluster.\\n\\n[Head over to the documentation to learn how to start using job push!](https://docs.camunda.io/docs/components/concepts/job-workers/#job-streaming)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why job activation latency matters\\n\\nJobs are one of the fundamental building blocks of Zeebe, representing primarily all tasks (e.g. service, send, user), as well as some less obvious symbols (e.g. intermediate message throw event). In essence, they represent the actual unit of work in a process, the part users will implement, i.e. the actual application code. To reduce the likelihood of a job being worked on by multiple clients at the same time, it first goes through an activation process, where it is soft-locked for a specific amount of time. Soft-locked here means anyone can still interact with it - they can complete the job, fail it, etc. Only the activation is locked out, meaning no one else can activate the job until it\'s timed out.\\n\\nThis means that most workloads will consist mostly of job interactions: creation, activation, completion, etc. As such, it\'s critical to ensure clients receive jobs as fast as possible in order to make progress.\\n\\n\\n## Polling: a first implementation\\n\\nBack in 2018, Zeebe introduced the `ActivateJobs` RPC for its gRPC clients, analogous to fetching and locking [external tasks in Camunda 7.x](https://docs.camunda.org/manual/7.20/user-guide/process-engine/external-tasks/). This endpoint allowed clients to activate fetch and activate a specific number of available jobs. In other words, it allowed them to _poll_ for jobs.\\n\\nThis was the first implementation to activate and work on jobs in Zeebe for multiple reason:\\n\\n- It follows a simple request/response pattern\\n- Flow control is delegated to the client/user\\n- Most other approaches will build onto the building blocks used by polling\\n- You will likely implement polling anyway as a fallback for other approaches (e.g. pushing)\\n\\nGrossly simplified, the implementation worked like this:\\n\\n![Job polling](./job-poll.png)\\n\\n- A client initiates an `ActivateJobs` call by sending an initial request\\n- The gateway receives the request and validates it\\n- The gateway starts polling each partition synchronously one by one\\n- Whenever jobs are received from a partition, it forwards them to the client\\n- When all partitions are exhausted, or the maximum number of jobs have been activated, the request is closed\\n\\nAlready we can infer certain performance bottle necks based on the following:\\n\\n- Every request - whether client to gateway, or gateway to broker - adds delay to the activation latency\\n- In the worst case scenario, we have to poll _every_ partition.\\n- The gateway does not know in advance which partitions have jobs available.\\n- Scaling out your clients may have adverse effects by sending out too many requests which all have to be processed independently\\n- [If you have a lot of jobs, you can run into major performance issues when accessing the set of available jobs](https://github.com/camunda/zeebe/issues/11813)\\n\\nSo if we have, say, 30 partitions, and each gateway-to-broker request takes 100ms, fetching the jobs on the last partition will take up to 3 seconds, even though the actual activation time on that partition was only 100ms.\\n\\nFurthermore, if we have a sequence of tasks, fetching the next task in the sequence requires, in the worst case scenario, another complete round of polling through all the partitions, even though the task may already be available.\\n\\nOne would think a workaround to this issue would simply be to poll more often, but this can have an adverse impact: each polling request has to be processed by the brokers, and sending too many will simply flood your brokers and slow down all processing, further compounding the problem.\\n\\n### Long polling: a second implementation\\n\\nTo simplify things, the Zeebe team introduced [long polling in 2019](https://github.com/camunda/zeebe/issues/2825). [Long polling](https://en.wikipedia.org/wiki/Push_technology#Long_polling) is a fairly common technique to emulate a push or streaming approach while maintaing the request-response pattern of polling. Essentially, if the server has nothing to send to the client, instead of completing the request it will hold it until content is available, or a timeout is reached.\\n\\nIn Zeebe, this means that if we did not reach the maximum number of jobs to activate after polling all partitions, the request is parked but not closed. Eventually when jobs are available, the brokers will make this information known to the gateways, who will then unpark the oldest request and start a new polling round.\\n\\n![Job polling](./job-long-poll.png)\\n\\nThis solved certain problems:\\n\\n- We reduced the amount of requests sent by clients, thus reducing load on the cluster.\\n- In some cases, we reduced the latency when activating the next task in sequence.\\n\\nHowever, there are still some issues:\\n\\n- When receiving the notification we _still_ have to poll all partitions.\\n- If you have multiple gateways, all gateways will start polling if they have parked requests. Some of them may not get any jobs, but they will still have sent requests to brokers which still all have to be processed.\\n- In high load cases, you still need another client request/poll cycle to fetch the next task in a sequence.\\n- Scaling out your clients still add more load on the system, even if the poll less often\\n\\n## Job push: third time\'s the charm\\n\\nIn order to solve these issues, the team decided to implement [a push-based approach to job activation](https://github.com/camunda/zeebe/issues/11231).\\n\\nEssentially, we added a new `StreamActivatedJobs` RPC to our gRPC protocol, a so-called [server streaming RPC](https://grpc.io/docs/what-is-grpc/core-concepts/#server-streaming-rpc). In our case, this is meant to be a long-lived stream, such that the call is completed only if the client terminates it, or if the server is shutting down.\\n\\nThe stream itself has the following lifecycle:\\n\\n![Job push](./job-push.png)\\n\\n- The client initiates the stream by sending a job activation request much like with the `ActivateJobs` RPC.\\n  - Since the stream is meant to be long lived, however, there is no upper bound on the number of jobs to activate.\\n- The gateway registers the new stream with all brokers in the cluster\\n  - Note that there is no direct connection between brokers and client; the gateway acts as a proxy for the client.\\n- When jobs are available for activation (e.g. on creation, on timeout, on backoff, etc.), the broker activates the job and pushes it to the gateway.\\n- The gateway forwards the job to the client.\\n\\n[You can read more about the implementation as part of our docs.](https://docs.camunda.io/docs/components/concepts/job-workers/#how-it-works)\\n\\n> Experienced readers will immediately spot that push-based approaches run the risk of overloading the client. Thanks to the built-in flow control facilities of gRPC, we can still ensure clients are resilient in the face of load surges. See [here for an explanation](https://docs.camunda.io/docs/components/concepts/job-workers/#backpressure).\\n\\nThis solved most, if not all, of the problems listed above:\\n\\n- Brokers push jobs out immediately as they become available, removing the need for a gateway-to-broker request.\\n- Since the stream is long lived, there are almost no client requests required after the initial one.\\n- No need to poll every partition anymore.\\n- No thundering herd issues if you have many gateways all polling at the same time due to a notification.\\n- Scaling out your clients adds little to no load to the system, as idle clients simply do nothing.\\n- Even if you have a lot of jobs, in the average case, you never have to iterate over them and instead the broker pushes the job out on creation.\\n\\n### Tests, results, and comparisons\\n\\nIn order to compare the advantages of pushing to polling, we did three different experiments. \\n\\n> Note that all throughput measurements are in process instances executed per second, shortened to PI/s. Additionally, in the results shown below, dotted lines in graphs always refer to job polling measurements, and filled lines to job pushing.\\n\\n#### Cluster specifications\\n\\nNote that, unless specificed otherwise, we used the following clusters to run the tests: 3 brokers, 2 gateways, 3 partitions, replication factor 3.\\n\\n##### Brokers\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| CPU request | 1350m |\\n| Memory request | 4Gi |\\n| CPU thread count | 3 |\\n| IO thread count | 3 |\\n| Disk type | [pd-ssd](https://cloud.google.com/compute/docs/disks#disk-types) |\\n| Disk size | 32Gi |\\n\\n> Disk type, size, and vCPU count in GCP is used to determine your maximum IOPS.\\n\\n##### Gateways\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| CPU request | 450m |\\n| Kubernetes memory request | 1Gi |\\n| Management thread count | 2 |\\n\\n##### Workers\\n\\nTo simulate work, whenever workers receive an activated job, they will wait 50ms before completing it.\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| CPU request | 500m |\\n| Kubernetes memory request | 256Mi |\\n| Thread count | 10 |\\n| Max jobs active | 60 |\\n\\n#### One task\\n\\nAs our baseline test, we ran a constant throughput of 150 PI/s of a single task process workload:\\n\\n![A single task BPMN process: start -> task -> end](./single-task-bpmn.png)\\n\\nSince each job takes at least 50ms of work, the lower bound execution latency for this process is 50ms.\\n\\n![Results of 150 PI/s single task process](./single-task-benchmark.png)\\n\\nThe results show a sharp decrease in both the p50 and p99 of the job lifetime (i.e. the time between creation and completion). Since this workload only consists of a single task, this is mirrored in the overall process execution latency. Overall, we see that switching to a push approach yields a p50 latency improvement of 50%, and a p99 improvement of 75%!\\n\\nAdditionally, we can see with job push that the Zeebe the p50 processing overhead is ~14ms, and the p99 ~390ms. For job polling, the p50 overhead is ~70ms, and the p99 overhead is ~1.7s.\\n\\n#### Ten tasks\\n\\nFor our next test, we ran a constant throughput of 150 PI/s of a ten tasks sequence process:\\n\\n![A ten tasks sequence BPMN process: start -> task_1 -> task_2 -> ... -> task_10 -> end](./ten-tasks-bpmn.png)\\n\\nSince each job takes at least 50ms of work, the lower bound execution latency for this process is 500ms.\\n\\n![Results of 150 PI/s single task process](./ten-tasks-benchmark.png)\\n\\nThe results show a sharp decrease in both the p50 and p99 of the job lifetime (i.e. the time between creation and completion). In this case, the process consists of several tasks, so the process execution latency is noticeably higher. But we can see that the p50 latency for job push is ~640ms. Overall, we see that switching to a push approach yields a p50 latency improvement of 30%, and a p99 improvement of 50%!\\n\\nAdditionally, we can see with job push that the Zeebe the p50 processing overhead is ~140ms, and the p99 ~1.8s. For job polling, the p50 overhead is ~1.4s, and the p99 overhead is ~4.3s.\\n\\n#### Large cluster\\n\\nIn order to verify that the approach will scale along with the cluster size, we next compared polling and pushing with a cluster of 30 brokers and 30 partitions. Again, we tested with the single task process as above, and a constant throughput of 150 PI/s.\\n\\n![Results of 150 PI/s against a large cluster](./thirty-partitions-benchmark.png)\\n\\nFor job push, we see a greatly improved p99 - since each partition is doing less work than before with 3 partitions, we can achieve much more stable performance, with the p99 being quite close to the p50. \\n\\nFor job poll however, we see the downside of having to poll each partition in turn: the p50 is worse than before, and even though the p99 is greatly improved, we can see a wave pattern where it will spike up to 3s, so a decrease compared to the smaller cluster.\\n\\n#### Client backpressure & load surges\\n\\nOne of the downsides of switching to a push approach, unfortunately, is that the client is now at risk of receiving more work than it can safely handle.\\n\\nThankfully, HTTP/2 and gRPC both have mechanisms to ensure flow control for server streaming RPCs.\\n\\n[You can find our tests results in a separate blog post](https://zeebe-io.github.io/zeebe-chaos/2023/11/30/Job-push-overloading).\\n\\n## Further reading\\n\\nYou can read more about job push here:\\n\\n- [Streaming job workers](https://docs.camunda.io/docs/components/concepts/job-workers/#job-streaming)\\n- [Job push for the Java client](https://docs.camunda.io/docs/apis-tools/java-client/job-worker/#job-streaming)\\n- [Job push for the Go client](https://docs.camunda.io/docs/apis-tools/go-client/job-worker/#job-streaming)\\n- [Job push for spring-zeebe](https://github.com/camunda-community-hub/spring-zeebe#enable-job-streaming)\\n\\nAdditionally, we\'ve already written two other blog posts:\\n\\n- [Client backpressure resilience](https://zeebe-io.github.io/zeebe-chaos/2023/11/30/Job-push-overloading)\\n- [Job stream fault tolerance](https://zeebe-io.github.io/zeebe-chaos/2023/12/06/Job-Push-resiliency)"},{"id":"/2023/12/20/Broker-scaling-performance","metadata":{"permalink":"/zeebe-chaos/2023/12/20/Broker-scaling-performance","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-12-20-Broker-scaling-performance/index.md","source":"@site/blog/2023-12-20-Broker-scaling-performance/index.md","title":"Broker Scaling and Performance","description":"With Zeebe now supporting the addition and removal of brokers to a running cluster, we wanted to test three things:","date":"2023-12-20T00:00:00.000Z","formattedDate":"December 20, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":5.9,"hasTruncateMarker":true,"authors":[{"name":"Ole Sch\xf6nburg","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/oleschoenburg","imageURL":"https://github.com/oleschoenburg.png","key":"ole"},{"name":"Deepthi Akkoorath","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/deepthidevaki","imageURL":"https://github.com/deepthidevaki.png","key":"deepthi"}],"frontMatter":{"layout":"posts","title":"Broker Scaling and Performance","date":"2023-12-20T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability","performance"],"authors":["ole","deepthi"]},"unlisted":false,"prevItem":{"title":"","permalink":"/zeebe-chaos/2024/01/19/Job-Activation-Latency"},"nextItem":{"title":"Dynamic Scaling with Dataloss","permalink":"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss"}},"content":"With Zeebe now supporting the addition and removal of brokers to a running cluster, we wanted to test three things:\\n1. Is there an impact on processing performance while scaling?\\n2. Is scaling resilient to high processing load?\\n3. Can scaling up improve processing performance?\\n\\n**TL;DR;** Scaling up works even under high load and has low impact on processing performance. After scaling is complete, processing performance improves in both throughput and latency.\\n\\n\x3c!--truncate--\x3e\\n\\n## Impact of scaling on processing performance\\n\\nScaling up and down is an expensive operation where partition data is transferred between brokers, and leadership for partitions changes.\\nWe wanted to test how much impact this has on regular processing performance.\\n\\nTo do this, we ran a benchmark with 3 brokers, 6 partitions and replication factor 3.\\n\\nThe brokers are limited to 1.35 CPUs and 4GiB RAM each. \\nThey run with additional safety checks that are usually disabled in production and that slightly decrease the baseline processing performance.\\nEach broker uses a small 32GiB SSD for storage, limiting them to a few thousand IOPS.\\n\\nThe processing load was 150 processes per second, with a large payload of 32KiB each.\\nEach process instance has a single service task:\\n\\n![](./one_task.png)\\n\\nThe processing load is generated by our own [benchmarking application](https://github.com/camunda/zeebe/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks).\\n\\n### Expected\\n\\nWhen we scale up from 3 to 6 brokers, we expect a small impact on processing performance.\\nRequest latency may increase slightly, some requests may time out and some will be rejected due to backpressure.\\nThe overall throughput in terms of created and completed process instances as well as jobs may similarly decrease slightly.\\n\\n### Actual\\n\\nFollowing are screenshots of collected metrics.\\nThe blue annotation marks the time where scaling occurred.\\n\\nWe see a short increase in process instance duration, meaning that some process instances were finished slightly slower than before.\\n\\n![](./increased_process_duration.png)\\n\\nThe throughput in terms of created and completed process instances and jobs remained very stable.\\n\\n![](./stable_throughput.png)\\n\\nWe see a small increase in requests timing out or getting rejected by backpressure.\\n\\n![](./failed_requests.png)\\n\\nOverall, this matches our expectation and shows that scaling up has a small impact on processing performance.\\n\\n## Scaling under load\\n\\nSince scaling up is supposed to alleviate high processing load for brokers, it\'s important that it works even under high load.\\nFor this test, we increased the load on the same cluster setup as before to 210 instead of 150 process instances per second.\\nThis is roughly the maximum throughput that the 3 brokers with 6 partitions and replication factor 3 can handle.\\nWe can see this from the relatively high backpressure, as well as high process instance duration.\\n\\n![](./high_load_backpressure.png)\\n![](./high_load_latency.png)\\n\\n### Expected\\n\\nWe expect that scaling up to 6 brokers will still complete successfully, even under high load.\\nThe time it takes until scaling is complete might be slightly higher.\\nThe impact on processing performance, both in terms of throughput and latency, may be slightly larger than in the previous experiment.\\n\\n### Actual\\n\\nThe process instance duration did not increase, and even decreased slightly.\\n![](./high_load_scaling_latency.png)\\n\\nSimilarly, the throughput in terms of created and completed process instances and jobs remained relatively stable.\\n![](./high_load_scaling_throughput.png)\\n\\nThe number of failed requests increased slightly, but still well within an acceptable range.\\n![](./high_load_scaling_failed_requests.png)\\n\\nThe scaling operation took 5 minutes, a good portion of which is waiting for the new brokers to get scheduled and start up.\\n\\nOverall, this matches our expectation and shows that scaling can complete fast and with low impact on processing performance, even under high load.\\n\\n## Scaling up to improve performance\\n\\nThe most obvious goal of scaling brokers is to unlock additional processing performance.\\nWhile vertical scaling is also a great option, this can hit limits imposed by your infrastructure provider.\\nFor example, some machine types may offer great CPU performance but are severely limited in IOPS.\\nAdditionally, vertical scaling is often more expensive than horizontal scaling.\\nIt also comes with increased risk when a single machine fails because the remaining machines may already run at their limits and will then struggle to handle the additional load during failover.\\n\\nTo show how broker scaling can improve processing performance, we reused the same cluster setup as before.\\nWe have 3 brokers, 6 partitions and replication factor 3.\\n\\nThe brokers are limited to 1.35 CPUs and 4GiB RAM each. \\nThey run with additional safety checks that are usually disabled in production and that slightly decrease the baseline processing performance.\\nEach broker uses a small 32GiB SSD for storage, limiting them to a few thousand IOPS.\\n\\nWe changed the processing load slightly to simulate a more realistic scenario.\\nThe new process model consists of 10 tasks with two timers in-between, each delaying the process instance by 1 minute.\\n\\n![](./ten_tasks.png)\\n\\nThe processing load is generated by our own [benchmarking application](https://github.com/camunda/zeebe/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks), initially starting 40 process instances per second.\\n\\nThis results in 400 jobs created and completed per second.\\n\\nThis stresses the 3 brokers and we see backpressure on all partitions.\\n![](./perf_initial_backpressure.png)\\n\\nWe also see a few jobs timing out, indicating that the cluster is unable to handle this load consistency:\\n![](./perf_initial_timeouts.png)\\n\\nWe also see that many jobs are active for much longer than 1 second, even though the workers only delay completion by 50ms.\\n![](./perf_initial_job_lifetime.png)\\n\\nAs hinted at before, much of this performance limit can be attributed to the limited IOPS of the small SSDs.\\nWe see this in a very high commit and write latency, while the IOPS remain stable, right at the limit of what the SSDs can handle.\\n\\n![](./perf_initial_iops.png)\\n![](./perf_initial_commit_latency.png)\\n![](./perf_initial_write_latency.png)\\n\\n### Expected\\n\\nWhen we scale up to 6 brokers, and thus distribute the partitions such that each broker is only leader for 1 instead of 2 partitions, we expect that processing performance improves.\\n\\nAs these things usually go, we don\'t expect a doubling in performance but aiming for a 1.5x improvement seems reasonable.\\n\\n### Actual\\n\\nShortly after scaling up and after partition leadership has balanced, we see a significant improvement in backpressure.\\n![](./perf_after_backpressure.png)\\n\\nThe job lifetime decreases dramatically, with most jobs now taking < 50ms from creation until completion.\\n![](./perf_after_job_lifetime.png)\\n\\nOverall processing latency improves similarly.\\n![](./perf_after_processing_latency.png)\\n\\nMuch of this improvement can be attributed to the reduced IOPS.\\n\\n![](./perf_after_iops.png)\\n\\nCommit and write latency improves accordingly.\\n![](./perf_after_commit_latency.png)\\n![](./perf_after_write_latency.png)\\n\\nAnother source for improved performance is reduced CPU load.\\nWith 3 brokers being leader for 2 partitions each, they were hitting their CPU limits and got throttled by the underlying infrastructure.\\nWith 6 brokers, each only being leader for 1 partition, the CPU load is reduced and brokers are no longer throttled.\\n![](./perf_after_cpu.png)\\n\\n\\nWhile this is already a success, we can push things further now.\\nWe are able to increase the load from 40 to 65 process instances per second, resulting in 650 jobs created and completed per second.\\nThis is a 1.6x improvement over the initial load while achieving similar backpressure.\\n\\n![](./perf_increased_load_backpressure.png)\\n\\nJob lifetime and overall processing latency is still better than before scaling up, even though load increased by 1.6x\\n![](./perf_increased_load_job_lifetime.png)\\n![](./perf_increased_load_processing_latency.png)\\n\\n\\nOverall, this shows that scaling up can improve processing performance significantly, especially when the initial cluster setup is resource limited and vertical scaling is not possible."},{"id":"/2023/12/19/Dynamic-Scaling-with-Dataloss","metadata":{"permalink":"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-12-19-Dynamic-Scaling-with-Dataloss/index.md","source":"@site/blog/2023-12-19-Dynamic-Scaling-with-Dataloss/index.md","title":"Dynamic Scaling with Dataloss","description":"We continue our previous experiments with dynamically scaling by now also testing whether","date":"2023-12-19T00:00:00.000Z","formattedDate":"December 19, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.425,"hasTruncateMarker":true,"authors":[{"name":"Ole Sch\xf6nburg","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/oleschoenburg","imageURL":"https://github.com/oleschoenburg.png","key":"ole"}],"frontMatter":{"layout":"posts","title":"Dynamic Scaling with Dataloss","date":"2023-12-19T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"ole"},"unlisted":false,"prevItem":{"title":"Broker Scaling and Performance","permalink":"/zeebe-chaos/2023/12/20/Broker-scaling-performance"},"nextItem":{"title":"Dynamically scaling brokers","permalink":"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers"}},"content":"We continue our [previous experiments](../2023-12-18-Dynamically-scaling-brokers/index.md) with dynamically scaling by now also testing whether\\nthe cluster survives dataloss during the process.\\n\\nOne goal is to verify that we haven\'t accidentally introduced a single point of failure in the cluster.\\nAnother is to ensure that data loss does not corrupt the cluster topology.\\n\\n**TL;DR;**\\nEven with dataloss, the scaling completes successfully and with the expected results.\\nWe found that during scaling, a single broker of the previous cluster configuration can become a single point of failure by preventing a partition from electing a leader.\\nThis is not exactly a bug, but something that we want to improve.\\n\\n\x3c!--truncate--\x3e\\n\\n## Dataloss on the Coordinator\\n\\nZeebe uses Broker 0 as the coordinator for changes to the cluster topology.\\nWhile changes can only be initiated by the coordinator, losing the coordinator and it\'s data should not prevent the scaling operation from completing.\\nWhen the coordinator restarts without any data, it should be able to recover the cluster topology as well as the partition data from the remaining brokers.\\n\\nTo test this, we use the `zbchaos dataloss delete` and `zeebe dataloss recover` commands.\\nAfter deleting, the broker will not restart directly, instead waiting for the `zbchaos dataloss recover` command to be executed.\\nThe `zbchaos dataloss recover` command only unblocks the broker and allows it to start, it does not restore any data and we rely on normal replication for that.\\n\\nShortly after triggering a scale up with `zbchaos cluster scale --brokers 6`, we trigger dataloss on the coordinator with `zbchaos broker dataloss delete --nodeId 0`.\\nAfter observing the system for a while, we then restore the coordinator with `zbchaos dataloss recover --nodeId 0`.\\n\\n### Expected\\n\\nThe scaling operation eventually completes with the expected result of 6 brokers and 6 partitions, evenly distributed.\\nThe coordinator recovers after dataloss and eventually receives the cluster topology from the remaining brokers.\\nThe scaling operation should make progress while the coordinator is down.\\n\\n### Actual\\n\\nAfter starting the operation with `zbchaos cluster scale --brokers 6` we see that the operation has started:\\n```\\n$ zbchaos cluster scale --brokers 6\\nChange 18 is IN_PROGRESS with 0/24 operations complete\\n...\\n```\\n\\nWe then trigger dataloss on the coordinator with `zbchaos broker dataloss delete --nodeId 0`.\\n\\nAfter this, the operations do not make progress anymore and broker 5 is stuck trying to join partition 5:\\n\\n```json\\n{\\n  \\"Version\\": 18,\\n  ...\\n  \\"PendingChange\\": {\\n    \\"Id\\": 18,\\n    \\"Status\\": \\"IN_PROGRESS\\",\\n    \\"StartedAt\\": \\"\\",\\n    \\"CompletedAt\\": \\"\\",\\n    \\"InternalVersion\\": 0,\\n    \\"Completed\\": [\\n      {\\n        \\"Operation\\": \\"BROKER_ADD\\",\\n        \\"BrokerId\\": 3,\\n        \\"PartitionId\\": 0,\\n        \\"Priority\\": 0\\n      },\\n      {\\n        \\"Operation\\": \\"BROKER_ADD\\",\\n        \\"BrokerId\\": 4,\\n        \\"PartitionId\\": 0,\\n        \\"Priority\\": 0\\n      },\\n      {\\n        \\"Operation\\": \\"BROKER_ADD\\",\\n        \\"BrokerId\\": 5,\\n        \\"PartitionId\\": 0,\\n        \\"Priority\\": 0\\n      }\\n    ],\\n    \\"Pending\\": [\\n      {\\n        \\"Operation\\": \\"PARTITION_JOIN\\",\\n        \\"BrokerId\\": 5,\\n        \\"PartitionId\\": 5,\\n        \\"Priority\\": 2\\n      },\\n      ...\\n    ]\\n  }\\n}\\n```\\n\\nThe coordinator is a member of partition 5 but there are two remaining members of partition 5 that should allow broker 5 to join.\\n![](p5-roles.png)\\n\\nAfter restoring the coordinator again with `zbchaos dataloss recover --nodeId 0`, joining eventually completes and the scaling operation finishes successfully.\\n\\n![](scaleup-complete.png)\\n\\n```json\\n{\\n  \\"Version\\": 19,\\n  \\"LastChange\\": {\\n    \\"Id\\": 18,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T17:05:55.849442157Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T17:17:32.913050015Z\\"\\n  },\\n  \\"PendingChange\\": null\\n}\\n```\\n\\nOverall, we achieve our goal that the scaling operation eventually completes with the expected result.\\nThe coordinator recovers after dataloss and eventually receives the cluster topology from the remaining brokers.\\n\\nHowever, it was unexpected that the scaling did not make progress while the coordinator was down.\\n\\n## Single point of failure during scaling\\n\\nThe issue that scaling did not make progress while the coordinator was reproducible.\\nEventually, we diagnosed it as the following edge case:\\n\\nWhen scaling up and and adding a new member to the replication group of a partition, the raft partition goes through joint consensus.\\nThe details of this process are described in the [raft paper](https://raft.github.io/raft.pdf), but here is a very short summary:\\nJoint consensus is similar to a 2-phase commit, where the leader of the partition first introduces a new _joint consensus_ configuration that requires quorum from both the old and new set of members.\\nAfter committing the joint consensus configuration, the leader leaves joint consensus by \\"forgetting\\" the old member set and only using the new member set.\\nOnly after this second configuration is committed, joining of the new member is complete.\\n\\nIn our example, the new set of members has size 4, one of which is the coordinator and one is the newly joining member.\\nWith 4 members, the quorum is 3, meaning that the partition can only elect a leader and process if at least 3 members are available.\\nIn our experiment, we made the coordinator unavailable, so we were already down to 3 members.\\nAdditionally, the newly joining member did not start yet because it was waiting for a successful join response from the leader.\\nThe newly joining member never received such a response because the joint-consensus phase was not completed.\\nThis resulted in only 2 out of 4 members being available, which is not enough to elect a leader.\\n\\nWe want to improve this behavior in the future but likely can\'t prevent it completely.\\nThat means that there is an increased risk of unavailable partitions during scaling.\\nHowever, this only occurs if another broker becomes unavailable with an unfortunate timing and resolves itself automatically once the broker is available again.\\n\\nZeebe issue: https://github.com/camunda/zeebe/issues/15679"},{"id":"/2023/12/18/Dynamically-scaling-brokers","metadata":{"permalink":"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-12-18-Dynamically-scaling-brokers/index.md","source":"@site/blog/2023-12-18-Dynamically-scaling-brokers/index.md","title":"Dynamically scaling brokers","description":"We experimented with the first version of dynamic scaling in Zeebe, adding or removing brokers for a running cluster.","date":"2023-12-18T00:00:00.000Z","formattedDate":"December 18, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":6.05,"hasTruncateMarker":true,"authors":[{"name":"Ole Sch\xf6nburg","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/oleschoenburg","imageURL":"https://github.com/oleschoenburg.png","key":"ole"}],"frontMatter":{"layout":"posts","title":"Dynamically scaling brokers","date":"2023-12-18T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability","performance"],"authors":"ole"},"unlisted":false,"prevItem":{"title":"Dynamic Scaling with Dataloss","permalink":"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss"},"nextItem":{"title":"Job push resiliency","permalink":"/zeebe-chaos/2023/12/06/Job-Push-resiliency"}},"content":"We experimented with the first version of [dynamic scaling in Zeebe](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/operations/cluster-scaling/), adding or removing brokers for a running cluster.\\n\\nScaling up and down is a high-level operation that consists of many steps that need to be carried co-operatively by all brokers in the cluster.\\nFor example, adding new brokers first adds them to the replication group of the assigned partitions and then removes some of the older brokers from the replication group.\\nAdditionally, [priorities](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/configuration/priority-election/) need to be reconfigured to ensure that the cluster approaches balanced leadership eventually.\\n\\nThis orchestration over multiple steps ensures that all partitions are replicated by at least as many brokers as configured with the `replicationFactor`.\\nAs always, when it comes to orchestrating distributed systems, there are many edge cases and failure modes to consider.\\n\\nThe goal of this experiment was to verify that the operation is resilient to broker restarts.\\nWe can accept that operations take longer than usual to complete, but we need to make sure that the operation eventually succeeds with the expected cluster topology as result.\\n\\n**TL;DR;** Both scaling up and down is resilient to broker restarts, with the only effect that the operation takes longer than usual to complete.\\n\\n\x3c!--truncate--\x3e\\n\\n## Scaling up should be resilient to broker restarts\\n\\nWe start with a cluster of 3 brokers, 6 partitions and replication factor 3.\\nIf leadership is balanced, each broker should be leader for 2 partitions and follower for 4 partitions.\\nUsing more partitions than brokers allows us to scale up to more brokers, distributing the partitions such that each broker has less work to do.\\n\\nFor this experiment, we introduce chaos by letting a random broker restart every 30 seconds.\\n\\n### Expected\\n\\nEven when brokers are restarting, the scale operation should eventually succeed.\\nThe expected cluster topology after scaling up is 6 brokers, 6 partitions and replication factor 3, leading to 3 partitions for each broker instead of 6.\\n\\n### Actual\\n\\n#### Verify steady state\\n\\nThe current cluster topology queried with `zbchaos cluster status` shows 6 partitions with 3 replicas each, evenly distributed across the 3 brokers.\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 3,\\n        },\\n        {\\n          \\"Id\\": 4,\\n        },\\n        {\\n          \\"Id\\": 5,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n}\\n```\\nThe above is an abbreviated version of the actual output, which contains more information.\\n\\nAll partitions are reported as healthy and leadership is balanced::\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3         |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\n#### Scaling up with broker restarts\\n\\nWe start the scaling with `zbchaos cluster scale --brokers 6` and restart a random broker every 30 seconds:\\n\\n```bash\\n$ zbchaos cluster scale --brokers 6 & \\n$ while true; do sleep 30; zbchaos restart broker --nodeId $(shuf -i 0-5 -n 1); done\\n```\\n\\nAfter the scaling completed, we stop the restarting and let the cluste settle again for a few minutes.\\n\\n### Result\\n\\nThe scale operation succeeds and the newly reported cluster topology shows us 6 partitions with 3 replicas each, evenly distributed across 6 instead of 3 brokers:\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 3,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 4,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 5,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n  \\"LastChange\\": {\\n    \\"Id\\": 14,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T15:12:57.790824149Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T15:30:20.920657536Z\\"\\n  },\\n}\\n```\\n\\nAll partitions are reported as healthy and leadership is balanced:\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3         |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |\\n3         |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |\\n4         |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |\\n5         |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\nThe operation succeeded in about 17 minutes, longer than usual because of the restarts:\\n![](scaleup-completed.png)\\n\\n\\n## Scaling down should be resilient to broker restarts\\n\\nExactly like scaling up, scaling down is also a high-level operation that consists of many steps that need to be carried out by all brokers in the cluster.\\nBefore a broker can leave, another broker first needs to join the replication group to ensure that we maintain a replication factor of 3 at all times.\\n\\n### Expected\\n\\nEven when brokers are restarting, the scale operation should eventually succeed with the expected cluster topology as result.\\n\\n### Actual\\n\\n#### Verify steady state\\n\\nWe start with the cluster topology that we got as result of the previous experiment.\\n6 partitions with 3 replicas distributed over 6 brokers:\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 3,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 4,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 5,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n  \\"LastChange\\": {\\n    \\"Id\\": 14,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T15:12:57.790824149Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T15:30:20.920657536Z\\"\\n  },\\n}\\n```\\n\\nAll partitions are reported as healthy and leadership is balanced:\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3         |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |\\n3         |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |\\n4         |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |\\n5         |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\n\\n#### Scaling down with broker restarts\\n\\nWe scale down with `zbchaos cluster scale --brokers 3` and restart a random broker every 30 seconds:\\n\\n```bash\\n$ zbchaos cluster scale --brokers 3 &\\n$ while true; do sleep 30; zbchaos restart broker --nodeId $(shuf -i 0-5 -n 1); done\\n```\\n\\n### Result\\n\\nAll 6 partitions with 3 replicas each are evenly distributed across 3 brokers, leading to 6 partitions for each broker again.\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 3,\\n        },\\n        {\\n          \\"Id\\": 4,\\n        },\\n        {\\n          \\"Id\\": 5,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n  \\"LastChange\\": {\\n    \\"Id\\": 16,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T16:07:07.208363298Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T16:28:58.836369836Z\\"\\n  },\\n  \\"PendingChange\\": null\\n}\\n```\\n\\nAll partitions are healthy and leadership is distributed evenly:\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3           |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (UNHEALTHY)  |FOLLOWER (HEALTHY)  |FOLLOWER (UNHEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (UNHEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)    |FOLLOWER (HEALTHY)  |LEADER (UNHEALTHY)  |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)      |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\nThe operation completes in 21 minutes, longer than usual because of the restarts:\\n![](scaledown-completed.png)"},{"id":"/2023/12/06/Job-Push-resiliency","metadata":{"permalink":"/zeebe-chaos/2023/12/06/Job-Push-resiliency","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-12-06-Job-Push-resiliency/index.md","source":"@site/blog/2023-12-06-Job-Push-resiliency/index.md","title":"Job push resiliency","description":"In today\'s chaos day we experimented with job push resiliency.","date":"2023-12-06T00:00:00.000Z","formattedDate":"December 6, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"resiliency","permalink":"/zeebe-chaos/tags/resiliency"}],"readingTime":6.41,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"},{"name":"Nicolas Pepin-Perreault","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/npepinpe","imageURL":"https://github.com/npepinpe.png","key":"nicolas"}],"frontMatter":{"layout":"posts","title":"Job push resiliency","date":"2023-12-06T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability","resiliency"],"authors":["zell","nicolas"]},"unlisted":false,"prevItem":{"title":"Dynamically scaling brokers","permalink":"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers"},"nextItem":{"title":"Job push overloading","permalink":"/zeebe-chaos/2023/11/30/Job-push-overloading"}},"content":"In today\'s chaos day we experimented with job push resiliency.\\n\\nThe following experiments we have done today:\\n\\n1. Job streams should be resilient to gateway restarts/crash\\n2. Job streams should be resilient to leadership changes/leader restarts\\n3. Job streams should be resilient to cluster restarts\\n\\n**TL;DR;** All experiments succeeded and showcased the resiliency even on component restarts. :rocket:\\n\\n\x3c!--truncate--\x3e\\n\\nTo reduce the blast radius and to better verify that everything works as expected we use a trimmed version of our benchmark setup. This means three brokers, one partition, replication factor three, and one gateway. No starter deployed. We deployed one worker with a very high polling interval, to make sure that we rely on streaming.\\n\\n## Gateway restarts\\n\\nIn our first experiment, we wanted to verify that: Job streaming should be resilient to gateway restarts/crashes.\\n\\nThe experiment will look like the following:\\n\\n* Verify steady state:\\n  * Cluster is healthy\\n  * When creating an instance, and start streaming we can retrieve and complete the corresponding job\\n* Chaos injection:\\n  * Restarting the gateway\\n* Verify steady state:\\n    * Cluster is healthy\\n    * When creating an instance, and start streaming we can retrieve and complete the corresponding job\\n\\n### Expected\\n\\nWe expect that even after a gateway restart we can retrieve a job (the stream should be recreated) and complete our new instance.\\n\\n### Actual\\n\\nWe deployed the worker (with a replica of one), and configured it with a high polling interval `-Dapp.worker.pollingDelay=24h`.\\n\\nTo run any instances we need to deploy once the benchmark process model\\n```\\nzbchaos deploy process\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nDeployed given process model , under key 2251799813685249!\\n```\\n\\n#### Verify steady state\\n\\nWe verify the readiness and the instance creation.\\n\\n```sh\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n\\n$ zbchaos verify instance-creation --awaitResult --verbose\\nFlags: {1 LEADER -1  10  msg true 1 LEADER -1 2 LEADER -1 1701853048870 false false true false false 30 false -1 benchmark 30   1 1 benchmark-task}\\nConnecting to ck-np-chaos-day-job-push\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRunning experiment in self-managed environment.\\nPort forward to ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-lhgg5\\nSuccessfully created port forwarding tunnel\\nWe await the result of the process instance creation, thus we skip the partition id check.\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: true]\\nCreated process instance with key 2251799813685251 on partition 1, required partition 0.\\nThe steady-state was successfully verified!\\n```\\n\\n#### Injecting chaos\\n\\nNext, we will restart the gateway.\\n\\n```shell\\n$ zbchaos restart gateway --verbose\\nFlags: {1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1701853221588 false false true false false 30 false -1 benchmark 30   1 1 benchmark-task}\\nConnecting to ck-np-chaos-day-job-push\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRunning experiment in self-managed environment.\\nRestarted ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-lhgg5\\n```\\n\\n#### Verify steady state\\n\\n```shell\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n```\\n\\n```shell\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n\\n### Result \\nThe experiment succeeded. We were able to verify the steady state after the chaos injection. Furthermore, we observe in the metrics as well that the jobs have been pushed after the gateway restart. :white_check_mark:\\n\\n![](job-push-gw-restart.png)\\n\\n\\n### With termination\\n\\nWe wanted to verify the same by terminating the gateway instead of a graceful shutdown (which is done within the restart command).\\n\\n```shell\\n$ zbchaos terminate gateway --verbose\\nFlags: {1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1701853482263 false false true false false 30 false -1 benchmark 30   1 1 benchmark-task}\\nConnecting to ck-np-chaos-day-job-push\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRunning experiment in self-managed environment.\\nTerminated ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-jqfzg\\n```\\n\\nVerifying the steady stated again showed no unexpected issues.\\n\\nOut of interest we checked what is happening in worker:\\n\\n```\\n09:05:44.047 [pool-5-thread-3] WARN  io.camunda.zeebe.client.job.worker - Failed to stream jobs of type \'benchmark-task\' to worker \'benchmark-worker\'\\nio.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\n...\\n```\\n\\nWe see as expected several `UNAVAILABLE: io exception` and later the worker recovered.\\n\\nBased on the metrics we can observe the same. Jobs are pushed to the workers even after restarting the gateway.\\n\\n![](job-push-gw-terminate.png)\\n\\n## Leader restart\\n\\nIn this experiment, we want to verify how resilient job push is on leader changes/restarts.\\n\\nThe verification of the steady state is the same as above, so I will skip this description here.\\n\\n### Expected\\n\\nWorkers shouldn\'t care about leader change, this should be handled fully by the gateway.\\n\\n### Actual\\n\\n#### Verify steady state\\n\\n```\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n#### Inject chaos\\n\\n```shell\\n$ zbchaos restart broker --partitionId 1 --role LEADER\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRestarted ck-np-chaos-day-job-push-zeebe-0\\n```\\n\\n#### Verify steady state\\n\\n```shell\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n### Result \\n\\nWe were able to verify that a leader restart doesn\'t cause issues and job push can handle such events.\\n\\n![](job-push-leader-restart.png)\\n\\n\\nWe can see that the leader was changed, and also switched back shortly after.\\n![](leaderchanges.png)\\n\\nThis is caused by our leader-balancing cron job.\\n![](job-push-leader-restart-cronjob.png)\\n\\nThis also means we had two leader changes, and the push was even pushed by the restarted node.\\n\\n\\n## Complete cluster restart\\n\\nIn this experiment, we wanted to verify whether job push can also handle a complete cluster restart.\\n\\n### Expected\\n\\nJob push can handle a cluster restart and a corresponding job is pushed to the worker afterwards.\\n\\n### Actual\\n\\n#### Verify steady state\\n```\\n\u276f zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n\u276f zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n#### Inject chaos\\n\\nRight now `zbchaos` doesn\'t support restarting a complete cluster, so we had to fall back to `kubectl`. \\n\\n```sh\\n$ kubectl delete pod -l=app=camunda-platform\\npod \\"ck-np-chaos-day-job-push-zeebe-0\\" deleted\\npod \\"ck-np-chaos-day-job-push-zeebe-1\\" deleted\\npod \\"ck-np-chaos-day-job-push-zeebe-2\\" deleted\\npod \\"ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-hj2pf\\" deleted\\n```\\n\\n#### Verify steady state\\n\\n```\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n### Result\\n\\nAgain we were able to show that job push is resilient, and can even handle a complete cluster restart.\\n\\n![](job-push-cluster-restart.png)\\n\\n\\n## Found Bugs\\n\\n* On restart (especially on cluster restart) it looks like job push engine metrics are counted multiple times\\n* [We found a place where we should better handle the exception in pushing async.](https://github.com/camunda/zeebe/blob/a86decce9a46218798663e3466267a49adef506e/transport/src/main/java/io/camunda/zeebe/transport/stream/impl/RemoteStreamPusher.java#L55-L56C14)"},{"id":"/2023/11/30/Job-push-overloading","metadata":{"permalink":"/zeebe-chaos/2023/11/30/Job-push-overloading","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-11-30-Job-push-overloading/index.md","source":"@site/blog/2023-11-30-Job-push-overloading/index.md","title":"Job push overloading","description":"In today\'s chaos day we (Nicolas and I) want to verify how job push behaves and in general, the Zeebe system when we have slow workers.","date":"2023-11-30T00:00:00.000Z","formattedDate":"November 30, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.585,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Job push overloading","date":"2023-11-30T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Job push resiliency","permalink":"/zeebe-chaos/2023/12/06/Job-Push-resiliency"},"nextItem":{"title":"Hot backups impact on processing","permalink":"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing"}},"content":"In today\'s chaos day we (Nicolas and I) want to verify how job push behaves and in general, the Zeebe system when we have slow workers.\\n\\n**TL;DR;** Right now it seems that even if we have a slow worker it doesn\'t impact the general system, and only affects the corresponding process instance, not other instances. We found no unexpected issues, everything performed pretty well. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFirstly we want to verify that job push will not overload a worker or gateway when workers are slow.\\n\\n### Expected\\n\\nWe expect that if the workers are slowing down, the load is distributed to other workers (if available), and it is expected that the general performance (of the affected process instance) should be slowed down. We wouldn\'t expect any restarts/failures on the gateway or workers.\\n\\n### Actual\\n\\n\\nWe deployed a normal benchmark, with [default configurations](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/default/values.yaml).\\n\\n\\nWe slowed the workers down, in the sense that we changed [the completionDelay to 1250 ms](https://github.com/zeebe-io/benchmark-helm/blob/main/charts/zeebe-benchmark/templates/worker.yaml#L30)\\n\\n![](exp1-general.png)\\n\\nThe throughput is lower than normal, as expected.\\n\\nWe see no significant increase in memory usage on the gateway, nor any outages because of this.\\n\\n![](exp1-gw-memory.png)\\n\\nWe see that a high amount of job pushes fail (due to capacity constraints now in the workers).\\n!Jobs are yielded back to the engine.\\n\\n![](exp1-records.png)\\n\\n\\nSo far so good, first experiment worked as expected :white_check_mark:\\n\\n\\n## Second Chaos Experiment \\n\\nThe normal scenario when something is slow is for a user to scale up. This is what we did in the next experiment, we scaled the workers to 10 replicas (from 3), to verify how the system behaves in this case.\\n\\n\\nSomething to keep in mind when the completion delay is 1250ms, we [multiply the activation timeout by 6 in our workers](https://github.com/camunda/zeebe/blob/7002d53a079c06ab3a94f5485f022681a41dc9ed/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L113). This means completionDelay: 1250 -> job timeout 7.5s\\n\\n### Expected\\n\\nWe expect that we can reach a higher throughput.\\n\\n### Actual\\n\\nScaling the workers to 10:\\n\\n```sh\\nk scale deployment worker --replicas=10\\n```\\n\\n![](exp2-general.png)\\n\\nWe can see that after scaling we can complete more jobs.\\n\\n\\nThe gateway memory seems to be not really affected.\\n![](exp2-gw-mem.png)\\n\\nIn the job push metrics we see less job push failures.\\n\\n![](exp2-job-push.png)\\n\\nWhen we check the written records we can see a decrease in yield, but an increase in timeouts. The reason is that we have to try several workers before giving it back.\\n\\n![](exp2-records.png)\\n![](exp2-records2.png)\\n\\n\\nExperiment two worked as expected. :white_check_mark:\\n\\n## Third Chaos Experiment \\n\\nIn a real-world scenario, it will not happen if you have a slow dependency, for which for example a worker waits that you can scale and this will solve your problems. Likely you will get even slower because more pressure is put on the dependency. To mimic this scenario we experimented with increasing the completion time again. A completion delay set to 2500ms, means a job timeout of 15s.\\n\\n### Expected\\n\\nWe expect after slowing down all workers again, that our throughput goes down again, but we should see no general error. Potentially a slight memory increase because of buffering of jobs.\\n\\nThis also means more yields and fewer timeouts.\\n\\n### Actual\\n\\nAs expected again we see a drop in throughput, but it is still a bit higher than at the first experiment.\\n\\n![](exp3-general.png)\\n\\nNo difference at all in the memory consumption, by the gateway.\\n\\n![](exp3-memory.png)\\n\\n\\nIn the records we can also again see that yield increase, and timeouts have been decreased.\\n\\n![](exp3-records.png)\\n\\n\\nExperiment three worked as expected. :white_check_mark:\\n\\n## Several further experiments\\n\\nWe did several further experiments where we scaled the workers, played with the completion delay, reduced the starter load etc. At some point, we reached a state size that was too big (~2 Gig) such that this impacted our processing. We had to drain the cluster and stop the starters completely.\\n\\n\\nInterestingly was that when we reduced the completion delay, we just had a slight increase in completion, when we scaled down the workers (marked with the annotation in the graph), to reduce activations, we saw no difference. \\n\\n![](exp6-general.png)\\n\\n\\nOnly when we hit a certain threshold in RocksDb (it seems to be at least), the completion went up by a lot.\\n\\n\\n![](exp6-state.png)\\n\\nThis is because the record processing latency was heavily reduced (likely the commit latency or iteration latency in RocksDb).\\n\\n![](exp6-latency.png)\\n\\n\\n\\n## Experiment with worker impact\\n\\nWe wanted to understand and experiment with the impact of a slow worker on different process instances. \\n\\nTo see such an impact in our metrics we had to patch our current execution metrics, such that includes the BPMN processId, so we can differentiate between execution times of different processes.\\n\\nSee the related branch for more details [ck-latency-metrics](https://github.com/camunda/zeebe/tree/ck-latency-metrics)\\n\\n\\nFurthermore, a new process model was added `slow-task.bpm` and new deployments to create such instances and work on them. The process model was similar to the benchmark model, only the job type has been changed.\\n\\n### Expected\\n\\nTo verify was that whether a slow worker would impact other instances, this was uncertain territory we were hitting. \\n\\nTo be honest we expected it would affect them.\\n\\n### Actual\\n\\n\\nWe started the benchmark (default configs for broker and gateway), with additional configurations:\\n\\n * benchmark starter with 75 PI/s rate\\n * 3 benchmark worker (60 capacity) and completion delay of 50 ms\\n * slow-task starter with 75 PI/s rate\\n * 3 slow-worker (60 capacity) and a completion delay of 50 ms (at the begin)\\n\\n\\n![](slow-exp-base.png)\\n\\nWe can see based on the metrics that the execution latency is the same for both process instances, and we are able to complete our 150 PI/s.\\n\\n![](slow-exp-base-general.png) \\n\\n\\nWe slowed now the worker for the type `slow-task` down to a completion delay of 2500ms.\\n\\n![](slow-exp-slow-worker-2500ms-records.png)\\n\\nWe can see that we start to get `Job.YIELD` commands from the gateway, and we can see that the process instance execution is slowed down.\\n\\n![](slow-exp-slow-worker-2500ms.png)\\n\\nInterestingly that is only for the affected process instance, which we wanted to validate/verify. \\n\\n\\n### Reasoning\\n\\nOur first assumption was that both instance latencies would be impacted, because are writing YIELD commands, instead of being able to complete them. \\n\\n\\nBut another consequence comes into play. If fewer jobs are worked on, there are also fewer jobs completed, this means fewer process instances have to be continued (with batch processing until the end).\\n\\nThis means a load of yield underweights the normal load of job completions, with additional process instance continuation. That was an interesting insight for us.\\n\\n## Result\\n\\nRight now it seems that even if we have a slow worker it doesn\'t impact badly the general system, and only affects the corresponding process instance, not other instances.\\n\\nWhat we should or need to investigate further what if the job completion delay is much larger than the timeout. This is something we might want to test soon."},{"id":"/2023/11/07/Hot-backups-impact-on-processing","metadata":{"permalink":"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-11-07-Hot-backups-impact-on-processing/index.md","source":"@site/blog/2023-11-07-Hot-backups-impact-on-processing/index.md","title":"Hot backups impact on processing","description":"Today, we want to experiment with hot backups in SaaS and a larger runtime state in Zeebe and how it impacts the ongoing processing in Zeebe (or not?). This is part of the investigation of a recently created bug issue we wanted to verify/reproduce #14696.","date":"2023-11-07T00:00:00.000Z","formattedDate":"November 7, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.82,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Hot backups impact on processing","date":"2023-11-07T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Job push overloading","permalink":"/zeebe-chaos/2023/11/30/Job-push-overloading"},"nextItem":{"title":"Using Large Multi-Instance","permalink":"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance"}},"content":"Today, we want to experiment with hot backups in SaaS and a larger runtime state in Zeebe and how it impacts the ongoing processing in Zeebe (or not?). This is part of the investigation of a recently created bug issue we wanted to verify/reproduce [#14696](https://github.com/camunda/zeebe/issues/14696).\\n\\n**TL;DR;** We were able to prove that hot backups are indeed not impacting overall processing throughput in Zeebe. We found that having a full Elasticsearch disk might impact or even fail your backups, which is intransparent to the user.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor the experiment, we have set up a Camunda SaaS cluster (G3-M configuration), and run the [cloud benchmark](https://github.com/camunda/zeebe/tree/main/benchmarks/setup/cloud-default) workload against it. During the experiment, we will run a stable load, which will cause to increase in the runtime state. We will create/initiate in different stages backups to verify the impact on processing depending on state size.\\n\\nWe kept the starter rate (creation of process instance 100 PI/s) but reduced the worker capacity and replicas.\\n\\n\\n### Expected\\n\\nHot backups were built with the premise of not disrupting the processing throughput in Zeebe, which is why we define the following hypothesis:\\n\\n> **Hypothesis**\\n>\\n> Creating hot backups should not impact Zeebe\'s _processing throughput _no_ matter how_ large the runtime state is in Zeebe.\\n\\n### Actual\\n\\n\\nWe created a cluster in the Camunda SaaS environment (in our internal stage).\\n\\n#### Step one\\n\\nWe created a first backup to verify that it works without issues.\\n\\n##### Result\\n\\nSuccess on stage one creating a backup with no actual state.\\n\\n![first-backup](first-backup.png)\\n\\n\\n#### Step two\\n\\nWe started a stable load as mentioned [above](#chaos-experiment). After reaching around ~100 MB runtime state at each partition we triggered a backup.\\n\\n##### Result\\n\\n![](general-sec-bk.png)\\n\\nThe backup was successful and we were not able to observe any disruption in the processing throughput. We can see that during the backup is taken the exporting is paused (which is expected) and afterwards it is starting to export again. :white_check_mark:\\n\\n#### Step three\\n\\nAt a later stage, we tried to take a backup again with around ~300MB of runtime state in Zeebe.\\n\\n##### Result\\n\\nBased on the output from Console the backup was successful and took around one hour.\\n\\n![third-console](sec-bk-console.png)\\n\\nBased on our internal metrics we can also see that there is no impact on the processing throughput\\n\\n![general-third](general-third-bk.png)\\n\\n\\nWhat is unclear to me is that it looks like we only took a backup of partition two. This needs to be further investigated, it might be also that the metrics are just confusing since it is resetting after the pod restarts.:bug:\\n\\n#### Step four\\n\\nHere we come into struggle after running the load on the cluster for quite some time we reached a runtime state size of ~1 gig. Furthermore, we filled our Elasticsearch disk tremendously.\\n\\n##### Result\\n\\nAt this time we were no longer able to successfully create backups. Here I tried it first without interacting with the cluster. It failed after 1.5 hours, which is potentially the timeout.\\n\\n![](third-bk-console.png)\\n\\n\\nThe backup failed, because of elastic was full. \\n\\n\\n![elastic-full](deleting-indices4.png)\\n\\nI went ahead to remove some data from Elastic to keep experimenting.\\n\\n![elastic-indices](deleting-indices3.png)\\n![delete-indices](deleting-indices2.png)\\n\\n#### Step five\\nAfter cleaning the Elasticsearch we retried taking a backup again. At this step, we already reached a runtime state of ~1.25G for each partition, which is quite huge.\\n\\n##### Result\\n\\nThe backup took quite a while and failed again.\\n\\n\\n![last-bk](last-bk-failed.png)\\n\\nWhat we can see based on the times it is likely that it here timed out again. Taking a look at the metrics we see that a backup was processed in Zeebe.\\n\\nIt had no impact on the ongoing processing throughput.\\n\\n\\n![fifth-bk](general-fifth-bk.png)\\n\\nAt a later stage, we tried another backup with ~1.45G of runtime state even here we were not able to observe any issues related to the impact on processing throughput.\\n\\n\\n![sixth](general-sixth-bk.png)\\n\\n\\n### Conclusion\\n\\n:white_check_mark: We were able to prove that even on a large runtime state there is no observable impact on processing throughput in Zeebe. \\n\\nFurthermore, we have seen that having a large Elastic state (almost full disk) will impact taking backups and is likely to fail them. We might need to iterate here, whether we want to tune the backup strategy, give elastic more space when taking backups, or adjust watermarks, etc.\\n\\n#### Follow-ups\\n\\nWe have to investigate the marking of failed backups, whether it is because of a timeout in the Operator or whether these backups are failed. It looks to me like they are marked as failed, even if they may succeed.\\n\\nFurthermore, the completed backups seem to be misleading and are reset which causes inconsistent views."},{"id":"/2023/06/02/Using-Large-Multi-Instance","metadata":{"permalink":"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-06-02-Using-Large-Multi-Instance/index.md","source":"@site/blog/2023-06-02-Using-Large-Multi-Instance/index.md","title":"Using Large Multi-Instance","description":"New day new chaos. In today\'s chaos day I want to pick up a topic, which had bothered people for long time. I created a chaos day three years ago around this topic as well.","date":"2023-06-02T00:00:00.000Z","formattedDate":"June 2, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.77,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Using Large Multi-Instance","date":"2023-06-02T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Hot backups impact on processing","permalink":"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing"},"nextItem":{"title":"Continuing SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle"}},"content":"New day new chaos. :skull: In today\'s chaos day I want to pick up a topic, which had bothered people for long time. I created a [chaos day three years ago](https://zeebe-io.github.io/zeebe-chaos/2020/07/16/big-multi-instance/) around this topic as well. \\n\\nToday, we experiment with large multi-instances again. In the recent patch release [8.2.5](https://github.com/camunda/zeebe/releases/tag/8.2.5) we fixed an issue with spawning larger multi instances. Previously if you have created a process instance with a large multi-instance it was likely that this caused to blacklist the process instance, since the multi-instance spawning ran into `maxMessageSize` limitations. \\n\\nThis means the process instance was stuck and was no longer executable. In Operate this was not shown and caused a lot of friction or confusion to users. With the recent fix, Zeebe should chunk even large collections into smaller batches to spawn/execute the multi-instance without any issues.\\n\\n**TL;DR;** We were able to see that even large multi-instances can be executed now. :white_check_mark: At some point, we experienced performance regressions (during creating new multi-instance elements) but the execution of the process instance doesn\'t fail anymore. One problem at a time, we will likely investigate further to improve the performance of such a use case.\\n\\nWhen we reached the `maxMessageSize` we got a rejection, if the input collection is too large we see some weird unexpected errors from NGINX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe do regularly game days in Camunda, and for such we also create projects to make incidents, etc. reproducible. In today\'s chaos day, I will reuse some code created by [Philipp Ossler](https://github.com/saig0), thanks for that :bow: Since we mimic in such game days customers, the process is a bit more complex than necessary for such chaos day, but I will keep it like that.\\n\\n![order-process](order-process.png)\\n\\nThe input collection `items`, which is used in the multi-instance is generated via:\\n\\n```shell\\n    // input size\\n    final var items = IntStream.range(0, size).mapToObj(i -> Map.ofEntries(\\n        entry(\\"id\\", i)\\n    )).toList();\\n```\\n\\nIn the following experiment, we will play around with the `size` value.\\n\\nFor the experiment, we will use a Camunda 8 SaaS cluster with the generation `Zeebe 8.2.5` (G3-S).\\n\\n### Expected\\n\\nWhen creating a process instance with a large collection, we expect based on the recent bug fix that the multi-instance creation is batched and created without issues. \\n\\nOne limiting factor might be the `maxMessageSize` with regard to the input collection, but in this case, I would expect that the creation of the process instance is already rejected before.\\n\\n### Actual\\n\\nBetween the following experiments, I always recreated the clusters, to reduce the blast radius and better understand and isolate the impact. \\n\\n#### Starting small (20k)\\n\\nIn previous versions, the multi-instance creation failed already quite early. For example in the game day reproducer project, we had a collection defined with `20.000` items, which we are now reusing for the start.\\n\\nThe creation of the process instance worked without any issues. We can observe in Operate the incremental creation of sub-process instances, which is great.\\n\\n![incremental-creation-20k](20k-operate-inc.png)\\n\\nWe can see in the metrics that batch processing is limited by only 2-4 commands in a batch. That is an interesting fact that might explain why it takes a while until all instances of the multi-instance sub-process are created. We can even see rollbacks during batch processing, visible in the \\"Number of batch processing retries\\" panel.\\n\\n![processing-metrics-20k](20k-processing-metrics.png)\\n\\nThe processing queue seems to increase dramatically.\\n\\nAfter a while, we can see that all 20k instances are created without any bigger issues. :rocket:\\n\\n![complete-20k](20k-operate-complete.png)\\n\\nIt took around 10 minutes. Taking a look at the metrics again we see that in between big command batches have been created/processed, which allowed us to reduce the processing queue.\\n\\n![processing-metrics-20k-pt2](20k-processing-metrics-2.png)\\n\\nIn between the backpressure was quite high, but after the creation of all instances, the cluster is in a healthy state again. The creation of such multi-instance worked :white_check_mark:\\n\\n![general-metrics-20k](20k-general-metrics.png)\\n\\n#### Increase collection (200k)\\n\\nAgain, the creation of such a process instance was not a problem itself. We can observe the creation of the sub-process instances (multi-instance) in Operate, which happens incrementally.\\n\\n![incremental-creation-200k](200k-operate-inc.png)\\n\\nIt takes ages until the instances are created (After 3h ~66k instances are created). Again we see here small chunks of batches, and there are also rollbacks during batch processing.\\n\\n![processing-metrics-200k](200k-processing-metrics.png)\\n\\nThe processing of that partitions is in this case blocked by the multi-instance creation, we can see that on the 100% back pressure. :x:\\n\\n![general-metrics-200k](200k-general-metrics.png)\\n\\nEven after one hour, not all instances are created (not even 20k), it takes longer than before the creation of 20.000 instances.\\n\\n![incremental-creation-200k-part2](200k-operate-inc2.png)\\n\\n#### Make it really big (2 million)\\n\\nTo escalate this even more I increase the input collection again by a factor of 10 to 2 million.\\n\\nAfter creation, I see as a response the following log message in my log:\\n\\n```\\nFailed to create process instance of \'order-process\'\\n\\nio.camunda.zeebe.client.api.command.ClientStatusException: HTTP status code 502\\ninvalid content-type: text/html\\nheaders: Metadata(:status=502,date=Fri, 02 Jun 2023 11:44:57 GMT,content-type=text/html,strict-transport-security=max-age=63072000; includeSubDomains,content-length=150)\\nDATA-----------------------------\\n<html>\\n<head><title>502 Bad Gateway</title></head>\\n<body>\\n<center><h1>502 Bad Gateway</h1></center>\\n<hr><center>nginx</center>\\n</body>\\n</html>\\n\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createProcessInstance(ProcessApplication.java:90) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createInstanceOfProcess(ProcessApplication.java:71) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.run(ProcessApplication.java:58) ~[classes/:na]\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:791) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:775) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:345) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1343) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1332) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.main(ProcessApplication.java:46) ~[classes/:na]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: HTTP status code 502\\ninvalid content-type: text/html\\nheaders: Metadata(:status=502,date=Fri, 02 Jun 2023 11:44:57 GMT,content-type=text/html,strict-transport-security=max-age=63072000; includeSubDomains,content-length=150)\\nDATA-----------------------------\\n<html>\\n<head><title>502 Bad Gateway</title></head>\\n<body>\\n<center><h1>502 Bad Gateway</h1></center>\\n<hr><center>nginx</center>\\n</body>\\n</html>\\n\\n\\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) ~[na:na]\\n\\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[na:na]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\t... 9 common frames omitted\\n```\\n\\n\\n\\nI tried to incrementally decrease the input collection until it is working again, when reaching 250k I finally see a better understandable error.\\n\\n```shell\\n2023-06-02 13:53:51.485 ERROR 29870 --- [           main] i.c.cloud.gameday.ProcessApplication     : Failed to create process instance of \'order-process\'\\n\\nio.camunda.zeebe.client.api.command.ClientStatusException: Command \'CREATE\' rejected with code \'EXCEEDED_BATCH_RECORD_SIZE\': \\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createProcessInstance(ProcessApplication.java:90) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createInstanceOfProcess(ProcessApplication.java:71) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.run(ProcessApplication.java:58) ~[classes/:na]\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:791) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:775) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:345) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1343) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1332) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.main(ProcessApplication.java:46) ~[classes/:na]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNKNOWN: Command \'CREATE\' rejected with code \'EXCEEDED_BATCH_RECORD_SIZE\': \\n\\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) ~[na:na]\\n\\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[na:na]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\t... 9 common frames omitted\\nCaused by: io.grpc.StatusRuntimeException: UNKNOWN: Command \'CREATE\' rejected with code \'EXCEEDED_BATCH_RECORD_SIZE\': \\n\\tat io.grpc.Status.asRuntimeException(Status.java:535) ~[grpc-api-1.45.1.jar:1.45.1]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[na:na]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[na:na]\\n\\tat java.base/java.lang.Thread.run(Thread.java:833) ~[na:na]\\n\\n2023-06-02 13:53:51.485  INFO 29870 --- [           main] i.c.cloud.gameday.ProcessApplication     : Created process instances with large collection. [order-id: \'ba65b59b-1584-48bb-af05-3724ea15fac9\']\\n\\n```\\n\\n### Results\\n\\nAs we have seen above we are able now to create much larger multi instances than before, with some drawbacks in performance, which needs to be investigated further.\\n\\nWhen reaching a certain limit (maxMessageSize) we get a described rejection by the broker, until we reach the limit of NGINX where the description is not that optimal. Here we can and should improve further.\\n\\n\\n## Found Bugs\\n\\n  * in a previous test I run into https://github.com/camunda/zeebe/issues/12918\\n  * Related bug regarding the input collection https://github.com/camunda/zeebe/issues/12873"},{"id":"/2023/05/19/Continuing-SST-Partitioning-toggle","metadata":{"permalink":"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-05-19-Continuing-SST-Partitioning-toggle/index.md","source":"@site/blog/2023-05-19-Continuing-SST-Partitioning-toggle/index.md","title":"Continuing SST Partitioning toggle","description":"Today we want to continue with the experiment from last Chaos day, but this time","date":"2023-05-19T00:00:00.000Z","formattedDate":"May 19, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":8.105,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Continuing SST Partitioning toggle","date":"2023-05-19T00:00:00.000Z","categories":["chaos_experiment","configuration"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Using Large Multi-Instance","permalink":"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance"},"nextItem":{"title":"SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle"}},"content":"Today we want to continue with the experiment from [last Chaos day](../2023-05-15-SST-Partitioning-toggle/index.md), but this time\\nwith a bit more load. This should make sure that we trigger the compaction of RocksDB and cause the SST partitioning to happen, for real.\\n\\nThe reasons stay the same we want to find out whether it would be possible to enable and disable the flag/configuration without issues.\\n\\n**TL;DR;** Today\'s, experiments succeeded :rocket:. We were able to show that even with a higher number of process instances (bigger state) we can easily disable and enable the SST partitioning flag without issues. I also got a confirmation from a RocksDb contributor that our observations are correct, and that we can easily toggle this feature without issues.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nSimilar setup to the [last Chaos day](../2023-05-15-SST-Partitioning-toggle/index.md#chaos-experiment).\\nExcept this time we will enable Operate as well, in order to verify easily whether all instances have been completed.\\nOther than that we use the standard benchmark configuration, without clients.\\n\\nThe verification of the steady state will consist, of checking the readiness and healthiness of the cluster, via zbchaos and metrics. Furthermore, we will verify that we can access operate and that no instances are running. As defined in chaos engineering principles the process of a chaos experiment looks always the same, Verify the steady state, introduce chaos, and verify the steady state.\\n\\nIn our first experiment, we will enable the SST partitioning.\\n\\n**First chaos action**\\n  * Deploy a process model (which contains a [simple model](https://github.com/zeebe-io/zeebe-chaos/blob/main/go-chaos/internal/bpmn/one_task.bpmn))\\n  * Start 1000 process instances (PIs), with a service task\\n  * Enable the SST partitioning\\n  * Restart the cluster, and await readiness\\n  * Complete the jobs (in consequence the PIs)\\n\\nIn our second experiment, we will disable the SST partitioning again.\\n\\n**Second chaos action:**\\n\\n  * Start 1000 process instances (PIs), with a service task\\n  * Disable the SST partitioning\\n  * Restart the cluster, and await readiness\\n  * Complete the jobs (in consequence the PIs)\\n\\n### Expected\\n\\n> When operating a cluster, I can enable and disable the SST partitioning without an impact on executing existing process instances. Existing PIs should still be executable and completable.\\n\\n### Actual\\n\\nAs linked above I used again our [benchmark/setup](https://github.com/camunda/zeebe/tree/main/benchmarks/setup) scripts to set up a cluster.\\n\\n```shell\\n$ diff ../default/values.yaml values.yaml \\n40c40\\n<   replicas: 3\\n---\\n>   replicas: 0\\n47c47\\n<   replicas: 1\\n---\\n>   replicas: 0\\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"false\\"\\n96a98,100\\n>     identity:\\n>       auth:\\n>         enabled: false\\n326c330\\n<     enabled: false\\n---\\n>     enabled: true\\n```\\n\\n\\n#### First Experiment: Verify Steady state\\nTo verify the readiness and run all actions I used the [zbchaos](https://github.com/zeebe-io/zeebe-chaos/tree/zbchaos-v1.0.0) tool.\\n\\n```shell\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n```\\n\\nLooking at the metrics shows that everything looks healthy. The only weird part is the topology panel which seems to be broken.\\n![start](start.png)\\n\\nWhen requesting the topology via `zbchaos` we retrieve this:\\n\\n```shell\\n$ zbchaos topology\\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1684476531915 false false true false false 30 false -1 benchmark 30  }\\nNode      |Partition 1         |Partition 2         |Partition 3\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n0         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n```\\n\\nFor now, we assume the dashboard has an issue and continue with the experiment.\\n\\nWe are able to access operate without issues, and see no instances yet.\\n\\n![operate](operate.png)\\n\\n\\n#### First Experiment: Chaos Action\\n\\nAfter the verification stage, we start with our chaos action, injecting chaos into the system.\\nThe first step is to deploy the mentioned simple process model:\\n```shell\\n$ zbchaos deploy process -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPort forward to zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\nSuccessfully created port forwarding tunnel\\nDeploy file bpmn/one_task.bpmn (size: 2526 bytes).\\nDeployed process model bpmn/one_task.bpmn successful with key 2251799813685249.\\nDeployed given process model , under key 2251799813685249!\\n```\\n\\nThis is then as well visible in operate.\\n\\n![operate-process](operate-process.png)\\n\\nAs the next step, we will create 1000 process instances of our simple process model, with one service task.\\nFor that, we can [use a new functionality](https://github.com/zeebe-io/zeebe-chaos/tree/zell-chaos-create-count-of-instances) of `zbchaos` I built for this chaos day.\\n\\nOn the first try, I had smaller issues, with timeouts etc.\\n```shell\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[299/999] Created process instance with key 6755399441056339 on partition 3.\\npanic: Expected to create 999 process instances, but timed out after 30s created 299 instances.\\n```\\n\\nThis is the reason why I had to retry the creations in the end the count is not exactly 1000 :)\\n```shell\\n./dist/zbchaos verify instance-count --instanceCount 697 -v --timeoutInSec 300\\n...\\n[695/697] Created process instance with key 4503599627372489 on partition 2.\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[696/697] Created process instance with key 6755399441057737 on partition 3.\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[697/697] Created process instance with key 2251799813687255 on partition 1.\\nThe steady state was successfully verified!\\n```\\n\\n![pi](operate-pi.png)\\n\\nNow we are coming to the interesting part. Enabling SST partitioning.\\n\\n```shell\\n$ diff ../default/values.yaml values.yaml \\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"true\\"\\n$ make update \\nhelm upgrade --namespace zell-chaos zell-chaos zeebe-benchmark/zeebe-benchmark -f values.yaml\\n```\\n\\n> **Note**\\n> Changing the configmap doesn\'t restart pods! We need to delete all Zeebe pods, to apply the configuration.\\n\\n```shell\\n$ k delete pod -l app=camunda-platform\\npod \\"zell-chaos-zeebe-0\\" deleted\\npod \\"zell-chaos-zeebe-1\\" deleted\\npod \\"zell-chaos-zeebe-2\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-8j7d6\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\" deleted\\n```\\n\\n```shell\\n$ zbchaos verify readiness \\nAll Zeebe nodes are running.\\n```\\n\\nNow starting to complete the previously created jobs, we can use again a new feature in `zbchaos` ([which has been added during the chaos day](https://github.com/zeebe-io/zeebe-chaos/tree/zell-chaos-create-count-of-instances))\\nUnfortunately, I missed using the verbose flag.\\n```shell\\n$ ./dist/zbchaos verify job-completion --jobCount 1001 --timeoutInSec 1200\\nThe steady-state was successfully verified!\\n```\\n\\n#### First Experiment: Verify Steady state\\n\\nThe job completions worked without issues. The metrics are looking good, the topology panel seems to work again as well.\\n\\n![complete](metrics-complete.png)\\n\\nIn operate we can see that there are no longer any running instances and all of them have been completed.\\n\\n![complete-operate](operate-complete.png)\\n![complete-operate2](operate-complete2.png)\\n\\nThe first part of the experiment worked as expected :white_check_mark:\\n\\n#### Second Experiment: Chaos Action\\n\\nWe are skipping the verification step, due to previous verification, we directly start with creating 1000 process instances.\\n```\\n$ ./dist/zbchaos verify instance-count --instanceCount 1000 -v --timeoutInSec 300\\n\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[1000/1000] Successful command sent, got response with key 2251799813690599 on partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\n![second](second-exp.png)\\n\\nDisable the SST partitioning flag and update the cluster.\\n```shell\\n$ make update \\n$ k delete pod -l app=camunda-platform\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n```\\n\\nComplete all jobs:\\n\\n```shell\\n$ ./dist/zbchaos verify job-completion --jobCount 1000 --timeoutInSec 1200 -v\\n[999/1000] Successful command sent, got response with key 6755399441061073 on partition 3.\\nSend job activate command, with job type \'benchmark-task\'\\n[1000/1000] Successful command sent, got response with key 2251799813690604 on partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\n#### Second Experiment: Verify Steady state\\n\\nAgain the experiment succeeded, we were able to show that even with a higher number of process instances we can easily disable and enable the SST partitioning flag.\\n\\n![verify](second-exp-verify.png)\\n![op-complete](second-exp-operate-complete.png)\\n\\nIn the snapshots at we can see that some more files are used.\\n\\n![snap](second-exp-snap.png)\\n\\nBut in RocksDb metrics we see no real compaction going on, which is why we will retry the same with a higher amount of data.\\n\\n![rocks](second-exp-rocks.png)\\n\\n\\n#### SST Partitioning and compaction\\n\\nI tried to run the experiment again but with more data (~11K instances).\\n\\nEven when the metrics don\'t show the compaction, I was able to see in the RocksDB that compacting is happening.\\n\\nAround 11:56 between different loads\\n\\n![thirdrun](thirdrun.png)\\n\\nWe see in the metrics of RocksDB that nothing\\n\\n![rocks](compacting.png)\\n\\nBut when checking the RocksDB logs\\n\\n```\\n$ cat data/raft-partition/partitions/1/runtime/LOG.old.1684493206724692 \\n...\\n\\n2023/05/19-09:56:40.652111 140580419004160  Options.sst_partitioner_factory: SstPartitionerFixedPrefixFactory\\n\\n...\\n2023/05/19-09:56:41.354244 140579153618688 (Original Log Time 2023/05/19-09:56:41.354149) EVENT_LOG_v1 {\\"time_micros\\": 1684490201354123, \\"job\\": 2, \\"event\\": \\"compaction_finished\\", \\"compaction_time_micros\\": 374078, \\"compaction_time_cpu_micros\\": 72361, \\"output_level\\": 3, \\"num_output_files\\": 14, \\"total_output_size\\": 6283132, \\"num_input_records\\": 69787, \\"num_output_records\\": 39118, \\"num_subcompactions\\": 1, \\"output_compression\\": \\"NoCompression\\", \\"num_single_delete_mismatches\\": 0, \\"num_single_delete_fallthrough\\": 0, \\"lsm_state\\": [0, 0, 0, 14]}\\n2023/05/19-09:56:41.354763 140579153618688 [le/delete_scheduler.cc:77] Deleted file /usr/local/zeebe/data/raft-partition/partitions/1/runtime/000045.sst immediately, rate_bytes_per_sec 0, total_trash_size 0 max_trash_db_ratio 0.250000\\n2023/05/19-09:56:41.354786 140579153618688 EVENT_LOG_v1 {\\"time_micros\\": 1684490201354782, \\"job\\": 2, \\"event\\": \\"table_file_deletion\\", \\"file_number\\": 45}\\n2023/05/19-09:56:41.355217 140579153618688 [le/delete_scheduler.cc:77] Deleted file /usr/local/zeebe/data/raft-partition/partitions/1/runtime/000044.sst immediately, rate_bytes_per_sec 0, total_trash_size 0 max_trash_db_ratio 0.250000\\n2023/05/19-09:56:41.355247 140579153618688 EVENT_LOG_v1 {\\"time_micros\\": 1684490201355243, \\"job\\": 2, \\"event\\": \\"table_file_deletion\\", \\"file_number\\": 44}\\n2023/05/19-09:56:41.355765 140579153618688 [le/delete_scheduler.cc:77] Deleted file /usr/local/zeebe/data/raft-partition/partitions/1/runtime/000043.sst immediately, rate_bytes_per_sec 0, total_trash_size 0 max_trash_db_ratio 0.250000\\n```\\nWe see several lines which indicate the compaction.\\n\\n## Conclusion\\n\\n We have seen that even when we toggle the SST partitioning, we are able to make progress and our stored data is not impacted. This is a great out come since it means we can easily enable such configuration on existing clusters and gains the performance benefits for larger states as we have seen in previous benchmarks.\\n\\nI have posted a question related to this topic in the [RocksDb google group](https://groups.google.com/g/rocksdb/c/Ys-yZIznZwU) and I got a private answer which contains the following:\\n\\n> Partitioner is just a hinter when compaction should split the file. Default compaction is also splitting by file size. So it has no functional effect and you can change configuration anytime.\\n>\\n> Partitioner does not need to be simple prefix only, but one can use more complicated strategy.\\n\\nThis confirms our observation and makes it much more trustworthy.\\n\\n\\n## Found Bugs\\n\\n * Grafana Topology Panel seems to be buggy from time to time\\n * RocksDB compaction panel seems to show no data (might be related to a short time frame)"},{"id":"/2023/05/15/SST-Partitioning-toggle","metadata":{"permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-05-15-SST-Partitioning-toggle/index.md","source":"@site/blog/2023-05-15-SST-Partitioning-toggle/index.md","title":"SST Partitioning toggle","description":"On this chaos day I wanted to experiment with a new experimental feature we have released recently. The enablement of the partitioning of the SST files in RocksDB. This is an experimental feature from RocksDb, which we made available now for our users as well, since we have seen great benefits in performance, especially with larger runtime data.","date":"2023-05-15T00:00:00.000Z","formattedDate":"May 15, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":6.695,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"SST Partitioning toggle","date":"2023-05-15T00:00:00.000Z","categories":["chaos_experiment","configuration"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Continuing SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle"},"nextItem":{"title":"Gateway Termination","permalink":"/zeebe-chaos/2023/04/06/gateway-termination"}},"content":"On this chaos day I wanted to experiment with a new experimental feature we have released recently. The [enablement of the partitioning of the SST files in RocksDB](https://github.com/camunda/zeebe/pull/12483). This is an experimental feature from RocksDb, which we made available now for our users as well, since we have seen great benefits in performance, especially with larger runtime data.\\n\\nI wanted to experiment a bit with the SST partitioning and find out whether it would be possible to enable and disable the flag/configuration without issues.\\n\\n**TL;DR;** The first experiment was successful, it looks like we can enable and disable the partitioning without impacting the execution of one existing PI. We need to experiment a bit more with larger data sets to force RocksDB compaction, to be fully sure. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor our chaos experiment we set up again our [normal benchmark cluster](https://github.com/camunda/zeebe/tree/main/benchmarks/setup), this time without any clients (no workers/starters).\\n\\nSetting all client replicas to zero:\\n```diff\\n$ diff default/values.yaml zell-chaos/values.yaml \\n40c40\\n<   replicas: 3\\n---\\n>   replicas: 0\\n47c47\\n<   replicas: 1\\n---\\n>   replicas: 0\\n``` \\n\\nThe experiment we want to do on this chaos day will look like the following:\\n\\n**First part:** \\n\\n * Verify steady state:\\n   * verify the readiness of the cluster \\n   * deploy a process model (which contains a [simple model](https://github.com/zeebe-io/zeebe-chaos/blob/main/go-chaos/internal/bpmn/one_task.bpmn))\\n * Chaos Action: \\n   * start a process instance (PI), with a service task\\n   * enable the SST partitioning\\n   * restart the cluster\\n   * verify the readiness\\n   * verify that job is activatable\\n   * complete the job (in consequence the PI)\\n   \\n**Second part:**\\n\\n* Chaos Action:\\n    * start a process instance (PI), with a service task\\n    * disable the SST partitioning\\n    * restart the cluster\\n    * verify the readiness\\n    * verify that job is activatable\\n    * complete the job (in consequence the PI)\\n\\n\\n### Expected\\n\\nWhen operating a cluster, I can enable the SST partitioning without an impact on executing existing process instances. Existing PIs should still be executable and completable.\\n\\n### Actual\\n\\nAs linked above I used again our [benchmark/setup](https://github.com/camunda/zeebe/tree/main/benchmarks/setup) scripts to set up a cluster.\\n\\n#### First Part: Verify Steady state\\nTo verify the readiness and run all actions I used the [zbchaos](https://github.com/zeebe-io/zeebe-chaos/tree/zbchaos-v1.0.0) tool.\\n\\nVerifying readiness is fairly easy with zbchaos.\\n\\n```shell\\n$ zbchaos verify readiness -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPod zell-chaos-zeebe-0 is in phase Pending, and not ready. Wait for some seconds.\\n[...]\\nPod zell-chaos-zeebe-0 is in phase Running, and not ready. Wait for some seconds.\\nPod zell-chaos-zeebe-0 is in phase Running, and not ready. Wait for some seconds.\\nAll Zeebe nodes are running.\\n```\\n\\nWe then deploy the mentioned simple process model:\\n```shell\\n$ zbchaos deploy process -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPort forward to zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\nSuccessfully created port forwarding tunnel\\nDeploy file bpmn/one_task.bpmn (size: 2526 bytes).\\nDeployed process model bpmn/one_task.bpmn successful with key 2251799813685249.\\nDeployed given process model , under key 2251799813685249!\\n```\\n\\n#### First Part: Chaos Action\\n\\nAs the first step in the chaos action we create a process instance. \\n\\n```shell\\n$ zbchaos verify instance-creation -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPort forward to zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\nSuccessfully created port forwarding tunnel\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\nCreated process instance with key 2251799813685251 on partition 1, required partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\nNext, we enable the SST partitioning in our broker configuration, we can do this in the `values.yaml` file and run a `helm update`.\\n\\n```shell\\n$ diff ../default/values.yaml values.yaml \\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"true\\"\\n```\\n\\n```shell\\n$ make update\\nhelm upgrade --namespace zell-chaos zell-chaos zeebe-benchmark/zeebe-benchmark -f values.yaml\\nRelease \\"zell-chaos\\" has been upgraded. Happy Helming!\\nNAME: zell-chaos\\nLAST DEPLOYED: Mon May 15 15:54:24 2023\\nNAMESPACE: zell-chaos\\nSTATUS: deployed\\nREVISION: 2\\nNOTES:\\n# Zeebe Benchmark\\n\\nInstalled Zeebe cluster with:\\n\\n * 3 Brokers\\n * 2 Gateways\\n\\nThe benchmark is running with:\\n\\n * Starter replicas=0\\n * Worker replicas=0\\n * Publisher replicas=0\\n * Timer replicas=0\\n```\\n\\n> **Note**\\n> Changing the configmap doesn\'t restart pods! We need to delete all Zeebe pods, to apply the configuration.\\n\\n```shell\\n$ k delete pod -l app=camunda-platform\\npod \\"zell-chaos-zeebe-0\\" deleted\\npod \\"zell-chaos-zeebe-1\\" deleted\\npod \\"zell-chaos-zeebe-2\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-8j7d6\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\" deleted\\n```\\n\\nNext we can use `zbchaos verify readiness` again to await the readiness of the cluster.\\n```\\nAll Zeebe nodes are running.\\n```\\n\\nTaking a look at the logs of the broker we can also see that the broker configuration was correctly set:\\n\\n```\\n   \\\\\\"disableWal\\\\\\" : true,\\\\n      \\\\\\"enableSstPartitioning\\\\\\" : true\\\\n    }\\n```\\n\\n> **Note**\\n> Right now zbchaos can\'t complete an job (missing feature). We use zbctl for that, we need to port-forward to the gateway in order to send the commands.\\n\\n```shell\\n$ k port-forward svc/zell-chaos-zeebe-gateway 26500\\nForwarding from 127.0.0.1:26500 -> 26500\\nForwarding from [::1]:26500 -> 26500\\n\\n```\\n\\nActivating the right job.\\n```shell\\n$ zbctl --insecure activate jobs benchmark-task\\n{\\n  \\"jobs\\":  [\\n    {\\n      \\"key\\":  \\"2251799813685256\\",\\n      \\"type\\":  \\"benchmark-task\\",\\n      \\"processInstanceKey\\":  \\"2251799813685251\\",\\n      \\"bpmnProcessId\\":  \\"benchmark\\",\\n      \\"processDefinitionVersion\\":  1,\\n      \\"processDefinitionKey\\":  \\"2251799813685249\\",\\n      \\"elementId\\":  \\"task\\",\\n      \\"elementInstanceKey\\":  \\"2251799813685255\\",\\n      \\"customHeaders\\":  \\"{}\\",\\n      \\"worker\\":  \\"zbctl\\",\\n      \\"retries\\":  3,\\n      \\"deadline\\":  \\"1684173544716\\",\\n      \\"variables\\":  \\"{}\\"\\n    }\\n  ]\\n}\\n```\\nCompleting the job and the PI.\\n```shell\\n$ zbctl complete job 2251799813685256 --insecure\\nCompleted job with key \'2251799813685256\' and variables \'{}\'\\n```\\n\\n#### Second Part:\\n\\nCreate again a process instance `$ zbchaos verify instance-creation`\\n\\n```\\nCreated process instance with key 2251799813685263 on partition 1, required partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\nDisabling the configuration again, and running the update.\\n```shell\\n$ diff default/values.yaml zell-chaos/values.yaml \\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"false\\"\\n\\n\\n$ make update \\nhelm upgrade --namespace zell-chaos zell-chaos zeebe-benchmark/zeebe-benchmark -f values.yaml\\nRelease \\"zell-chaos\\" has been upgraded. Happy Helming!\\nNAME: zell-chaos\\nLAST DEPLOYED: Mon May 15 20:00:53 2023\\n...\\n\\n$ k delete pod -l app=camunda-platform\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n```\\n\\nAgain the job completion worked without problems (skipping here the port-forward and activate output)\\n\\n```\\n$ zbctl complete job 2251799813685268 --insecure\\nCompleted job with key \'2251799813685268\' and variables \'{}\'\\n```\\n\\n:white_check_mark: The experiment was successful. \\n\\n#### Further investigation\\n\\n![](general-after2.png)\\n\\nWhen running the experiment I also observed the metrics of the cluster and was not able to see any differences in the snapshot file counts, which we would expect on the SST partitioning (there should be more files).\\n\\nBefore the experiment:\\n![](snapshot-before.png)\\n\\nAfter the experiment, we still see that for each partition we have around ~6 files.\\n![](snapshot-after2.png)\\n\\nIn order to make sure whether the options have been applied correctly I investigated the RocksDB log files and option files.\\n\\nIn the current LOG file, we can see the current options printed, which is indeed the disabled partitioner. Since this is the default as well it is not a proof yet.\\n\\n```\\n2023/05/15-18:01:46.223234 139711509092096 [/column_family.cc:621] --------------- Options for column family [default]:\\n2023/05/15-18:01:46.223237 139711509092096               Options.comparator: leveldb.BytewiseComparator\\n2023/05/15-18:01:46.223239 139711509092096           Options.merge_operator: None\\n2023/05/15-18:01:46.223241 139711509092096        Options.compaction_filter: None\\n2023/05/15-18:01:46.223242 139711509092096        Options.compaction_filter_factory: None\\n2023/05/15-18:01:46.223244 139711509092096  Options.sst_partitioner_factory: None\\n```\\n\\nWhat we can see in the runtime folder of the partition is that there exist two Options files, an older one `OPTIONS-000014`, and a newer one `OPTIONS-000023`.\\n\\nThe older one contains the expected configuration for the SST partitioning:\\n\\n```shell\\n$ cat OPTIONS-000014 \\n[CFOptions \\"default\\"]\\n...\\n  sst_partitioner_factory={id=SstPartitionerFixedPrefixFactory;length=8;}\\n```\\n\\nThe most recent options file has the configuration set to null.\\n```shell\\n\\n$ cat OPTIONS-000023\\n[CFOptions \\"default\\"]\\n...\\nsst_partitioner_factory=nullptr\\n\\n```\\n\\nWe can see that the current snapshot only copied the most recent options file:\\n\\n```shell\\n$ ll ../snapshots/188-4-230-244\\ntotal 56\\ndrwxr-xr-x 2 root root 4096 May 15 18:13 ./\\ndrwxr-xr-x 3 root root 4096 May 15 18:13 ../\\n...\\n-rw-r--r-- 2 root root 7015 May 15 18:01 OPTIONS-000023\\n-rw-r--r-- 1 root root   92 May 15 18:13 zeebe.metadata\\n```\\n\\n### Conclusion\\n\\nWe were able to toggle the SST partitioning flag without problems back and forth. We were able to make still progress on an existing process instance, which we wanted to prove.\\n\\nNevertheless, we need to prove this once more for multiple process instances (100-1000 PIs), which cause or forces compaction of the SST files. Right now I\'m not 100% convinced whether this experiment was enough, but it was a good first step."},{"id":"/2023/04/06/gateway-termination","metadata":{"permalink":"/zeebe-chaos/2023/04/06/gateway-termination","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-04-06-gateway-termination/index.md","source":"@site/blog/2023-04-06-gateway-termination/index.md","title":"Gateway Termination","description":"In today\'s chaos day, we wanted to experiment with the gateway and resiliency of workers.","date":"2023-04-06T00:00:00.000Z","formattedDate":"April 6, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"resiliency","permalink":"/zeebe-chaos/tags/resiliency"}],"readingTime":7.24,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway Termination","date":"2023-04-06T00:00:00.000Z","categories":["chaos_experiment","gateway"],"tags":["availability","resiliency"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle"},"nextItem":{"title":"Recursive call activity","permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity"}},"content":"In today\'s chaos day, we wanted to experiment with the gateway and resiliency of workers.\\n\\nWe have seen in recent weeks some issues within our benchmarks when gateways have been restarted,\\nsee [zeebe#11975](https://github.com/camunda/zeebe/issues/11975).\\n\\nWe did a similar experiment [in the past](../2022-02-15-Standalone-Gateway-in-CCSaaS/index.md),\\ntoday we want to focus on self-managed ([benchmarks with our helm charts](https://helm.camunda.io/)).\\nIdeally, we can automate this as well soon.\\n\\nToday [Nicolas](https://github.com/npepinpe) joined me on the chaos day :tada: \\n\\n**TL;DR;** We were able to show that the workers (clients) can reconnect after a gateway is shutdown :white_check_mark:\\nFurthermore, we have discovered a potential performance issue on lower load, which impacts process execution latency ([zeebe#12311](https://github.com/camunda/zeebe/issues/12311)). \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe will use our [Zeebe benchmark helm charts](https://github.com/zeebe-io/benchmark-helm) to set up the test cluster, and\\nour helper scripts [here](https://github.com/camunda/zeebe/tree/main/benchmarks/setup).\\n\\n### Setup:\\n\\nWe will run with the default benchmark configuration, which means:\\n\\n * three brokers\\n * three partitions\\n * replication count three\\n * two gateways\\n\\nWe will run the benchmark with a low load, 10 process instances per second created and completed. For that,\\nwe deploy one starter and worker. This reduces the blast radius and allows us to observe more easily how the workers\\nbehave when a gateway is restarted.\\n\\nDuring the experiment, we will use our [grafana dashboard](https://github.com/camunda/zeebe/tree/main/monitor/grafana) to\\nobserve to which gateway the worker will connect and which gateway we need to stop/restart.\\n\\n\\n```shell\\nLAST DEPLOYED: Thu Apr  6 10:21:27 2023\\nNAMESPACE: zell-chaos\\nSTATUS: deployed\\nREVISION: 1\\nNOTES:\\n# Zeebe Benchmark\\n\\nInstalled Zeebe cluster with:\\n\\n * 3 Brokers\\n * 2 Gateways\\n\\nThe benchmark is running with:\\n\\n * Starter replicas=1\\n * Worker replicas=1\\n * Publisher replicas=0\\n * Timer replicas=0\\n```\\n\\n### Expected\\n\\n\\nWhen we terminate a gateway to which the worker has connected, **we expect** that the worker connects to the different\\nreplica and starts completing jobs again.\\n\\nThe performance drop is expected to be not significant, or at least should recover fast.\\n\\n### Actual\\n\\nWe will run the experiment in two ways, first via terminating the gateway (using [zbchaos](https://github.com/zeebe-io/zeebe-chaos/releases/tag/zbchaos-v1.0.0))\\nand later via scaling down the gateway deployment to one replica. \\n\\nWe want to verify whether this makes any difference, since terminating will cause Kubernetes to recreate immediately the pod.\\n\\n#### Termination\\n\\nBefore we start the experiment we check our current deployed state:\\n```shell\\n$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012860-zk72q     0/1     Completed   0          7m24s\\nelasticsearch-master-0                      1/1     Running     0          45m\\nelasticsearch-master-1                      1/1     Running     0          45m\\nelasticsearch-master-2                      1/1     Running     0          45m\\nleader-balancer-28012860-7cwmd              0/1     Completed   0          7m25s\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          45m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          45m\\nzell-chaos-zeebe-0                          1/1     Running     0          45m\\nzell-chaos-zeebe-1                          1/1     Running     0          45m\\nzell-chaos-zeebe-2                          1/1     Running     0          45m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-vs28f   1/1     Running     0          45m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          45m\\n\\n```\\n\\nVia our Grafana dashboard (and the gRPC metrics) we are able to track to which gateway the worker connects to:\\n\\n![grpc](grpc.png)\\n\\nIt is `zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f`.\\nVia zbchaos we can easily terminate the gateway (it will always take the first in the pod list).\\n\\n```shell\\n$ zbchaos terminate gateway \\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1680772377704 false false true false false 30 false -1 benchmark 30  }\\nTerminated zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f\\n```\\n\\nAfter terminating we can see that a new gateway pod has started.\\n```sh\\n$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012860-zk72q     0/1     Completed   0          13m\\nelasticsearch-master-0                      1/1     Running     0          52m\\nelasticsearch-master-1                      1/1     Running     0          52m\\nelasticsearch-master-2                      1/1     Running     0          52m\\nleader-balancer-28012860-7cwmd              0/1     Completed   0          13m\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          52m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          52m\\nzell-chaos-zeebe-0                          1/1     Running     0          52m\\nzell-chaos-zeebe-1                          1/1     Running     0          52m\\nzell-chaos-zeebe-2                          1/1     Running     0          52m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          33s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          52m\\n```\\n\\nIn the metrics, we can see that due to the restart, the throughput slightly dropped, but recovered pretty fast. The worker\\nwas able to connect to the different gateway. :white_check_mark:\\n\\n![restart](restart.png)\\n\\n> **Note**\\n> \\n> _In the panel `Pod Restarts` on the top right, we don\'t see any restarts and that is something\\nwe should always be aware of that the _metrics are just samples of data_. If a pod, like a gateway, restarts fast enough\\nand the metric collect interval is higher (per default we have ~30 s (?)) then you might not see a change._\\n\\n\\n\\n#### Scale down\\n\\nAs described earlier we wanted to verify whether it makes a difference if we scale down the replica instead of terminating/restarting\\nit, which causes restarting a new pod (which might get the same IP).\\n\\nFor scaling down Nicolas found this annotation: `controller.kubernetes.io/pod-deletion-cost`\\n\\n[That annotation allows giving hints to the schedule which pod to turn-down, because another pod might have a higher cost \\nto be deleted (this is of course best-effort).](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#pod-deletion-cost)\\n\\nThis means we edit one pod and gave the following annotation:\\n\\n```yaml\\nannotations:\\n  controller.kubernetes.io/pod-deletion-cost:  \\"-1\\"\\n```\\n\\nWe have similarly chosen the pod as we have seen above, based on the gRPC metrics.\\n\\nChecking the running pods and editing the correct gateway:\\n```shell\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          6m26s\\nelasticsearch-master-0                      1/1     Running     0          59m\\nelasticsearch-master-1                      1/1     Running     0          59m\\nelasticsearch-master-2                      1/1     Running     0          59m\\nleader-balancer-28012875-sctwq              0/1     Completed   0          6m26s\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          59m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          59m\\nzell-chaos-zeebe-0                          1/1     Running     0          59m\\nzell-chaos-zeebe-1                          1/1     Running     0          59m\\nzell-chaos-zeebe-2                          1/1     Running     0          59m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          8m29s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          59m\\n```\\n\\n\\nWhen I did the following I was wondering why it didn\'t scale down the deployment, one pod was recreated.\\n```shell\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k scale replicaset zell-chaos-zeebe-gateway-7bbdf9fd58 --replicas=1\\nWarning: spec.template.spec.containers[0].env[16].name: duplicate name \\"ZEEBE_LOG_LEVEL\\"\\nreplicaset.apps/zell-chaos-zeebe-gateway-7bbdf9fd58 scaled\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS            RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed         0          6m53s\\nelasticsearch-master-0                      1/1     Running           0          60m\\nelasticsearch-master-1                      1/1     Running           0          60m\\nelasticsearch-master-2                      1/1     Running           0          60m\\nleader-balancer-28012875-sctwq              0/1     Completed         0          6m53s\\nstarter-cb69c447f-l2zbh                     1/1     Running           0          60m\\nworker-cb7f7c469-qvqqv                      1/1     Running           0          60m\\nzell-chaos-zeebe-0                          1/1     Running           0          60m\\nzell-chaos-zeebe-1                          1/1     Running           0          60m\\nzell-chaos-zeebe-2                          1/1     Running           0          60m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   0/1     PodInitializing   0          7s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running           0          8m56s\\n```\\n\\n\\n> **Note:**\\n> \\n> During the experiment I learned that when you have deployed a [deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), you need to scale down the deployment, not the [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/).\\n> Otherwise your Kubernetes deployment controller will recreate the ReplicaSet in the next reconcile loop, which means you\\n> will have again the same replicas as defined in the deployment.\\n\\n\\n\\nSo correct is to scale down the deployment (!), if you ever wonder.\\n\\n\\n```shell\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k edit pod zell-chaos-zeebe-gateway-7bbdf9fd58-pkj48\\npod/zell-chaos-zeebe-gateway-7bbdf9fd58-pkj48 edited\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          10m\\nelasticsearch-master-0                      1/1     Running     0          63m\\nelasticsearch-master-1                      1/1     Running     0          63m\\nelasticsearch-master-2                      1/1     Running     0          63m\\nleader-balancer-28012875-sctwq              0/1     Completed   0          10m\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          63m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          63m\\nzell-chaos-zeebe-0                          1/1     Running     0          63m\\nzell-chaos-zeebe-1                          1/1     Running     0          63m\\nzell-chaos-zeebe-2                          1/1     Running     0          63m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   1/1     Running     0          3m40s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          12m\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k scale deployment zell-chaos-zeebe-gateway --replicas=1\\nWarning: spec.template.spec.containers[0].env[16].name: duplicate name \\"ZEEBE_LOG_LEVEL\\"\\ndeployment.apps/zell-chaos-zeebe-gateway scaled\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          10m\\nelasticsearch-master-0                      1/1     Running     0          64m\\nelasticsearch-master-1                      1/1     Running     0          64m\\nelasticsearch-master-2                      1/1     Running     0          64m\\nleader-balancer-28012875-sctwq              0/1     Completed   0          10m\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          64m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          64m\\nzell-chaos-zeebe-0                          1/1     Running     0          64m\\nzell-chaos-zeebe-1                          1/1     Running     0          64m\\nzell-chaos-zeebe-2                          1/1     Running     0          64m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   1/1     Running     0          4m\\n```\\n\\nWith that, we have only one gateway pod left, and all traffic goes to that gateway. Based on the metrics\\nwe can see that the workers recovered everytime when we restarted/terminated or scaled down.\\n\\n![activate](activate.png)\\n\\n![general](general.png)\\n\\nThe experiment itself succeeded :muscle: :white_check_marks:\\n\\n\\n## Found Bugs\\n \\n\\n### Zbchaos print verbose logs\\n\\nI realized that we still have [the issue with zbchaos](https://github.com/zeebe-io/zeebe-chaos/issues/323) which is printing verbose logs:\\n\\n```shell\\n$ zbchaos terminate gateway \\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1680772377704 false false true false false 30 false -1 benchmark 30  }\\nTerminated zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f\\n```\\n\\n### Segment creation impact\\n\\nDuring checking the metrics together with Nicolas, we realized that even on low load (10 PI/s) we have high spikes\\nin our processing execution latency.\\n\\n![latency](latency.png)\\n\\nThe spikes are going up to 1-1.5 seconds, while the avg is at 0.06s. This happens every 6 minutes.\\n\\nWe can see that the commit latency is as well at the same time high, which might be an issue because of the high IO.\\n\\n![commit](commit.png)\\n\\nWe first expected that to be related to snapshotting, but snapshots happen much more often.\\n\\n![snapshot](snapshot-count.png)\\n\\nInterestingly is that it seems to be related to our segment creation (again), even if we have \\nasync segment creation in our journal built recently. We need to investigate this further within [zeebe#12311](https://github.com/camunda/zeebe/issues/12311).\\n\\n![segment](segment.png)"},{"id":"/2023/02/23/Recursive-call-activity","metadata":{"permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-02-23-Recursive-call-activity/index.md","source":"@site/blog/2023-02-23-Recursive-call-activity/index.md","title":"Recursive call activity","description":"Long time no see. Happy to do my first chaos day this year. In the last week have implemented interesting features, which I would like to experiment with.","date":"2023-02-23T00:00:00.000Z","formattedDate":"February 23, 2023","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.03,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Recursive call activity","date":"2023-02-23T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Gateway Termination","permalink":"/zeebe-chaos/2023/04/06/gateway-termination"},"nextItem":{"title":"Message Correlation after Network Partition","permalink":"/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition"}},"content":"Long time no see. Happy to do my first chaos day this year. In the last week have implemented interesting features, which I would like to experiment with.\\n[Batch processing](https://github.com/camunda/zeebe/issues/11416) was one of them.\\n\\n**TL;DR;** Chaos experiment failed. :boom: Batch processing doesn\'t seem to respect the configured limit, which causes issues with processing and influences the health of the system. We found a bug :muscle:\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nIn today\'s chaos experiment, we want to experiment with [Batch processing](https://github.com/camunda/zeebe/issues/11416) and how it can handle error conditions, like deploying an endless recursive process model.\\n\\n![recursive process](call.png)\\n\\n### Expected\\n\\nWhen we deploy such a process model and create an instance of it, we expect that the execution is done endlessly. In normal process models with batch processing, the execution of a process instance is done until a wait state is reached. In this process model, there exists no wait state. To handle such cases, we have implemented a batch limit, which can be configured via [maxCommandsInBatch](https://github.com/camunda/zeebe/blob/main/dist/src/main/config/broker.standalone.yaml.template#L695). This configuration is by default set to 100 commands. Meaning the stream processor will process 100 commands until it stops, to make room for other things.\\n\\nWe expect that our limit handling steps in during the execution and we can execute also other instances or, cancel the problematic process instance. Furthermore, we expect to stay healthy, we should be able to update our health check continuously.\\n\\n### Actual\\n\\nBefore we can start with our experiment we need to start our benchmark Zeebe cluster. This has become easier now since I have written the last post. Previously we had to use the scripts and Makefile in the [zeebe/benchmark sub-directory](https://github.com/camunda/zeebe/tree/main/benchmarks/setup).\\n\\nWe have now provided new [Benchmark Helm charts](https://github.com/zeebe-io/benchmark-helm), based on our Camunda Platform Helm charts. They allow us to deploy a new zeebe benchmark setup via:\\n\\n```shell\\nkubectl create namespace zell-chaos # create a new namespace\\nkubens zell-chaos  # change context to a new namespace\\n# deploy zeebe benchmark cluster - without starter and worker\\nhelm install zell-chaos \\\\\\n    zeebe-benchmark/zeebe-benchmark \\\\\\n    --set starter.replicas=0 \\\\\\n    --set worker.replicas=0\\n```\\n\\n\\nTo deploy the model we can use [zbchaos v1.0.0](https://github.com/zeebe-io/zeebe-chaos/releases/tag/zbchaos-v1.0.0).\\n\\n\\n```shell\\n$ zbchaos deploy process --processModelPath call.bpmn \\n{1 LEADER -1 call.bpmn 10  msg false 1 LEADER -1 2 LEADER -1 1677157340943 false false true false false 30 false -1 benchmark 30  }\\nDeployed given process model call.bpmn, under key 2251799813685249!\\n```\\n\\n*Note: Looks like we have some left-over debug logs, which we should remove.*\\n\\nTo create an instance we can use:\\n\\n```shell\\n$ zbchaos verify instance-creation --bpmnProcessId super\\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1677157569058 false false true false false 30 false -1 super 30  }\\nThe steady state was successfully verified!\\n```\\n\\nAfter creating the instance we can observe the behavior of the Zeebe via [grafana](https://grafana.dev.zeebe.io/).\\n\\nWe can see that the processing starts immediately quite high and is continuously going on. \\n\\n![general](general.png)\\n\\n**We have two instances running, one on partition three and one on partition one.**\\n\\n_One interesting fact is that the topology request rate is also up to 0.400 per second, so potentially every 2.5 seconds we send a topology request to the gateway. But there is no application deployed that does this. [I have recently found out again](https://github.com/camunda/zeebe/pull/11599#discussion_r1109846523), that we have the Zeebe client usage in the gateway to request the topology. Might be worth investigating whether this is an issue._\\n\\nAfter observing this cluster for a while we can see that after around five minutes the cluster fails. The processing for the partitions breaks down to 1/10 of what was processed before. A bit later it looks like it tries to come back but, failed again.\\n\\n![fail-general](fail-general.png)\\n\\n_We can see in the metrics that in between also the balancing was triggered. A feature we have as part of our Benchmark Helm charts._\\n\\nThe logs (at stack driver) doesn\'t give us many insights, [except that we see that nodes becoming unhealthy](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22zell-chaos%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-broker%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22zell-chaos%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22;timeRange=2023-02-23T12:17:49.128812Z%2F2023-02-23T14:18:59.101Z;pinnedLogId=2023-02-23T13:13:40.945376476Z%2Fdr4gxdklsxtgx6h6;cursorTimestamp=2023-02-23T13:13:40.945376476Z?project=zeebe-io). Similar information we can see in the metrics, that followers are unhealthy.\\n\\n```shell\\nPartition-1 failed, marking it as unhealthy: Broker-2{status=HEALTHY}\\nDetected \'UNHEALTHY\' components. The current health status of components: [Partition-2{status=HEALTHY}, Partition-1{status=UNHEALTHY, issue=HealthIssue[message=null, throwable=null, cause=Broker-2-StreamProcessor-1{status=UNHEALTHY, issue=HealthIssue[message=actor appears blocked, throwable=null, cause=null]}]}, Partition-3{status=HEALTHY}]\\n```\\n\\nInteresting insights we can get in our new Batch processing metrics. We see that at the beginning we use our limit of 100 commands per batch, but soon as we start with the recursion we use an enormous high batch processing command count.\\n\\n![fail-batchprocessing.png](fail-batchprocessing.png)\\n\\nThe new sequence metric shows similar results, so there must be a problem with not respecting the limit.\\n\\n![sequencer](sequencer.png)\\n\\nWith this, I mark this chaos experiment as failed. We need to investigate this further and fix the related issue.:boom:\\n\\n## Found Bugs\\n\\n* [zbchaos logs debug message on normal usage](https://github.com/zeebe-io/zeebe-chaos/issues/323)\\n* [Every 2.5 seconds we send a topology request, which is shown in the metrics](https://github.com/camunda/zeebe/issues/11799)\\n* [Batch processing doesn\'t respect the limit](https://github.com/camunda/zeebe/issues/11798)"},{"id":"/2022/08/31/Message-Correlation-after-Network-Partition","metadata":{"permalink":"/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2022-08-31-Message-Correlation-after-Network-Partition/index.md","source":"@site/blog/2022-08-31-Message-Correlation-after-Network-Partition/index.md","title":"Message Correlation after Network Partition","description":"In the last weeks, we made several changes in our core components, we introduce some new abstractions, and changed how we communicate between partitions.","date":"2022-08-31T00:00:00.000Z","formattedDate":"August 31, 2022","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":9.97,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Message Correlation after Network Partition","date":"2022-08-31T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Recursive call activity","permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity"},"nextItem":{"title":"Bring Deployment distribution experiment back","permalink":"/zeebe-chaos/2022/08/02/deployment-distribution"}},"content":"In the last weeks, we made several changes in our core components, we introduce some new abstractions, and changed how we communicate between partitions.\\n\\nDue to these changes, we thought it might make sense to run some more chaos experiments in that direction and area since our benchmarks also recently found some interesting edge cases.\\n\\nToday we experimented with Message Correlation and what happens when a network partition disturbs the correlation process.\\n\\n**TL;DR;** The experiment was partially successful (after retry), we were able to publish messages during a network partition that have been correlated after the network partition. We need to verify whether we can also publish messages before a network partition and during the partition create the related instances.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nThe experiment is related to our previously described [deployment distribution experiment](../2022-08-02-deployment-distribution/index.md).\\n\\nWhen a user/client publishes a message, the message will be sent to a certain partition, based on the correlation key. There is some calculation going on related to the hashcode and the partition count.\\nThis calculation is deterministic in order to find later the message again if we reach a message catch event.\\n\\nA message can specify a time-to-live (TTL), [which allows buffering that message](https://docs.camunda.io/docs/components/concepts/messages/#message-buffering). If later a process instance is created and the TTL is not exceeded the message can be still correlated. The creation of process instances\\nhappens round-robin on the existing/available partitions (this is controlled by the gateway). When a process instance is created and reaches a message catch event it will be based on the correlation key search for a message on the expected partition. _Actually this happens based on subscriptions, for more details see the [docs](https://docs.camunda.io/docs/components/concepts/messages/#message-subscriptions) or the [ZEP-4](https://github.com/zeebe-io/enhancements/blob/master/ZEP004-wf-stream-processing.md#message-intermediate-catch-event)._ If the message still exists (TTL didn\'t expire) and this message [wasn\'t already correlated to the same process definition](https://docs.camunda.io/docs/components/concepts/messages/#message-cardinality) then it will be correlated.\\n\\nSince both partitions can be on different leader nodes this requires some network communication, which can be interrupted/disturbed.\\n\\n### Expected\\n\\nWe expect that if the network between the partition where the message was published and where the process instance was created is interrupted that no message correlation happens. But after the network recovers, we expect further that the message will be correlated and the process instance can continue.\\n\\n### Actual\\n\\nAs a setup, I installed our benchmarks, with Operate enabled.\\nThis allows us to also view the details in Operate.\\n\\n```shell\\n$ diff zeebe-values.yaml ../default/zeebe-values.yaml \\n5,8d4\\n<   identity:\\n<     auth:\\n<       enabled: false\\n< \\n28c24\\n<   containerSecurityContext:\\n---\\n>   podSecurityContext:\\n139c135\\n<   enabled: true\\n---\\n>   enabled: false\\n```\\n\\nDuring the experiment, it turned out that the `podSecurityContext` is outdated.\\n\\n#### Experiment Description\\n\\nSince we want to automate this experiment soon, or later I thought it would be a good idea to use the [create process instance with result](https://docs.camunda.io/docs/components/concepts/process-instance-creation/#create-and-await-results). We would start the following process:\\n\\n![msg-catch](msg-catch.png)\\n\\nBefore we start the process we need to publish the message to a certain partition and create a network partition between two partitions. After that, we can create the PI and verify that the message correlation shouldn\'t happen. Afterward, we would delete the network partition and verify that the process instance is completed.\\n\\n#### Details\\n\\nTo make the experiment easier to reproduce and allow us to experiment in different directions later as well I extend our new chaos cli (zbchaos), which I created during the last hack days. I will write a separate blog post about this tool soon.\\n\\n##### Message Publish\\n\\nI added a new feature ([PR #166](https://github.com/zeebe-io/zeebe-chaos/pull/166)) that allows us to publish a message to a specific partition:\\n\\n```sh\\n$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055796, which corresponds to partition: 3\\n```\\n\\n##### Extend Steady-state verification\\n\\nFor the steady-state verification, multiple enhancements have been added.\\n\\n1. Previously the `zbchaos` didn\'t allow us to create instances of specific models, which is now added as new feature ([PR #167](https://github.com/zeebe-io/zeebe-chaos/pull/167)).\\n2. In order to await the process instance completion a new flag was added `--awaitResult`, which allows us to await the PI completeness.\\n3. To make sure that our message can be correlated we have to set the right correlationKey/value. This means we need to create instances with certain variables, which is now possible as well (`--variables`).\\n\\n```shell\\n./zbchaos verify steady-state -h\\nVerifies the steady state of the Zeebe system.\\nA process model will be deployed and process instances are created until the required partition is reached.\\n\\nUsage:\\n  zbchaos verify steady-state [flags]\\n\\nFlags:\\n      --awaitResult               Specify whether the completion of the created process instance should be awaited.\\n  -h, --help                      help for steady-state\\n      --partitionId int           Specify the id of the partition (default 1)\\n      --processModelPath string   Specify the path to a BPMN process model, which should be deployed and an instance should be created of.\\n      --variables string          Specify the variables for the process instance. Expect json string.\\n\\nGlobal Flags:\\n  -v, --verbose   verbose output\\n\\n```\\n\\n#### Execution of the Experiment\\n\\n```shell\\n$./zbchaos verify readiness -v\\nConnecting to zell-chaos\\nAll Zeebe nodes are running.\\n```\\n\\nAfter checking the readiness we can check what the current topology is:\\n```shell\\n$ ./zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3\\n0         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\nWe can see that the leaders are well distributed. I pick partition 3 as our message publish partition, and partition 1 as our partition for the process instance. Since we can\'t control really the round-robin mechanism, we need to create multiple messages and multiple process instances (for each partition). During our experiment, we will only look at the instance on partition one.\\n\\n```shell\\n$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055745, which corresponds to partition: 3\\n[zell go-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055746, which corresponds to partition: 3\\n[zell go-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055747, which corresponds to partition: 3\\n```\\n\\nCreating the network partition:\\n\\n```shell\\n./zbchaos disconnect brokers --broker1PartitionId 3 --broker1Role LEADER --broker2PartitionId 1 --broker2Role LEADER -v\\n...\\nSuccessfully created port forwarding tunnel\\nFound Broker zell-chaos-zeebe-2 as LEADER for partition 3.\\nFound Broker zell-chaos-zeebe-0 as LEADER for partition 1.\\nExecute [\\"apt\\" \\"-qq\\" \\"update\\"] on pod zell-chaos-zeebe-2\\n...\\nDisconnect zell-chaos-zeebe-2 from zell-chaos-zeebe-0\\n...\\nDisconnect zell-chaos-zeebe-0 from zell-chaos-zeebe-2\\n```\\n\\nCreating the process instance and await the result:\\n\\n```shell\\n$ ./zbchaos verify steady-state --awaitResult --partitionId 1 --processModelPath ../msg-catch.bpmn --variables \'{\\"key\\":\\"2\\"}\'\\n\\n\\n```\\n\\nUnfortunately, I missed the verbose flag so we can\'t really see the output. But if failed later with:\\n\\n```shell\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=422, subject=command-api-1, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=1559826040}} to zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT15S\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=441, subject=command-api-2, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=-1786651527}} to zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT15S\\nEncountered an error during process instance creation. Error: rpc error: code = NotFound desc = Command \'CREATE_WITH_AWAITING_RESULT\' rejected with code \'NOT_FOUND\': Expected to find process definition with key \'2251799813685249\', but none found\\npanic: Expected to create process instance on partition 1, but timed out after 30s.\\n\\ngoroutine 1 [running]:\\ngithub.com/zeebe-io/zeebe-chaos/go-chaos/cmd.glob..func10(0x247f740?, {0x1758c60?, 0x7?, 0x7?})\\n\\t/home/zell/goPath/src/github.com/zeebe-io/zeebe-chaos/go-chaos/cmd/verify.go:97 +0x1c5\\ngithub.com/spf13/cobra.(*Command).execute(0x247f740, {0xc000426540, 0x7, 0x7})\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:876 +0x67b\\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x24808c0)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:990 +0x3bd\\ngithub.com/spf13/cobra.(*Command).Execute(...)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:918\\ngithub.com/zeebe-io/zeebe-chaos/go-chaos/cmd.Execute()\\n\\t/home/zell/goPath/src/github.com/zeebe-io/zeebe-chaos/go-chaos/cmd/root.go:61 +0x25\\nmain.main()\\n\\t/home/zell/goPath/src/github.com/zeebe-io/zeebe-chaos/go-chaos/main.go:8 +0x17\\n```\\nI retried it:\\n```shell\\n$ ./zbchaos verify steady-state --awaitResult --partitionId 1 --processModelPath ../msg-catch.bpmn --variables \'{\\"key\\":\\"2\\"}\' -v\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nDeploy file ../msg-catch.bpmn (size: 2980 bytes).\\nDeployed process model ../msg-catch.bpmn successful with key 2251799813685249.\\nCreate process instance with defition key 2251799813685249 [variables: \'{\\"key\\":\\"2\\"}\', awaitResult: true]\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=509, subject=command-api-1, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=1559826040}} to zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT15S\\nCreate process instance with defition key 2251799813685249 [variables: \'{\\"key\\":\\"2\\"}\', awaitResult: true]\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=530, subject=command-api-2, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=-1786651527}} to zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT14.999S\\nCreate process instance with defition key 2251799813685249 [variables: \'{\\"key\\":\\"2\\"}\', awaitResult: true]\\nEncountered an error during process instance creation. Error: rpc error: code = NotFound desc = Command \'CREATE_WITH_AWAITING_RESULT\' rejected with code \'NOT_FOUND\': Expected to find process definition with key \'2251799813685249\', but none found\\npanic: Expected to create process instance on partition 1, but timed out after 30s.\\n\\ngoroutine 1 [running]:\\ngithub.com/zeebe-io/zeebe-chaos/go-chaos/cmd.glob..func10(0x247f740?, {0x1758c60?, 0x8?, 0x8?})\\n\\t/home/zell/goPath/src/github.com/zeebe-io/zeebe-chaos/go-chaos/cmd/verify.go:97 +0x1c5\\ngithub.com/spf13/cobra.(*Command).execute(0x247f740, {0xc00007e500, 0x8, 0x8})\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:876 +0x67b\\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x24808c0)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:990 +0x3bd\\ngithub.com/spf13/cobra.(*Command).Execute(...)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:918\\ngithub.com/zeebe-io/zeebe-chaos/go-chaos/cmd.Execute()\\n\\t/home/zell/goPath/src/github.com/zeebe-io/zeebe-chaos/go-chaos/cmd/root.go:61 +0x25\\nmain.main()\\n\\t/home/zell/goPath/src/github.com/zeebe-io/zeebe-chaos/go-chaos/main.go:8 +0x17\\n```\\n\\nAnd got a similar exception. Taking a look at Operate we can see that process instances are created. It is likely that the await timed out since the message hasn\'t been correlated but the returned error is a bit unclear. Interesting is that on partition two the message is also not correlated.\\n\\n![operate](operate.png)\\n\\nRemoving the network partition:\\n\\n```shell\\n$ ./zbchaos connect brokers -v\\nConnecting to zell-chaos\\nExecute [\\"sh\\" \\"-c\\" \\"command -v ip\\"] on pod zell-chaos-zeebe-0\\n/usr/sbin/ip\\nExecute [\\"sh\\" \\"-c\\" \\"ip route | grep -m 1 unreachable\\"] on pod zell-chaos-zeebe-0\\nExecute [\\"sh\\" \\"-c\\" \\"ip route del unreachable 10.0.17.8\\"] on pod zell-chaos-zeebe-0\\nConnected zell-chaos-zeebe-0 again, removed unreachable routes.\\nExecute [\\"sh\\" \\"-c\\" \\"command -v ip\\"] on pod zell-chaos-zeebe-1\\nError on connection Broker: zell-chaos-zeebe-1. Error: Execution exited with exit code 127 (Command not found). It is likely that the broker was not disconnected or restarted in between.\\nExecute [\\"sh\\" \\"-c\\" \\"command -v ip\\"] on pod zell-chaos-zeebe-2\\nError on connection Broker: zell-chaos-zeebe-2. Error: Execution exited with exit code 127 (Command not found). It is likely that the broker was not disconnected or restarted in between.\\n``` \\n\\nIt looks like the Broker-2 was restarted in between, which is really the case if we check the `kubectl get pods`\\n```\\n[zell go-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-27698835-pwdjh     0/1     Completed   0          9m20s\\nelasticsearch-master-0                      1/1     Running     0          12m\\nelasticsearch-master-1                      1/1     Running     0          12m\\nelasticsearch-master-2                      1/1     Running     0          12m\\nzell-chaos-operate-64bbc6794d-vqtnc         1/1     Running     0          12m\\nzell-chaos-zeebe-0                          1/1     Running     0          12m\\nzell-chaos-zeebe-1                          1/1     Running     0          12m\\nzell-chaos-zeebe-2                          1/1     Running     0          2m48s\\nzell-chaos-zeebe-gateway-795f87fd64-c9mf4   1/1     Running     0          12m\\nzell-chaos-zeebe-gateway-795f87fd64-h5d9c   1/1     Running     0          12m\\nzell-chaos-zeebe-gateway-795f87fd64-nlr5v   1/1     Running     0          12m\\n```\\n\\nThe topology has also completely changed.\\n\\n```shell\\n$ ./zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3\\n0         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n1         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n```\\n\\nAfter connecting again the instances haven\'t been executed. My guess is that the TTL was already reached.\\n\\n##### Rerun\\n\\nI will disconnect again partitions one and three and publish a message to partition three. Afterward I will connect them again and see whether the message is correlated.\\n\\n```shell\\n./zbchaos disconnect brokers --broker1PartitionId 3 --broker1Role LEADER --broker2PartitionId 1 --broker2Role LEADER -v\\n./zbchaos publish message -v --partitionId 3\\n./zbchaos publish message -v --partitionId 3\\n./zbchaos publish message -v --partitionId 3\\n./zbchaos connect brokers -v\\n```\\n\\nTake a look at Operate again:\\n\\n![operate2](operate2.png)\\n\\nWe can see that the experiment was successful, and the message has been correlated even if they are published during a network partition. :tada:\\n\\n## Found Bugs\\n\\n* I learned that the go zeebe client, doesn\'t set a default TTL which was interesting to find out (and somehow unexpected).\\n* The zbchaos uses always the same port for connecting to the kubernetes and zeebe cluster, which makes it impossible to run multiple commands. We should use random ports to make this possible."},{"id":"/2022/08/02/deployment-distribution","metadata":{"permalink":"/zeebe-chaos/2022/08/02/deployment-distribution","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2022-08-02-deployment-distribution/index.md","source":"@site/blog/2022-08-02-deployment-distribution/index.md","title":"Bring Deployment distribution experiment back","description":"We encountered recently a severe bug zeebe#9877 and I was wondering why we haven\'t spotted it earlier, since we have chaos experiments for it. I realized two things:","date":"2022-08-02T00:00:00.000Z","formattedDate":"August 2, 2022","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":9.62,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Bring Deployment distribution experiment back","date":"2022-08-02T00:00:00.000Z","categories":["chaos_experiment","bpmn","deployment"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Message Correlation after Network Partition","permalink":"/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition"},"nextItem":{"title":"Standalone Gateway in CCSaaS","permalink":"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS"}},"content":"We encountered recently a severe bug [zeebe#9877](https://github.com/camunda/zeebe/issues/9877) and I was wondering why we haven\'t spotted it earlier, since we have chaos experiments for it. I realized two things:\\n\\n 1. The experiments only check for parts of it (BPMN resource only). The production code has changed, and a new feature has been added (DMN) but the experiments/tests haven\'t been adjusted.\\n 2. More importantly we disabled the automated execution of the deployment distribution experiment because it was flaky due to a missing standalone gateway in Camunda Cloud SaaS [zeebe-io/zeebe-chaos#61](https://github.com/zeebe-io/zeebe-chaos/issues/61). This is no longer the case, see [Standalone Gateway in CCSaaS](../2022-02-15-Standalone-Gateway-in-CCSaaS/index.md)\\n\\nOn this chaos day I want to bring the automation of this chaos experiment back to life. If I have still time I want to enhance the experiment. \\n\\n**TL;DR;** The experiment still worked, and our deployment distribution is still resilient against network partitions. It also works with DMN resources. I can enable the experiment again, and we can close [zeebe-io/zeebe-chaos#61](https://github.com/zeebe-io/zeebe-chaos/issues/61). Unfortunately, we were not able to reproduce [zeebe#9877](https://github.com/camunda/zeebe/issues/9877) but we did some good preparation work for it.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nTo recap, when a deployment is created by a client it is sent to the partition one leader. Partition one is in charge of distributing the deployment to the other partitions. That means it will send the deployment to the other partition leaders, this is retried as long no ACK was received from the corresponding partition leader.\\n\\n![deploymentDistribution](deploymentDistribution.png)\\n\\nWe already have covered that in more detail in another chaos day you can read [here](../2021-01-26-deployments/index.md).\\n\\n### Expected\\n\\n![](deploymentDistributionExperiment.png)\\n\\nAs you can see in the image we will create an asymmetric network partition and disconnect the partition one leader from partition three. That means the sending to partition three will not be possible. We use here an asymmetric network partition, in order to reduce the probability to cause a leader change. The partition one leader is a follower of partition three and will still receive heartbeats.\\n\\nAfter disconnecting the leaders we deploy multiple process versions and after connecting the leaders again we expect that the deployments are distributed. It is expected that we can create instances of the last version on all partitions.\\n\\nWe will run the existing experiment against the latest minor version, to verify whether the experiment still works. When we enable the experiment again for automation it will be executed against SNAPSHOT automatically.\\n\\n### Actual\\n\\n#### Setup\\n\\nAs a first step, we created a new Production-S cluster, which has three partitions, three nodes (brokers), and two standalone gateways. The Zeebe version was set to 8.0.4 (latest minor).\\n\\nIt was a while since I used the [chaostoolkit](https://chaostoolkit.org/) which is the reason I had to reinstall it again, which is quite a simple see [here](https://chaostoolkit.org/reference/usage/install/).\\n\\nTL;DR:\\n```sh\\npython3 -m venv ~/.venvs/chaostk\\nsource  ~/.venvs/chaostk/bin/activate\\npip install -U chaostoolkit\\nchaos --version\\n```\\n\\n#### Executing chaos toolkit\\n\\nAs mentioned, the deployment distribution was not enabled for Production-S clusters, which is currently the only configuration we test via [Zeebe Testbench](https://github.com/zeebe-io/zeebe-cluster-testbench). We have to use the experiment that is defined under [production-l/deployment-distribution](https://github.com/zeebe-io/zeebe-chaos/tree/master/chaos-workers/chaos-experiments/camunda-cloud/production-l/deployment-distribution), which is the same*.\\n\\n<sub>* That is not 100% true. During running the Production-l experiment I realized that it made some assumptions regarding the <a href=\\"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-workers/chaos-experiments/scripts/disconnect-leaders-one-way.sh#L19\\">partition count</a> which needs to be adjusted for the Production-S setup.</sub>\\n\\n```sh\\n chaos run production-l/deployment-distribution/experiment.json \\n[2022-08-02 09:35:25 INFO] Validating the experiment\'s syntax\\n[2022-08-02 09:35:25 INFO] Experiment looks valid\\n[2022-08-02 09:35:25 INFO] Running experiment: Zeebe deployment distribution\\n[2022-08-02 09:35:25 INFO] Steady-state strategy: default\\n[2022-08-02 09:35:25 INFO] Rollbacks strategy: default\\n[2022-08-02 09:35:25 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 09:35:25 INFO] Probe: All pods should be ready\\n[2022-08-02 09:35:25 INFO] Steady state hypothesis is met!\\n[2022-08-02 09:35:25 INFO] Playing your experiment\'s method now...\\n[2022-08-02 09:35:25 INFO] Action: Enable net_admin capabilities\\n[2022-08-02 09:35:26 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 09:35:26 INFO] Pausing after activity for 180s...\\n[2022-08-02 09:38:26 INFO] Probe: All pods should be ready\\n[2022-08-02 09:38:33 INFO] Action: Create network partition between leaders\\n[2022-08-02 09:38:34 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 09:38:34 INFO] Action: Deploy different deployment versions.\\n[2022-08-02 09:38:40 INFO] Action: Delete network partition\\n[2022-08-02 09:38:42 INFO] Probe: Create process instance of latest version on partition one\\n[2022-08-02 09:38:43 INFO] Probe: Create process instance of latest version on partition two\\n[2022-08-02 09:38:44 INFO] Probe: Create process instance of latest version on partition three\\n[2022-08-02 09:38:44 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 09:38:44 INFO] Probe: All pods should be ready\\n[2022-08-02 09:38:45 INFO] Steady state hypothesis is met!\\n[2022-08-02 09:38:45 INFO] Let\'s rollback...\\n[2022-08-02 09:38:45 INFO] No declared rollbacks, let\'s move on.\\n[2022-08-02 09:38:45 INFO] Experiment ended with status: completed\\n```\\n\\nBased on the tool output it looks like it succeed, to make sure it really worked, we will take a look at the logs in stackdriver.\\n\\nIn the following logs we can see that deployment distribution is failing for partition 3 and is retried, which is expected and what we wanted.\\n```\\n2022-08-02 09:38:53.114 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685347-3\'). Retrying\\n2022-08-02 09:38:53.115 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685347 was written on partition 3\\n2022-08-02 09:38:53.157 CEST zeebe Received new exporter state {elasticsearch=228, MetricsExporter=228} \\n2022-08-02 09:38:53.157 CEST zeebe Received new exporter state {elasticsearch=228, MetricsExporter=228}\\n2022-08-02 09:38:53.241 CEST zeebe Received new exporter state {elasticsearch=232, MetricsExporter=232}\\n2022-08-02 09:38:53.241 CEST zeebe Received new exporter state {elasticsearch=232, MetricsExporter=232}\\n2022-08-02 09:38:53.464 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685351-3\'). Retrying\\n2022-08-02 09:38:53.466 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685351 was written on partition 3\\n2022-08-02 09:38:54.216 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685353-3\'). Retrying\\n2022-08-02 09:38:54.218 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685353 was written on partition 3\\n2022-08-02 09:38:54.224 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685355-3\'). Retrying\\n2022-08-02 09:38:54.225 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685355 was written on partition 3\\n2022-08-02 09:38:55.055 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685359-3\'). Retrying\\n2022-08-02 09:38:55.057 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685359 was written on partition 3\\n2022-08-02 09:38:55.689 CEST zeebe Received new exporter state {elasticsearch=299, MetricsExporter=299}\\n2022-08-02 09:38:55.690 CEST zeebe Received new exporter state {elasticsearch=299, MetricsExporter=299}\\n2022-08-02 09:38:56.089 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685357-3\'). Retrying\\n2022-08-02 09:38:56.090 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685357 was written on partition 3\\n2022-08-02 09:38:56.272 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685363-3\'). Retrying\\n2022-08-02 09:38:56.273 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685363 was written on partition 3\\n2022-08-02 09:38:56.920 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685361-3\'). Retrying\\n2022-08-02 09:38:56.922 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685361 was written on partition 3\\n2022-08-02 09:39:08.369 CEST zeebe Received new exporter state {elasticsearch=252, MetricsExporter=252}\\n```\\n\\nAt some point, the retry stopped, and we can see in the experiment output that we were able to start process instances on all partitions. This is great because it means the experiment was successfully executed and our deployment distribution is failure tolerant.\\n\\n#### Enhancement\\n\\nAs described earlier the current experiment deploys a BPMN process model only. It looks like this:\\n\\n![v1](multiVersionModel.png)\\n\\nIn order to make DMN part of the experiment, we change the service task to a [Business Rule task](https://docs.camunda.io/docs/components/modeler/bpmn/business-rule-tasks/). \\n\\n![v2](multiVersionModelv2.png)\\n\\nThe decision is really simple and just defines a static input and returns that as output.\\n\\n![decision](decision.png)\\n\\nWhen we run our experiment and create process instances on all partitions the DMN needs to be available otherwise the execution would fail. Currently, we can\'t specify a specific version of the DMN in the Business Rule Task (always the latest will be executed). Because of that, we will not deploy different DMN model versions, since it is currently not that easy to verify whether the right version was chosen. \\n\\nAfter adjusting the model and adjusting the script, we run the experiment again.\\n\\n```\\n$ chaos run production-l/deployment-distribution/experiment.json \\n[2022-08-02 11:05:12 INFO] Validating the experiment\'s syntax\\n[2022-08-02 11:05:12 INFO] Experiment looks valid\\n[2022-08-02 11:05:12 INFO] Running experiment: Zeebe deployment distribution\\n[2022-08-02 11:05:12 INFO] Steady-state strategy: default\\n[2022-08-02 11:05:12 INFO] Rollbacks strategy: default\\n[2022-08-02 11:05:12 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 11:05:12 INFO] Probe: All pods should be ready\\n[2022-08-02 11:05:13 INFO] Steady state hypothesis is met!\\n[2022-08-02 11:05:13 INFO] Playing your experiment\'s method now...\\n[2022-08-02 11:05:13 INFO] Action: Enable net_admin capabilities\\n[2022-08-02 11:05:13 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 11:05:13 INFO] Pausing after activity for 180s...\\n[2022-08-02 11:08:14 INFO] Probe: All pods should be ready\\n[2022-08-02 11:08:14 INFO] Action: Create network partition between leaders\\n[2022-08-02 11:08:16 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 11:08:16 INFO] Action: Deploy different deployment versions.\\n[2022-08-02 11:08:25 INFO] Action: Delete network partition\\n[2022-08-02 11:08:27 INFO] Probe: Create process instance of latest version on partition one\\n[2022-08-02 11:08:27 INFO] Probe: Create process instance of latest version on partition two\\n[2022-08-02 11:08:28 INFO] Probe: Create process instance of latest version on partition three\\n[2022-08-02 11:08:29 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 11:08:29 INFO] Probe: All pods should be ready\\n[2022-08-02 11:08:29 INFO] Steady state hypothesis is met!\\n[2022-08-02 11:08:29 INFO] Let\'s rollback...\\n[2022-08-02 11:08:29 INFO] No declared rollbacks, let\'s move on.\\n[2022-08-02 11:08:29 INFO] Experiment ended with status: completed\\n```\\n\\nIt succeeded as well.\\n\\nTaking a look at Operate we can see some incidents.\\n\\n![dmn-error](dmn-error.png)\\n\\nIt seems the process instance execution runs into the Business Rule Task, but the DMN resource was not available on the partition. \\n\\n![dmn-retry](dmn-retry.png)\\n\\nAfter retrying in Operate the incident was resolved, which means the DMN resource was distributed at that time.\\n\\nWe can adjust the experiment further to await the result of the process execution, but I will stop here and leave that for a later point.\\n\\n#### Reproduce our bug\\n\\nThe current experiment didn\'t reproduce the bug in [zeebe#9877](https://github.com/camunda/zeebe/issues/9877), since the DMN resource has to be distributed multiple times. Currently, we create a network partition such that the distribution doesn\'t work at all. \\n\\n![](deploymentDistributionExperimentV2.png)\\n\\nIn order to reproduce our scenario, we can set the network partition in the other direction, such that the acknowledgment is not received by the leader one.\\n\\nAdjusting the experiment (script) like this:\\n\\n```diff\\n-retryUntilSuccess disconnect \\"$leader\\" \\"$leaderTwoIp\\"\\n+retryUntilSuccess disconnect \\"$leaderTwo\\" \\"$leaderIp\\"\\n```\\n\\nShould do the trick, but I was not yet able to reproduce the issue with 8.0.4. It seems we need to spend some more time reproducing the bug. But I think with today\'s changes we already did a good step in the right direction, and we can improve based on that. I will create a follow-up issue to improve our experiment.\\n\\n\\n## Further Work\\n\\nBased on today\'s outcome we can enable again the Deployment Distribution experiment for Production-S, such that is executed by Zeebe Testbench (our automation tooling). We can close [zeebe-io/zeebe-chaos#61](https://github.com/zeebe-io/zeebe-chaos/issues/61)\\n\\nWe should adjust our Chaos Worker implementation such that we also deploy DMN resources as we did in today\'s Chaos Day, since the scripts we changed aren\'t used in the automation.\\n\\n I will create a follow-up issue to improve our experiment, so we can reproduce the critical bug.\\n\\n## Found Bugs\\n\\n*none*"},{"id":"/2022/02/15/Standalone-Gateway-in-CCSaaS","metadata":{"permalink":"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2022-02-15-Standalone-Gateway-in-CCSaaS/index.md","source":"@site/blog/2022-02-15-Standalone-Gateway-in-CCSaaS/index.md","title":"Standalone Gateway in CCSaaS","description":"We recently introduced the Zeebe Standalone Gateway in CCSaaS. Today I wanted to do a first simple","date":"2022-02-15T00:00:00.000Z","formattedDate":"February 15, 2022","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Standalone Gateway in CCSaaS","date":"2022-02-15T00:00:00.000Z","categories":["chaos_experiment","gateway"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Bring Deployment distribution experiment back","permalink":"/zeebe-chaos/2022/08/02/deployment-distribution"},"nextItem":{"title":"High Snapshot Frequency","permalink":"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency"}},"content":"We recently introduced the Zeebe Standalone Gateway in CCSaaS. Today I wanted to do a first simple \\nchaos experiment with the gateway, where we just restart one gateway. \\n\\nIdeally in the future we could enable some gateway chaos experiments again, which we currently only support for [helm](https://github.com/zeebe-io/zeebe-chaos/tree/master/chaos-workers/chaos-experiments/helm).\\n\\n**TL;DR;** Our Camunda Cloud clusters can handle gateway restarts without issues. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nThis experiment is a simple restart / kill pod experiment. We want to verify that our cluster\\ncan still make progress even if a gateway is restarted / killed in between. Currently, we are running two gateway replicas in the CCSaaS. \\n\\nIn order to start with our experiment we created a new CCSaaS cluster with a `Production - S` plan, and the latest version (1.3.4). \\nTo run some load on the cluster we used the cloud benchmark deployments, which you can find [here](https://github.com/camunda-cloud/zeebe/tree/main/benchmarks/setup/cloud-default).\\nThe load was rather low with ~ 100 PI/s.\\n\\n### Expected\\n\\n_Hypothesis: When restarting / killing a zeebe standalone gateway we expect only a small \\nimpact on current requests, new requests should be routed to the right gateway and the cluster can\\nmake progress._\\n\\n\\n### Actual\\n\\nAfter creating the cluster and starting the benchmark we checked the current resource usage, to find a cluster which does most of the work. It looks like that the requests are well distributed.\\n```shell\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ k top pod\\nNAME                                                     CPU(cores)   MEMORY(bytes)   \\n...      \\nzeebe-gateway-6c9f95b557-f2zbf                           294m         407Mi           \\nzeebe-gateway-6c9f95b557-gk57z                           202m         396Mi\\n```\\n\\nWe deleted the first pod `zeebe-gateway-6c9f95b557-f2zbf` and observed the metrics.\\n```\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ k delete pod zeebe-gateway-6c9f95b557-f2zbf\\npod \\"zeebe-gateway-6c9f95b557-f2zbf\\" deleted\\n```\\n\\nWe can see that a new gateway pod is created quite fast.\\n\\n```\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ kgpo\\nNAME                                                     READY   STATUS             RESTARTS   AGE\\n...\\nzeebe-gateway-6c9f95b557-flgz6                           0/1     Running            0          16s\\nzeebe-gateway-6c9f95b557-gk57z                           1/1     Running            0          156m\\n```\\n\\nAs expected we see no high impact due to the restart. \\n\\n![](restart.png)\\n\\nJust out of interest I deleted all gateway pods:\\n\\n```shell\\n$ k delete pod -l app.kubernetes.io/component=standalone-gateway\\npod \\"zeebe-gateway-6c9f95b557-flgz6\\" deleted\\npod \\"zeebe-gateway-6c9f95b557-gk57z\\" deleted\\n```\\n\\nWe can see that the throughput goes almost directly down, but recovers again. It took ~4 min until we\\nreached the normal state (normal throughput of 100 PI/s) again. But we can also see that it is only a short period\\nof time, where nothing happens. \\n\\n![](restart2.png)\\n\\nIdeally we would define some anti-affinity on the gateway pods, to reduce the risk of losing all gateways at once.\\n\\n## Result\\n\\nWe were able to verify and proof our hypothesis.\\n\\n> _Hypothesis: When restarting / killing a zeebe standalone gateway we expect only a small\\nimpact on current requests, new requests should be routed to the right gateway and the cluster can\\nmake progress._\\n\\nAs we saw above the resources were almost equally used, which is in our normal zeebe benchmarks not the case. \\nIn the benchmarks we use the helm charts and there is no ingress controller enabled, \\nso we have no good load balancing of the incoming requests.\\n\\nThe benefit of the standalone gateway now stands out: losing one is not too problematic than one broker with an embedded gateway,\\nsince a broker takes longer to restart and is likely to be an leader for a partition, which will then also cause a leader change. \\nFurthermore, we can scale the gateways independently. \\n\\n## Found Issues\\n\\n### Optimize resources\\n\\nDuring experiment with the CCSaaS cluster I observed that the Optimize importer was crashlooping due to this load (PI 100/s).\\n\\n```shell\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ kgpo\\nNAME                                                     READY   STATUS             RESTARTS   AGE\\n...\\noptimize-deployment-importer-archiver-65679b6449-pb7kz   0/1     CrashLoopBackOff   5          128m\\n...\\n```\\n\\nChecking the pod shows:\\n```shell\\n    State:          Waiting\\n      Reason:       CrashLoopBackOff\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    3\\n      Started:      Tue, 15 Feb 2022 14:25:36 +0100\\n      Finished:     Tue, 15 Feb 2022 14:26:33 +0100\\n    Ready:          False\\n```\\n\\nChecking the logs we can see that it runs continuously out of memory.\\n```shell\\n\\nZell\ufffc  13 minutes ago\\n13:32:48.493 [ZeebeImportScheduler-1] WARN  org.elasticsearch.client.RestClient - request [POST http://elasticsearch:9200/zeebe-record-process-instance/_search?routing=2&typed_keys=true&max_concurrent_shard_requests=5&ignore_unavailable=false&expand_wildcards=open&allow_no_indices=true&ignore_throttled=true&request_cache=false&search_type=query_then_fetch&batched_reduce_size=512&ccs_minimize_roundtrips=true] returned 1 warnings: [299 Elasticsearch-7.16.2-2b937c44140b6559905130a8650c64dbd0879cfb \\"[ignore_throttled] parameter is deprecated because frozen indices have been deprecated. Consider cold or frozen tiers in place of frozen indices.\\"]\\n13:32:49.104 [ImportJobExecutor-pool-ZeebeProcessInstanceImportService-0] WARN  org.elasticsearch.client.RestClient - request [HEAD http://elasticsearch:9200/optimize-process-instance-benchmark?ignore_throttled=false&ignore_unavailable=false&expand_wildcards=open%2Cclosed&allow_no_indices=false] returned 1 warnings: [299 Elasticsearch-7.16.2-2b937c44140b6559905130a8650c64dbd0879cfb \\"[ignore_throttled] parameter is deprecated because frozen indices have been deprecated. Consider cold or frozen tiers in place of frozen indices.\\"]\\njava.lang.OutOfMemoryError: Java heap space\\nDumping heap to java_pid8.hprof ...\\nHeap dump file created [611503401 bytes in 1.070 secs]\\nTerminating due to java.lang.OutOfMemoryError: Java heap space\\n```\\n\\n### Gateway metrics\\n\\nIt looks like that in our latest ccsm helm charts, we no longer export the gateway metrics which we should fix.\\n\\n### Gateway Anti-affinity\\n\\nCurrently, we have no anti-affinity defined for the gateway, which could cause on preemption to take down all gateways."},{"id":"/2022/02/01/High-Snapshot-Frequency","metadata":{"permalink":"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2022-02-01-High-Snapshot-Frequency/index.md","source":"@site/blog/2022-02-01-High-Snapshot-Frequency/index.md","title":"High Snapshot Frequency","description":"Today we wanted to experiment with the snapshot interval and verify that a high snapshot frequency will not impact our availability (#21).","date":"2022-02-01T00:00:00.000Z","formattedDate":"February 1, 2022","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.52,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"High Snapshot Frequency","date":"2022-02-01T00:00:00.000Z","categories":["chaos_experiment","data"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Standalone Gateway in CCSaaS","permalink":"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS"},"nextItem":{"title":"Handling of Big Variables","permalink":"/zeebe-chaos/2022/01/19/big-variables"}},"content":"Today we wanted to experiment with the snapshot interval and verify that a high snapshot frequency will not impact our availability ([#21](https://github.com/zeebe-io/zeebe-chaos/issues/21)).\\n\\n\\n**TL;DR;** The chaos experiment succeeded :muscle: We were able to prove our hypothesis.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n### Snapshot Interval\\n\\nAs we can see in the [docs](https://docs.camunda.io/docs/self-managed/zeebe-deployment/operations/resource-planning/#snapshots) a snapshot is defined as:\\n\\n> A snapshot is a projection of all events that represent the current running state of the processes running on the partition. It contains all active data, for example, deployed processes, active process instances, and not yet completed jobs.\\n\\nPer default snapshots are taken every 5 minutes, by leaders and followers. If a follower is lagging behind (with replication) the leader will, after reaching a certain threshold, prefer to send the follower a snapshot instead of replicating X amount of records. We recently observed that this currently happens quite often, see [#8565](https://github.com/camunda-cloud/zeebe/issues/8565).\\n\\nThe snapshot interval can be changed via an environment variable: `ZEEBE_BROKER_DATA_SNAPSHOTPERIOD`\\n\\n### Expected\\n\\nWe expect that even if the snapshot interval is low (so the frequency of taking snapshot is high) we not run into any availability issues and the cluster should still be healthy. Lower snapshot interval might impact the performance, since taking a snapshot can take some time but other than that it shouldn\'t have any effect.\\n\\n### Actual\\n\\nAs usual, we run again two benchmarks to compare them. One base which has the [default benchmark configuration](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/setup/default) and one with a changed snapshot interval.\\n\\nFor the second benchmark we set the snapshot interval to one minute. Like this:\\n```\\nenv:\\n  ...\\n    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD\\n    value: \\"1m\\"\\n```\\n\\nThroughput wise we can see a small difference, but this might be more related that on the base benchmark one node is leader for all partitions.\\n\\n\\n| Base | Chaos |\\n|------|-------|\\n| ![](chaos-base-general.png) | ![](chaos-general.png) |\\n\\n\\nIn general the cluster with the small snapshot interval shows no negative effect. What we can see is that the install request rate increased. It seems to be currently have no affect, but it is likely that if more partitions are added it might become an issue.\\n\\n\\n| Base | Chaos |\\n|------|-------|\\n| ![](chaos-base-install-freq.png) | ![](chaos-install-freq.png) |\\n\\nFurther investigation needs to be done as part of [#8565](https://github.com/camunda-cloud/zeebe/issues/8565).\\n\\n\\n#### Smaller intervals\\n\\nThe smallest interval which Zeebe supports is `1m == 1 minute`. If we configure for example `1s`\\n\\n```\\nenv:\\n  ...\\n    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD\\n    value: \\"1s\\"\\n```\\n\\nWe see the following exception in the log and the broker fails to start.\\n\\n```\\njava.lang.IllegalArgumentException: Snapshot period PT1S needs to be larger then or equals to one minute.\\n```\\n\\n#### Bigger intervals\\n\\nIn order to verify how Zeebe reacts on a bigger snapshot interval we have set the interval to 30 minutes.\\n\\n```\\nenv:\\n  ...\\n    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD\\n    value: \\"30m\\"\\n```\\nIn general, it looked good. What we can see is that one node was restarted in between and took a while to come back. \\n\\n![](big-general.png)\\n\\n\\nThis is expected due to the high snapshot interval, but interesting to observe. The leader had no snapshot yet produced, which means it had to replicate all events to the restarted follower. Only if the follower catches up on all partitions its bootstrap process is complete, and it can mark itself as ready. As we see it can take a while if there is no snapshot available, since new records are incoming all the time. \\n\\nAfter the leader of partition two took a snapshot and the leader sent this snapshot to the follower, the follower were able to become ready.\\n\\nEven with a big snapshot interval we can see that as soon as a new snapshot is taken it is sent to the followers, which is suboptimal.\\n\\n![](big-install-freq.png)\\n\\nAn important thing to keep in mind when playing around with snapshots is the logstream/journal size. The journal is only compacted after taking a snapshot, if we take snapshot less frequent this means we clean up less frequent. The log can grow much bigger with big snapshot intervals.\\n\\n![](big-interval-log.png)\\n\\n\\n### Result\\n\\nThe chaos experiment succeeded :tada: We verified that a smaller snapshot interval has no negative impact on the cluster availability, at least for a small amount of partitions. \\n\\n## Found Bugs\\n\\n * Existing issue regarding the install requests [#8565](https://github.com/camunda-cloud/zeebe/issues/8565)"},{"id":"/2022/01/19/big-variables","metadata":{"permalink":"/zeebe-chaos/2022/01/19/big-variables","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2022-01-19-big-variables/index.md","source":"@site/blog/2022-01-19-big-variables/index.md","title":"Handling of Big Variables","description":"New Year;New Chaos","date":"2022-01-19T00:00:00.000Z","formattedDate":"January 19, 2022","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.29,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Handling of Big Variables","date":"2022-01-19T00:00:00.000Z","categories":["chaos_experiment","bpmn","variables"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"High Snapshot Frequency","permalink":"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency"},"nextItem":{"title":"Worker count should not impact performance","permalink":"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance"}},"content":"New Year;:tada:New Chaos:monkey:\\n\\nThis time I wanted to experiment with \\"big\\" variables. Zeebe supports a `maxMessageSize` of 4 MB, which is quite big. In general, it should be clear that using big variables will cause performance issues, but today I also want to find out whether the system can handle big variables (~1 MB) at all. \\n\\n**TL;DR;** Our Chaos experiment failed! Zeebe and Camunda Cloud is not able to handle (per default) big variables (~1 MB) without issues.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nNormally we run our benchmarks with ~32 KB payload size. This time we want to try out a payload size of ~1 MB and verify whether the system can handle such payload sizes. The payload we use can be found [here](pathname://big_payload.json). \\n\\nThe benchmark setup, is similar to default Zeebe benchmarks you can find [here](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/setup/default). To make it work and fair we updated the starter and worker resources for both, base and the chaos cluster.\\n\\n```diff\\ndiff --git a/benchmarks/setup/default/starter.yaml b/benchmarks/setup/default/starter.yaml\\nindex 78c6e81dbb..d0404d4d3e 100644\\n--- a/benchmarks/setup/default/starter.yaml\\n+++ b/benchmarks/setup/default/starter.yaml\\n@@ -30,11 +30,11 @@ spec:\\n             value: \\"warn\\"\\n         resources:\\n           limits:\\n-            cpu: 250m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 2Gi\\n           requests:\\n-            cpu: 250m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 2Gi\\n ---\\n apiVersion: v1\\n kind: Service\\ndiff --git a/benchmarks/setup/default/worker.yaml b/benchmarks/setup/default/worker.yaml\\nindex cd6f5ffeb6..05b195291f 100644\\n--- a/benchmarks/setup/default/worker.yaml\\n+++ b/benchmarks/setup/default/worker.yaml\\n@@ -31,11 +31,11 @@ spec:\\n             value: \\"warn\\"\\n         resources:\\n           limits:\\n-            cpu: 500m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 1Gi\\n           requests:\\n-            cpu: 500m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 1Gi\\n```\\n\\n### Expected\\n\\nIt is expected that the performance will drop, we formulate the following hypothesis.\\n\\n**Hypothesis: With a bigger payload size of e.g. 1 MB, Zeebe should be still able to handle process instances, maybe under a degraded performance, but in general the availability must not suffer from such a payload size.**\\n\\n\\n### Actual\\n\\n#### Base\\n\\nWe started a base benchmark with ~32 KB to verify how it looks like normally.\\n\\n![base](base-general.png)\\n\\n\\n#### Small Payload\\n\\nIn order to verify how Zeebe handles different payload, we first started with a small payload ~130 bytes, which is part of the Starter application (called `small_payload.json`). \\n\\n![small-payload](small-payload.png)\\n\\nWe can see that the system handles such payload without any issues, and we can reach ~190 process instances per second (PI/s).\\n\\n#### Big Payload\\n\\nAfter running with a small payload, we changed the payload to a size of ~1 MB. This immediately broke the standalone gateways.\\n\\n![big-payload](big-payload-starter-gw-restarts.png)\\n\\nThe gateways went out of memory (OOM) in a loop. No processing was made in this time.\\n\\n##### Increasing Resources\\n\\nIn order to continue the experiment and to verify how Zeebe itself can handle it, we increased the gateway resources.\\n\\n```diff\\ndiff --git a/benchmarks/setup/default/zeebe-values.yaml b/benchmarks/setup/default/zeebe-values.yaml\\nindex 371ba538dc..7a11c10366 100644\\n--- a/benchmarks/setup/default/zeebe-values.yaml\\n+++ b/benchmarks/setup/default/zeebe-values.yaml\\n@@ -38,10 +38,10 @@ gateway:\\n   resources:\\n     limits:\\n       cpu: 1\\n-      memory: 512Mi\\n+      memory: 4Gi\\n     requests:\\n       cpu: 1\\n-      memory: 512Mi\\n+      memory: 4Gi\\n```\\n\\nBut this doesn\'t help. The gateway went no longer OOM, but it was still not able to handle the payload.\\n\\n![increase-res](big-payload-increase-res.png)\\n\\nWe can see that in a short period of time some events have been processed (small spike in the \\"Current Events\\" panel), but this stopped quite fast again. In the gateway logs there are endless warnings:\\n\\n```\\nWarning 2022-01-20 10:09:32.644 CET zeebe-cluster-helm \\"Stream Error\\"\\nWarning 2022-01-20 10:09:56.847 CET zeebe-cluster-helm \\"Stream Error\\"\\n```\\n\\nWith an underlying exception: `io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place` \\n\\n<details>\\n<summary>Stacktrace</summary>\\n```\\nio.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\\n\\tat io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:172) ~[netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:481) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:105) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:357) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:1007) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:963) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:515) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:521) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler.closeStream(Http2ConnectionHandler.java:613) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onRstStreamRead(DefaultHttp2ConnectionDecoder.java:444) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onRstStreamRead(Http2InboundFrameLogger.java:80) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readRstStreamFrame(DefaultHttp2FrameReader.java:509) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:259) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:159) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:173) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:510) [netty-codec-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:449) [netty-codec-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:279) [netty-codec-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.73.Final.jar:4.1.73.Final]\\n\\tat java.lang.Thread.run(Unknown Source)\\n```\\n</details>\\n\\nOn the client side we can see that the Zeebe cluster seems to be unavailable.\\n\\n### Camunda Cloud\\n\\nWe wanted to verify how Camunda Cloud and our standard Cluster plan (GA Hardware Plan) handles such a payload. But the result was the same.\\n\\n![cc-general](cc-general.png)\\n\\nThe processing stopped quite fast due to OOM of the gateway. We can see that operate is also not able to handle such load.\\n\\n![failed-op](failed-operate.png)\\n\\nIn our console overview we see that all services (exception Zeebe) went unhealthy\\n\\n![console-healthy](console-healthy.png)\\n\\n### Result\\n\\n> *Hypothesis: With a bigger payload size of e.g. 1 MB Zeebe, should be still able to handle process instances, maybe under a degraded performance but in general the availability must not suffer from such a payload size.*\\n\\n**We were not able to validate our hypothesis, which means our chaos experiment failed!** :boom:\\n\\n### Found Bugs\\n\\nWe opened the following bug issues:\\n\\n * Gateway can\'t handle bigger payload sizes [#8621](https://github.com/camunda-cloud/zeebe/issues/8621)\\n\\n# Outtakes\\n\\nInteresting issues I run into when doing the chaos experiment, could be count as TIL events and mentioning them might help others.\\n\\n## Message pack is not valid\\n\\nWhen I first generated the JSON payload, it was an array on root level, which is not supported by Zeebe. \\n\\nI spent sometime to understand why I see no progress in processing. Taking a look at the gateway logs we can see:\\n\\n`\\"Expected to handle gRPC request, but messagepack property was invalid: io.camunda.zeebe.msgpack.MsgpackPropertyException: Property \'variables\' is invalid: Expected document to be a root level object, but was \'ARRAY\'\\"`\\n\\nOn the client side (if the logging is turned on, starter needs info logging) we see:\\n\\n`INVALID_ARGUMENT: Property \'variables\' is invalid: Expected document to be a root level object, but was \'ARRAY\'`\\n\\n\\n## Configure the Starter payload\\n\\nIn order to use different JSON payload for the starter we support a configuration on the starter application (`-Dapp.starter.payloadPath`). I had a lot of *\\"fun\\"* to find out the right syntax:\\n\\n * -Dapp.starter.payloadPath=\\"bpmn/small_payload.json\\" - *DOESN\'T WORK*\\n * -Dapp.starter.payloadPath=\\"/bpmn/small_payload.json\\" - *DOESN\'T WORK*\\n * -Dapp.starter.payloadPath=/bpmn/small_payload.json - *DOESN\'T WORK*\\n * -Dapp.starter.payloadPath=bpmn/big_payload.json - *WORKS*\\n\\n\\nSo be aware don\'t use `\\"` and no `/` in front, otherwise you might get a `java.io.FileNotFoundException: \\"bpmn/small_payload.json\\" (No such file or directory)` in your starter deployment and wonder why you see no progress."},{"id":"/2021/11/24/Worker-count-should-not-impact-performance","metadata":{"permalink":"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-11-24-Worker-count-should-not-impact-performance/index.md","source":"@site/blog/2021-11-24-Worker-count-should-not-impact-performance/index.md","title":"Worker count should not impact performance","description":"In this chaos day we experimented with the worker count, since we saw recently that it might affect the performance (throughput) negatively if there are more workers deployed. This is related to #7955 and #8244.","date":"2021-11-24T00:00:00.000Z","formattedDate":"November 24, 2021","tags":[{"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":2.5,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Worker count should not impact performance","date":"2021-11-24T00:00:00.000Z","categories":["chaos_experiment","bpmn","performance"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Handling of Big Variables","permalink":"/zeebe-chaos/2022/01/19/big-variables"},"nextItem":{"title":"Not produce duplicate Keys","permalink":"/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys"}},"content":"In this chaos day we experimented with the worker count, since we saw recently that it might affect the performance (throughput) negatively if there are more workers deployed. This is related to [#7955](https://github.com/camunda-cloud/zeebe/issues/7955) and [#8244](https://github.com/camunda-cloud/zeebe/issues/8244).\\n\\nWe wanted to prove, that even if we have more workers deployed the throughput of the process instance execution should not have an negative impact.\\n\\n**TL;DR;** We were not able to prove our hypothesis. Scaling of workers can have a negative impact on performance. Check out the [third chaos experiment](#third-chaos-experiment).\\n\\n\x3c!--truncate--\x3e\\n\\n## First Chaos Experiment\\n\\nWe run the first experiment with one partition, three brokers, one standalone gateway and one starter which creates 100 PI/s. In this experiment we deployed different zeebe [benchmarks](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks) with 4, 8 and 16 workers.\\n\\n### Expected\\n\\nThe workers should be able to complete all created instances and if the workers are scaled the throughput should remain.\\n\\n### Actual\\n\\n#### 4 Worker\\n\\n![worker-4-p1-general.png](worker-4-p1-general.png)\\n\\n#### 8 Worker\\n\\n![worker-8-p1-general.png](worker-8-p1-general.png)\\n#### 16 Worker\\n\\n![worker-16-p1-general.png](worker-16-p1-general.png)\\n\\n##### Result \\n\\nWhat we can see is that if we increase the worker number it will decrease the throughput, this might be explained with the case that we sent more activation requests / commands which need to be handled by the Brokers. We can see that the back pressure is higher with 16 workers.\\n\\n## Second Chaos Experiment\\n\\nWe will repeat first experiment with some changes, to the partition count. We will now use three partitions.\\n\\n### Expected\\n\\nThe workers should be able to complete all created instances and if the workers are scaled the throughput should remain or be better.\\n\\n### Actual\\n\\n#### 4 Worker\\n\\n![worker-4-p3-general.png](worker-4-p3-general.png)\\n\\n#### 8 Worker\\n\\n![worker-8-p3-general.png](worker-8-p3-general.png)\\n\\n#### 16 Worker\\n\\n![worker-16-p3-general.png](worker-16-p3-general.png)\\n\\n##### Result \\n\\nIn this experiment we can see that all benchmarks reach almost the same throughput, since we not really stressing the system and have enough resources to work with. There is no backpressure at all. In the next experiment we will increase the load.\\n\\n## Third Chaos Experiment\\n\\nWe will repeat second experiment with some changes to the instance creation count. We will now start 300 process instances per second, with one starter.\\n\\n### Expected\\n\\nThe workers should be able to complete most of the instances, we would expect that with more workers we would be able to complete more instances and have no negative impact on the system.\\n\\n### Actual\\n\\n#### 4 Worker\\n\\n![worker-4-p3-300-general.png](worker-4-p3-300-general.png)\\n\\n#### 8 Worker\\n\\n![worker-8-p3-general.png](worker-8-p3-300-general.png)\\n\\n#### 16 Worker\\n\\n![worker-16-p3-general.png](worker-16-p3-300-general.png)\\n\\n##### Result \\n\\nBetween eight and four workers we see the expected difference, that more workers increases the throughput and we are able to complete more instances in a second. The result of 16 workers is completely unexpected. We observed that after short time frame the completion throughput completely droped, and only process instances are created. \\n\\nWe were able to reproduce this behavior, which shows the weakness again.\\n\\n![worker-16-p3-general-reproduce.png](worker-16-p3-300-general-reproduce.png)\\n\\nWe were not able to prove our hypothesis, that scaling of workers has no negative impact on performance.\\n\\n## Further Analysis\\n\\nWe created a bug issue to analyze and fix this weakness [#8267](https://github.com/camunda-cloud/zeebe/issues/8267)."},{"id":"/2021/11/11/Not-produce-duplicate-Keys","metadata":{"permalink":"/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-11-11-Not-produce-duplicate-Keys/index.md","source":"@site/blog/2021-11-11-Not-produce-duplicate-Keys/index.md","title":"Not produce duplicate Keys","description":"Due to some incidents and critical bugs we observed in the last weeks, I wanted to spent some time to understand the issues better and experiment how we could detect them. One of the issue we have observed was that keys were generated more than once, so they were no longer unique (#8129). I will describe this property in the next section more in depth.","date":"2021-11-11T00:00:00.000Z","formattedDate":"November 11, 2021","tags":[{"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":5.425,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Not produce duplicate Keys","date":"2021-11-11T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Worker count should not impact performance","permalink":"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance"},"nextItem":{"title":"Throughput on big state","permalink":"/zeebe-chaos/2021/10/29/Throughput-on-big-state"}},"content":"Due to some incidents and critical bugs we observed in the last weeks, I wanted to spent some time to understand the issues better and experiment how we could detect them. One of the issue we have observed was that keys were generated more than once, so they were no longer unique ([#8129](https://github.com/camunda-cloud/zeebe/issues/8129)). I will describe this property in the next section more in depth.\\n\\n\\n**TL;DR;** We were able to design an experiment which helps us to detect duplicated keys in the log. Further work should be done to automate such experiment and run it agains newer versions.\\n\\n\x3c!--truncate--\x3e\\n\\n## Unique Keys\\n\\nIn Zeebe each element must have an cluster wide unique key. This is a property we expect in several areas inside, but also outside of Zeebe (external services). We can call it an invariant we have to guarantee.\\n\\nIn order to have cluster wide unique key\'s we encode the partition id in the key\'s. You can check this [code](https://github.com/camunda-cloud/zeebe/blob/develop/protocol/src/main/java/io/camunda/zeebe/protocol/Protocol.java#L71-L73) for more details. Furthermore only the Leader (of a partition) are in charge and allowed to generate new keys. If a fail-over happens the new leader need to continue with generating new keys, he has to resume where the other Leader have left-of. \\n\\nThe last part was exactly the issue we had, see [#8129](https://github.com/camunda-cloud/zeebe/issues/8129). The new leader generated key\'s, which have been already generated by the previous leader. This caused inconsistency in our internal state, but also in external services like Operate.\\n\\n## Chaos Experiment\\n\\nThis time it is a bit different approach, since we know it will *fail* and we want to examine how we can detect the weakness.\\nIn order to understand how we can detect and prevent/verify that this will not happen again, I want to run an experiment against 1.2.0. Later we should try to automate this experiment and run it against newer versions.\\n\\nAs described [above](#unique-keys) the issue was caused due to a fail-over, to be more specific, the Follower has already reached the end of the log (replayed all events) and on fail-over he had nothing to replay. In order to reproduce this situation we will do the following experiment:\\n\\n  1. Deploy an simple process model, with start and end event.\\n  2. Start an instance and await the result `zbctl create instance \\"simpleProcess\\" --withResult --insecure`\\n  3. Wait a short period of time\\n  4. Restart the current Leader `k delete pod zell-chaos-zeebe-2`\\n  5. Wait until a new Leader is choosen\\n  6. Start an instance and await the result `zbctl create instance \\"simpleProcess\\" --withResult --insecure`\\n\\nAfter doing this we should copy the data from the new Leader and exermine the log (with [zdb](https://github.com/Zelldon/zdb)). Alternatively, we could also verify the exporter log in elastic.\\n\\nIn general it should be clear that if we take a look at the log the key can appear multiple times, but they should only reference one single entity/element.\\n\\nA key is normally generated when processing a command and written as follow up event the first time to the log. For example when we process the `ACTIVATE_ELEMENT` command we write the key of the new entity on `ELEMENT_ACTIVATED` to the log.\\nKeys are assigned for other types of entities as well, like process instance creations, deployments etc. Here it works similar.\\n\\nIf the follow-up event contains a key which doesn\'t occurre the first time, then we have a problem, and this is how we want to detect it.\\n\\n### Expected\\n\\nTo follow the chaos engineering approach we will define here a hypothesis, even if we know it might not true for 1.2.0. But later we can reuse it for newer version, because here it should apply.\\n\\n*We expect even if we processed all records and have a fail-over that the new leader should not generate the same keys again.*\\n\\n### Actual\\n\\nWe started a benchmark with version 1.2.0, three brokers and one partition to reduce the scope and followed the described experiment above.\\n\\n\\nThe second process instance creation returned a key (`2251799813685258`) which was incremented by one, compared to the other process instance (`2251799813685257`). Here we already knew that our experiment caused the issue. Because, multiple new entities are generated during processing of a process instance and we awaited the result, at the first process instance creation. This means we have generated already more than one key (on the old leader), which hasn\'t been picked up by the new leader.\\n\\nWe copyed the data from the new Leader and exermined the log via `zdb`. First we printed the complete log into a separate file as json:\\n\\n```sh\\n  zdb log print -p raft-partition/partitions/1 | jq > data.json\\n```\\n\\nThis makes it human readable and also processable by other tools, like `jq`. After checking the file we already found one duplicate.\\n\\nThe first key was part of index 12 in term one and the duplicate was part of index 20 in term 2 (new leader term)\\n\\n```json\\n {\\n      \\"index\\": 12,\\n      \\"term\\": 1,\\n      \\"entries\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685256,\\n            \\"parentElementInstanceKey\\": -1,\\n            \\"flowScopeKey\\": -1,\\n            \\"parentProcessInstanceKey\\": -1,\\n            \\"elementId\\": \\"simpleProcess\\",\\n            \\"bpmnElementType\\": \\"PROCESS\\"\\n          },\\n          \\"timestamp\\": 1636627776427,\\n          \\"position\\": 26,\\n          \\"valueType\\": \\"PROCESS_INSTANCE\\",\\n          \\"intent\\": \\"ACTIVATE_ELEMENT\\",\\n          \\"recordType\\": \\"COMMAND\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 25,\\n          \\"key\\": 2251799813685256\\n        },\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685256,\\n            \\"variables\\": {}\\n          },\\n          \\"timestamp\\": 1636627776427,\\n          \\"position\\": 27,\\n          \\"valueType\\": \\"PROCESS_INSTANCE_CREATION\\",\\n          \\"intent\\": \\"CREATED\\",\\n          \\"recordType\\": \\"EVENT\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 25,\\n          \\"key\\": 2251799813685257\\n        }\\n      ]\\n    },\\n```\\n\\n```json\\n {\\n      \\"index\\": 20,\\n      \\"term\\": 2,\\n      \\"entries\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685257,\\n            \\"parentElementInstanceKey\\": -1,\\n            \\"flowScopeKey\\": -1,\\n            \\"parentProcessInstanceKey\\": -1,\\n            \\"elementId\\": \\"simpleProcess\\",\\n            \\"bpmnElementType\\": \\"PROCESS\\"\\n          },\\n          \\"timestamp\\": 1636627887305,\\n          \\"position\\": 46,\\n          \\"valueType\\": \\"PROCESS_INSTANCE\\",\\n          \\"intent\\": \\"ACTIVATE_ELEMENT\\",\\n          \\"recordType\\": \\"COMMAND\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 45,\\n          \\"key\\": 2251799813685257\\n        },\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685257,\\n            \\"variables\\": {}\\n          },\\n          \\"timestamp\\": 1636627887305,\\n          \\"position\\": 47,\\n          \\"valueType\\": \\"PROCESS_INSTANCE_CREATION\\",\\n          \\"intent\\": \\"CREATED\\",\\n          \\"recordType\\": \\"EVENT\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 45,\\n          \\"key\\": 2251799813685258\\n        }\\n```\\n\\nSince reading and checking the complete file is a bit hard and error prone, we tried to exermine the data with `jq`. This worked quite well, we were able to detect the duplicates with an `jq` expression.\\n\\n```sh\\ncat data.json | jq -r \'.records[] | select(.entries  != null) |  .entries[] | select (.intent == \\"ELEMENT_ACTIVATED\\" or .intent == \\"CREATED\\") | .key\' data.json | sort | uniq -c -d | awk \'{print $2}\'\\n\\n2251799813685257\\n2251799813685258\\n2251799813685263\\n2251799813685264\\n```\\n\\nThis expression we can use for further experiments and for automating such."},{"id":"/2021/10/29/Throughput-on-big-state","metadata":{"permalink":"/zeebe-chaos/2021/10/29/Throughput-on-big-state","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-10-29-Throughput-on-big-state/index.md","source":"@site/blog/2021-10-29-Throughput-on-big-state/index.md","title":"Throughput on big state","description":"In this chaos day we wanted to prove the hypothesis that the throughput should not significantly change even if we have bigger state, see zeebe-chaos#64","date":"2021-10-29T00:00:00.000Z","formattedDate":"October 29, 2021","tags":[{"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":3.145,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Throughput on big state","date":"2021-10-29T00:00:00.000Z","categories":["chaos_experiment","bpmn","processing"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Not produce duplicate Keys","permalink":"/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys"},"nextItem":{"title":"Recovery (Fail Over) time","permalink":"/zeebe-chaos/2021/10/05/recovery-time"}},"content":"In this chaos day we wanted to prove the hypothesis that the throughput should not significantly change even if we have bigger state, see [zeebe-chaos#64](https://github.com/zeebe-io/zeebe-chaos/issues/64)\\n\\nThis came up due observations from the [last chaos day](/2021-10-05-recovery-time/index.md). We already had a bigger investigation here [zeebe#7955](https://github.com/camunda-cloud/zeebe/issues/7955). \\n\\n**TL;DR;** We were not able to prove the hypothesis. Bigger state, more than 100k+ process instances in the state, seems to have an big impact on the processing throughput.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nSimilar to the [last chaos day](/2021-10-05-recovery-time/index.md) we set up three brokers, with one partition and replication factor three. \\n\\nAs first part of the experiment we start some amount of instances, afterwards we want to complete them with our workers. Based on the [last chaos day](/2021-10-05-recovery-time/index.md) we know what we can complete ~100 process instances with one worker and a capacity of `12`. An example worker configuration can be found [here](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/worker.yaml)\\n\\nWe changed the following:\\n\\n```sh\\n$ diff default/worker.yaml zell-chaos/worker.yaml \\n11c11\\n<   replicas: 12\\n---\\n>   replicas: 1\\n26c26\\n<               -Dapp.worker.capacity=120\\n---\\n>               -Dapp.worker.capacity=12\\n```\\n\\n### Experiment One\\n\\nAs first experiment we start with creating `100.000` instances, afterwards we start the worker.\\n\\nIn order to do that easily we can configure the [starter](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/starter.yaml) and reduce the rate to `100` and set the duration limit to `1000`. This means it will run for `1000` second and start each second `100` instances (which makes `100.000`).\\n\\n```shell\\n$ diff default/starter.yaml zell-chaos/starter.yaml \\n24,26c24,26\\n<               -Dapp.starter.rate=200\\n<               -Dapp.starter.durationLimit=0\\n---\\n>               -Dapp.starter.rate=100\\n>               -Dapp.starter.durationLimit=1000\\n```\\n\\n#### Expected\\n\\nWe expected that we can create `100.000` instances and can complete them with a rate of `~100`, as we have seen in other experiments.\\n\\n#### Actual\\n\\nWe were able to create all instances without any issues.\\n\\n![](exp1-100k-instances.png)\\n\\nWe were able to complete all instances without any issues and the throughput was ~100 completion per second.\\n\\n![](exp1-completion.png)\\n\\nWe can see that the snapshot was at some point ~650 MB big.\\n\\n![](exp1-state.png)\\n\\n\\n### Experiment Two\\n\\nAs second experiment we wanted to increase the state by factor `10`, which means `1.000.000` instances in the state and then start the worker.\\n\\nIn order to do that we changed the following in the starter configuration:\\n\\n```shell\\n$ diff default/starter.yaml zell-chaos/starter.yaml \\n24,26c24,26\\n<               -Dapp.starter.rate=200\\n<               -Dapp.starter.durationLimit=0\\n---\\n>               -Dapp.starter.rate=100\\n>               -Dapp.starter.durationLimit=10000\\n```\\n\\n#### Expected\\n\\nWe expect that we can create `1.000.000` instances and can complete them with a rate of `~100`, as we have seen in other experiments.\\n\\n#### Actual\\n\\nWe were **not** able to create all instances without issues.\\n\\n![](exp2-starting-general.png)\\n\\nWe see that at some point the throughput drops and backpressure seem to kick in. When this happens the state is only a bit bigger than in the previous experiment\\n\\n![](exp2-state.png)\\n\\nThe running instances seem to be around ~170K at this time.\\n\\n![](exp2-running.png)\\n\\n\\nWe can see that the processing queue increased at the same time and reached a critical point (over 150 records in the backlog).\\n\\n![](exp2-queue.png)\\n\\nThe overall latency seems to increase after this time (which makes sense)\\n\\n![](exp2-latency.png)\\n\\nWe haven\'t started the workers for this experiment, since the throughput already break before.\\n\\n### Experiment Three\\n\\nIn order to make sure that this is related to the state we run an experiment with the [simpleStarter](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/simpleStarter.yaml). This starter starts a process, which contains only a start and end event.\\n\\nWe let this starter run for more than one day and haven\'t experienced any issues on this one.\\n\\n![](exp3-general.png)\\n\\n## Result\\n\\nAs written above the throughput seem to break, after we reach a certain state size.\\n\\nIt might be just a trigger to get the system into stumblling, which means: after one thing takes a bit longer the processing queue gets longer and the processor is not able to catch up any more. This causes then backpressure to kick in etc.\\n\\nI think we need to further investigate this."},{"id":"/2021/10/05/recovery-time","metadata":{"permalink":"/zeebe-chaos/2021/10/05/recovery-time","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-10-05-recovery-time/index.md","source":"@site/blog/2021-10-05-recovery-time/index.md","title":"Recovery (Fail Over) time","description":"In the last quarter we worked on a new \\"feature\\" which is called \\"building state on followers\\". In short,","date":"2021-10-05T00:00:00.000Z","formattedDate":"October 5, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.895,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Recovery (Fail Over) time","date":"2021-10-05T00:00:00.000Z","categories":["chaos_experiment","fail_over"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Throughput on big state","permalink":"/zeebe-chaos/2021/10/29/Throughput-on-big-state"},"nextItem":{"title":"Old-Clients","permalink":"/zeebe-chaos/2021/09/23/Old-Clients"}},"content":"In the last quarter we worked on a new \\"feature\\" which is called \\"building state on followers\\". In short,\\nit means that the followers apply the events to build there state, which makes regular snapshot \\nreplication unnecessary and allows faster role transition between Follower-to-Leader. In this chaos\\nday I wanted to experiment a bit with this property, we already did some benchmarks [here](https://github.com/camunda-cloud/zeebe/issues/7515).\\nToday, I want to see how it behaves with larger state (bigger snapshots), since this needed to be\\ncopied in previous versions of Zeebe, and the broker had to replay more than with the newest version.\\n\\nIf you want to now more about build state on followers check out the [ZEP](https://github.com/zeebe-io/enhancements/blob/master/ZEP007-build-state-on-followers.md)\\n\\n**TL;DR;** In our experiment we had almost no downtime, with version 1.2, the new leader was very fast able to pick up the next work (accept new commands). \\n\\n\x3c!--truncate--\x3e\\n\\n## First Chaos Experiment\\n\\nWe will run two benchmarks one with 1.1 version and one with 1.2, to compare the differences between \\nthe versions. We will run three brokers, with one partition and replication factor three. \\n\\nIn order to build up state we run the `starter` with a `durationLimit`, example cfg:\\n\\n```shell\\n            value: >-\\n              -Dapp.brokerUrl=zell-chaos-12-zeebe-gateway:26500\\n              -Dapp.starter.rate=100\\n              -Dapp.starter.durationLimit=1000\\n              -Dzeebe.client.requestTimeout=62000\\n              -XX:+HeapDumpOnOutOfMemoryError\\n```\\n\\nThis means that we run a rate of 100 PI/s creations over 1000 seconds. We expect at the end around \\n100.000 PI, which should be enough to simulate a \\"big state\\". \\n\\nAfter executing the starters we can see in the metrics the running instances:\\n\\n![instances](instances.png)\\n\\nAnd that the snapshot is around 600 to 700 MB.\\n\\n![snapshot](snapshot.png)\\n\\n### Expected\\n\\nWe expect that if we restart the current leader that a new leader is fast (under seconds) able to\\ntake over and continues the work. The version 1.2 should perform here much better than 1.1.\\n\\n### Actual\\n\\nJust normal bootstrap takes some time, on version 1.1:\\n\\n![base-start-up](base-start-up.png)\\n\\nFor version 1.2:\\n\\n![12-start-up](12-start-up.png)\\n\\nAfter running the starters for a certain duration and restarting the leader we can see that \\nthe processor recovery takes by *factor 10* longer on version 1.1. Unfortunately, we have not the\\nleader transition metric in that version to compare against.\\n\\n| **Version** | **1.1**  |  **1.2**  |\\n|---|---|---|\\n| Recovery  | [![base-recovery](base-recovery.png)](base-recovery.png) | [![12-recovery](12-recovery.png)](12-recovery.png) | \\n| General  | [![base-recovery-general](base-recovery-general.png)](base-recovery-general.png)  | [![12-recovery-general](12-recovery-general.png)](12-recovery-general.png)  | \\n\\n*Sorry for the small pictures*\\n\\nIn general what we have seen is that it is not so easy to compare if there is no longer load on the\\nsystem, which is the reason I did a second experiment with: A) \\"big state\\" and B) steady load.\\n\\n## Second Chaos Experiment\\n\\nSimilar setup to the first experiment, but additionally after the \\"big state\\" is reached a steady\\nload is put on the system. One starter with a rate of 100 PI/s and one worker completing some jobs.\\n\\nWith that setup we want to verify how it affects the system if now a leader change happens.\\n\\n### Expected\\n\\nSimilar to above expect that if we restart the current leader that a new leader is fast \\n(under seconds) able to take over and continues the work. The version 1.2 should perform here much\\nbetter than 1.1.\\n\\n### Actual\\n\\n\\n| **Version** | **1.1**  |  **1.2**  |\\n|---|---|---|\\n| Recovery  | [![base-general-state-and-throughput-recover-time.png](base-general-state-and-throughput-recover-time.png)](base-general-state-and-throughput-recover-time.png) | [![12-general-state-and-throughput-recover-time.png](12-general-state-and-throughput-recovery-time.png)](12-general-state-and-throughput-recovery-time.png) | \\n| General  | [![base-general-state-and-throughput-recover-general.png](base-general-state-and-throughput-recover-general.png)](base-general-state-and-throughput-recover-general.png)  | [![12-general-state-and-throughput-recover-general.png](12-general-state-and-throughput-recovery-general.png)](12-general-state-and-throughput-recovery-general.png)  |\\n\\nAfter running the experiment again, this time with load, we can see that the version 1.1 took almost\\n2 minutes! The newest Zeebe version (1.2), with building state on followers, took ~80 milliseconds!\\n\\nWe can see this much better also in the processing and throughput metrics on version 1.1 we have ~2\\nminutes gap.\\n\\n![base-general-state-and-throughput-recover-general-zoom.png](base-general-state-and-throughput-recover-general-zoom.png)\\n\\nThe exporters can recover a bit faster than the processing, but we are for a while not able to accept\\nany commands.\\n\\nIn version 1.2 on the other hand we are able to almost immediately continue with the processing, some\\nmetrics are not even able to show a gap in between, like the current events.\\n\\n![12-general-state-and-throughput-recover-general-zoom.png](12-general-state-and-throughput-recovery-general-zoom.png)\\n\\n# Result\\n\\nIn general, we were able to show that the new approach of building state on followers, gives us an\\nexcellent benefit in transitioning between Follower and Leader. Furthermore, it allows us to handle\\nmuch larger state, since this doesn\'t need to be replicated on a regular basis.\\n\\n# Found Bugs\\n\\n## Running Instances\\n\\nWhen experimenting with the clusters, building the state and deploying the steady load I \\naccidentally deployed to many workers. This caused to complete all existing running instances. The \\nissues here is that on the new leader the metric is zero, which results in a negative metric. \\n\\n![broken-metric](broken-metric.png)\\n\\nMore problematic is actually that if you than build state again, you might reach the zero and if you\\nobserve the cluster you can\'t be sure what the actual count of instances are. This makes the metric\\nkind of useless.\\n\\n![broken-metric](broken-metric-zero.png)\\n\\n## Performance\\n\\nDuring the experimenting it looked like that the performance of 1.2 degraded compared to 1.1. At the\\nend I had on each benchmark one starter with 100 PI/s and one worker with capacity 12.\\n\\nWith version 1.1 it looked like we reached ~100 PI/s created/completed\\n![base-general-state-and-throughput-recovery-general-perf.png](base-general-state-and-throughput-recovery-general-perf.png)\\n\\nWith version 1.2 we just reached ~30, which means it reduced by factor 3.\\n![12-general-state-and-throughput-recovery-general-perf.png](12-general-state-and-throughput-recovery-general-perf.png)\\n\\nI think we need to verify whether this is really the case.\\n\\n**Update:**\\n\\nI run again a benchmark for both versions, with one worker and one starter. It showed no significant \\ndifference on throughput.\\n\\n| **Version** | **1.1**  |  **1.2**  |\\n|---|---|---|\\n| Performance  | [![perf-11](perf-11.png)](perf-11.png) | [![perf-12](perf-12.png)](perf-12.png) |\\n\\nMy current assumption is that it was related to the previous build up state and switching between \\ndifferent worker configurations etc. Let us see whether we can observe this again.\\n\\n**Update 2:**\\n\\nThe second benchmark failed several days again, without any intervention. I investigated that issue further and it seem to be related to frequent install requests, which are sent by the leader. See for more information the related issue https://github.com/camunda-cloud/zeebe/issues/7955"},{"id":"/2021/09/23/Old-Clients","metadata":{"permalink":"/zeebe-chaos/2021/09/23/Old-Clients","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-09-23-Old-Clients/index.md","source":"@site/blog/2021-09-23-Old-Clients/index.md","title":"Old-Clients","description":"It has been awhile since the last post, I\'m happy to be back.","date":"2021-09-23T00:00:00.000Z","formattedDate":"September 23, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":2.61,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Old-Clients","date":"2021-09-23T00:00:00.000Z","categories":["chaos_experiment","clients"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Recovery (Fail Over) time","permalink":"/zeebe-chaos/2021/10/05/recovery-time"},"nextItem":{"title":"Slow Network","permalink":"/zeebe-chaos/2021/07/06/Slow-Network"}},"content":"It has been awhile since the last post, I\'m happy to be back.\\n\\nIn today\'s chaos day we want to verify the hypothesis from [zeebe-chaos#34](https://github.com/zeebe-io/zeebe-chaos/issues/34) that old \\nclients can\'t disrupt a running cluster.\\n\\nIt might happen that after upgrading your Zeebe to the newest shiny version, you might forget to \\nupdate some of your workers or starters etc. This should normally not an issue since Zeebe is \\nbackwards compatible, client wise since 1.x. But what happens when older clients are used. Old \\nclients should not have a negative effect on a running cluster.\\n\\n**TLDR** Older clients (0.26) have no negative impact on a running cluster (1.2), and clients after \\n1.x are still working with the latest version. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe will run a simple setup with, three nodes and three partitions (replication factor 3). The \\nversion we use is the latest release candidate (1.2.0). Normally we run a load of 200 process \\ninstances per second (pi/s) on our benchmarks. This time we will put a load of 100 pi/s to get\\nsomething running and start an old starter (v0.26.x) with the same frequency. Later we will scale\\nthe old starter to see whether it makes any effect.\\n\\n### Expected\\n\\nWe expect that we can start and complete the 100 pi/s, since we can normally run 200 pi/s.\\n\\n### Actual\\n\\nThe cluster was first started with starters of the same version, and we saw a stable load of ~100 \\nprocess instances completed per second. After starting the old starters (with version 0.26.3), we\\ncan\'t observe any difference. \\n\\nInteresting is even when scaling the starters up to 10 replicas, which means 1000 PI creations per \\nsecond, it doesn\'t seem to make any effect. *Side note:* The \\n[starters](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/project) have been\\nmodified, such they only start instances without deploying the model.\\n\\n![old26-general](old26-general.png)\\n\\nThe drops we see in the processing are related to restart\'s.\\n\\nThe gateway and grpc metrics doesn\'t indicate that more requests are sent. \\n\\n![old26-grpc](old26-grpc.png)\\n![old26-gateway](old26-gateway.png)\\n\\nIf we take a look in the clients log, we can see that the request are failing because the RPC Method names have been changed between 0.26 and 1.0. \\n\\n```shell\\njava.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNIMPLEMENTED: Method not found: gateway_protocol.Gateway/CreateWorkflowInstance\\n\\t...\\nCaused by: io.grpc.StatusRuntimeException: UNIMPLEMENTED: Method not found: gateway_protocol.Gateway/CreateWorkflowInstance\\n\\t...\\n```\\n\\nIt seems this kind of \\"old\\" requests can be blocked quite early in the request chain to make no effect. \\n\\nIn order to experiment a bit further I created a starter image with version 1.0 to see whether this still works with our newest release candidate 1.2.\\n\\nWe can see in the metrics right after starting the starter that the throughput goes up and we can reach our 200 pi/s.\\n\\n![old10-general](old10-general.png)\\n\\nWe run the benchmark overnight, and we haven\'t seen any issues. Be aware that the throughput is calculated over the 24h which makes is lower than 200.\\n\\n![general](general.png)\\n\\nFurthermore, taking a look at the resource consumption, especially at the gateway, gives no evidence that something wrong is going on.\\n\\n![res](res.png)\\n\\n### Result\\n\\nWe were able to confirm the hypothesis written in [zeebe-chaos#34](https://github.com/zeebe-io/zeebe-chaos/issues/34), that an old client can\'t disrupt a running cluster. \\n\\n## Found Bugs\\n\\nNone this time :)"},{"id":"/2021/07/06/Slow-Network","metadata":{"permalink":"/zeebe-chaos/2021/07/06/Slow-Network","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-07-06-Slow-Network/index.md","source":"@site/blog/2021-07-06-Slow-Network/index.md","title":"Slow Network","description":"On a previous Chaos Day we played around with ToxiProxy , which allows injecting failures on the network level. For example dropping packages, causing latency etc.","date":"2021-07-06T00:00:00.000Z","formattedDate":"July 6, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Slow Network","date":"2021-07-06T00:00:00.000Z","categories":["chaos_experiment","broker","network","leader"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Old-Clients","permalink":"/zeebe-chaos/2021/09/23/Old-Clients"},"nextItem":{"title":"Full Disk Recovery","permalink":"/zeebe-chaos/2021/06/08/Full-Disk"}},"content":"On a previous [Chaos Day](/2020-10-06-toxi-proxy/index.md) we played around with [ToxiProxy](https://github.com/Shopify/toxiproxy) , which allows injecting failures on the network level. For example dropping packages, causing latency etc.\\n\\nLast week [@Deepthi](https://github.com/deepthidevaki) mentioned to me that we can do similar things with [tc](https://man7.org/linux/man-pages/man8/tc.8.html), which is a built-in linux command. Today I wanted to experiment with latency between leader and followers using `tc`.\\n\\n**TL;DR;** The experiment failed; With adding 100ms network delay to the Leader we broke the complete processing throughput. :boom:\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe want to experiment with network latency and what kind of effect has a slow network on the cluster. \\n\\n### Hypothesis\\n\\nWe expect that we can handle certain network latency, due to our heartbeat and election time timeouts. After, reaching the deadlines we expect fail overs and followers which are lagging behind.\\n\\nThis means under a certain threshold we should be able to still process user commands, with slightly delay but without real issues. After reaching the deadline and the fail overs, it should be possible to continue, since we will add only to one node the delay.\\n\\n### Experiment\\n\\n#### TC\\n\\nTC is a built in linux command, the manpage summarizes it as `tc - show / manipulate traffic control settings`.\\n\\nIn order to [add delay to a network interface](https://netbeez.net/blog/how-to-use-the-linux-traffic-control/), we can run the following:\\n\\n```shell\\ntc qdisc add dev eth0 root netem delay 200ms\\n```\\n\\nMore details ([taken from the blog post](https://netbeez.net/blog/how-to-use-the-linux-traffic-control/)):\\n```\\nqdisc: modify the scheduler (aka queuing discipline)\\nadd: add a new rule\\ndev eth0: rules will be applied on device eth0\\nroot: modify the outbound traffic scheduler (aka known as the egress qdisc)\\nnetem: use the network emulator to emulate a WAN property\\ndelay: the network property that is modified\\n200ms: introduce delay of 200 ms\\n```\\n\\nAdding this kind of rule means that we add a delay to all outgoing connections, which are going over this network interface.\\n\\n#### Actual\\n\\nIn order to reduce the blast radius we will run the experiment with one partition on a three broker cluster (replication factor 3).\\n\\n##### Steady State\\n\\n As we can see in the benchmark we are able to reach in avg. ~77 process instance creation and completions per second.\\n\\n![base](general.png)\\n\\nThe process instance execution time (from start to end) is under 1 second.\\n![base](latency.png)\\n\\nThe commit latency is about 100 ms.\\n![base](commit-lat.png)\\n\\nWe can see that one of the followers is a bit lagging behind, but not too far.\\n\\n![base](raft-follower.png)\\n\\n##### 100 ms\\n\\nIn the first iteration of the experiment we added a 100 ms delay to the leaders outgoing traffic. \\n\\n```sh\\n[zell zell-chaos-day/ cluster: zeebe-cluster ns:zell-chaos-day]$ k exec -it zell-chaos-day-zeebe-gateway-579c76978f-npz2k -- zbctl status --insecure\\nCluster size: 3\\nPartitions count: 1\\nReplication factor: 3\\nGateway version: 1.1.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - zell-chaos-day-zeebe-0.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy  Broker 1 - zell-chaos-day-zeebe-1.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy  Broker 2 - zell-chaos-day-zeebe-2.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Leader, Healthy\\n```\\n\\nBased on the `zbctl` output, or the grafana dashboard we can find out who is the leader. Note that the output of `zbctl` looks still a bit broken, related issue [#6692](https://github.com/camunda-cloud/zeebe/issues/6692).\\n\\nWith the following commands we can add the delay of 100 ms to the leaders outgoing traffic.\\n```sh\\n[zell zell-chaos-day/ cluster: zeebe-cluster ns:zell-chaos-day]$ k exec -it zell-chaos-day-zeebe-2 -- bash\\nroot@zell-chaos-day-zeebe-2:/usr/local/zeebe# tc qdisc add dev eth0 root netem delay 100ms\\nroot@zell-chaos-day-zeebe-2:/usr/local/zeebe# tc -s qdisc\\nqdisc noqueue 0: dev lo root refcnt 2 \\n Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) \\n backlog 0b 0p requeues 0\\nqdisc netem 8001: dev eth0 root refcnt 2 limit 1000 delay 100.0ms\\n Sent 11635496 bytes 10086 pkt (dropped 0, overlimits 0 requeues 0) \\n backlog 339773b 77p requeues 0\\n```\\n\\nAlmost immediately we see a drop in our general section of the Grafana Dashboard.\\n\\n![base](100ms-general.png)\\n\\nThe backpressure increased significantly. As expected the commit latency increased.\\n\\n![base](100ms-commit-lat.png)\\n\\nThe processing latency as well.\\n\\n![base](100ms-latency.png)\\n\\nIt was unexpected that the throughput breaks down so much. We can see in the send request that a lot of the requests are ended with timeouts or with resource exhausted.\\n\\n![base](100ms-grpc.png)\\n![base](100ms-gateway.png)\\n\\nInteresting is that no worker is able to activate nor complete any job. This cause increasing of the running process instances, so the state is growing.\\n\\n![base](100ms-running-proc.png)\\n\\nTaking a look at the raft metrics we see that this already caused some heartbeat misses, but no leader change.\\n\\n![base](100ms-heartbeats.png)\\n\\n\\n![base](100ms-follower.png)\\n\\nThe follower is now lagging far more behind. We can see in the logs that the Leader tries to send `InstallRequests`, but these are also timing out.\\n\\n```shell\\nRaftServer{raft-partition-partition-1} - InstallRequest{currentTerm=1, leader=2, index=1173627, term=1, version=1, chunkId=HeapByteBuffer{position=0, remaining=10, limit=10, capacity=10, mark=java.nio.HeapByteBuffer[pos=0 lim=10 cap=10], hash=1744147670}, nextChunkId=HeapByteBuffer{position=0, remaining=7, limit=7, capacity=7, mark=java.nio.HeapByteBuffer[pos=0 lim=7 cap=7], hash=1283029304}, data=HeapByteBuffer{position=0, remaining=10626817, limit=10626817, capacity=10626817, mark=java.nio.HeapByteBuffer[pos=0 lim=10626817 cap=10626817], hash=1083445787}, initial=false, complete=false} to 1 failed: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request ProtocolRequest{id=489616, subject=raft-partition-partition-1-install, sender=zell-chaos-day-zeebe-2.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26502, payload=byte[]{length=10647731, hash=1655826849}} to zell-chaos-day-zeebe-1.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26502 timed out in PT5S\\"\\n```\\n\\nThe follower is starting regularly elections, but is not able to overturn the existing leader.\\n\\n\\n\\nIn general, in the Gateway logs we can see the start of the delay injection quite good. \\n\\n```shell\\nD 2021-07-06T10:05:51.263195Z Received REACHABILITY_CHANGED for broker 2, do nothing.\\nD 2021-07-06T10:05:53.264480Z Received REACHABILITY_CHANGED for broker 1, do nothing.\\nD 2021-07-06T10:05:54.184981Z Received REACHABILITY_CHANGED for broker 2, do nothing.\\nD 2021-07-06T10:05:54.213904Z Received REACHABILITY_CHANGED for broker 1, do nothing.\\nD 2021-07-06T10:05:54.361408Z Received REACHABILITY_CHANGED for broker 0, do nothing.\\nD 2021-07-06T10:05:54.986270Z Received REACHABILITY_CHANGED for broker 0, do nothing.\\nD 2021-07-06T10:05:56.617134Z Received REACHABILITY_CHANGED for broker 2, do nothing.\\nD 2021-07-06T10:05:57.072707Z Expected to handle gRPC request, but request timed out between gateway and broker\\nD 2021-07-06T10:05:57.126207Z Expected to handle gRPC request, but request timed out between gateway and broker\\nD 2021-07-06T10:05:57.157683Z Expected to handle gRPC request, but request timed out between gateway and broker\\n```\\n\\nSimilar logs we see on the broker side.\\n\\nIn general, we can say the experiment failed. The cluster was not able to run our normal workload. It seem to behave quite bad, but there were no leader change at all.\\n\\n##### 250 ms\\n\\nIn order to verify whether 250 ms, will cause a leader election we reconfigured the delay. It looked quite similar, performance wise, but the heartbeats for one of the followers increased. Still it was not enough to cause a leader change.\\n\\nAfter several minutes (~30) of running this configuration we were able to observe that one of the other followers missed heartbeats as well. This finally caused a leader change.\\n\\n\\n![base](250ms-raft.png)\\n\\nThe throughput came back, it was similar to what is was before.\\n\\n![base](250ms-general.png)\\n\\nThe processing execution latency was higher than usual.\\n\\n![base](250ms-latency.png)\\n\\nSimilar to the commit latency, interesting to see such an affect caused by a follower with network issues.\\n\\n![base](250ms-commit-lat.png)\\n\\n### Result\\n\\nAs written before the experiment failed, the hypothesis was not met. We were not able to add latency to the network, which is much lower than our deadlines (heartbeat timeout is 2.5 seconds), without causing any harm to our processing throughput/latency.\\n\\nStill we had some interesting observations we can use for our next experiments and insight which we need to consider on setting up Zeebe in environment where the network might be unreliable/slow."},{"id":"/2021/06/08/Full-Disk","metadata":{"permalink":"/zeebe-chaos/2021/06/08/Full-Disk","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-06-08-Full-Disk/index.md","source":"@site/blog/2021-06-08-Full-Disk/index.md","title":"Full Disk Recovery","description":"On this chaos day we wanted to experiment with OOD recovery and ELS connection issues. This is related to the following issues from our hypothesis backlog: zeebe-chaos#32 and zeebe-chaos#14. This time @Nico joined me.","date":"2021-06-08T00:00:00.000Z","formattedDate":"June 8, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.125,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Full Disk Recovery","date":"2021-06-08T00:00:00.000Z","categories":["chaos_experiment","broker","disk"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Slow Network","permalink":"/zeebe-chaos/2021/07/06/Slow-Network"},"nextItem":{"title":"Time travel Experiment","permalink":"/zeebe-chaos/2021/05/25/Reset-Clock"}},"content":"On this chaos day we wanted to experiment with OOD recovery and ELS connection issues. This is related to the following issues from our hypothesis backlog: [zeebe-chaos#32](https://github.com/zeebe-io/zeebe-chaos/issues/32) and [zeebe-chaos#14](https://github.com/zeebe-io/zeebe-chaos/issues/14). This time [@Nico](https://github.com/korthout) joined me.\\n\\n**TL;DR** The experiment was successful :muscle: and we found several things in the dashboard which we can improve :)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment \\n\\nWith this experiment we want to verify that Zeebe can recover after OOD, which was caused by not exporting to ELS. For that we want to disconnect Zeebe and ELS first and see how it behaves. Afterwards we connect the services again and expect a recovery of the system.\\n\\nAs usual, we have set up a normal benchmark cluster with three nodes, three partitions and replication factor three. We run 200 PI/s and 12 workers against that cluster.\\n\\n### Expected\\n\\nWe expect the following properties:\\n\\n * at the beginning the system is stable (we can start instances without issues)\\n * after disconnecting ELS we start to fill the disk, since we can\'t export (which means we can\'t compact)\\n * after reaching the disk limits, Zeebe doesn\'t accept any commands anymore\\n * after connecting ELS, Zeebe should start to export again (compacting should be possible again)\\n * after come below the limit, Zeebe should accept commands again\\n\\n\\n#### Network disconnect to ELS\\n\\nIn order to disconnect the Brokers with ELS, we wanted to reuse one of our network disconnect scripts, e.g. [disconnect-leaders.sh](https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/scripts/disconnect-leaders.sh). This resolves the IP\'s of the brokers and creates an unreachable route via the `ip` tool at the given brokers.\\n\\nWe copied that and adjusted it to our needs:\\n\\n\x3c!-- {% raw %} --\x3e\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\nelasticName=\\"elasticsearch-master\\"\\n\\nbroker0=$(getBroker 0)\\nbroker2=$(getBroker 2)\\nbroker1=$(getBroker 1)\\n\\nelastic0Ip=$(kubectl get pod \\"$elasticName-0\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\nelastic1Ip=$(kubectl get pod \\"$elasticName-1\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\nelastic2Ip=$(kubectl get pod \\"$elasticName-2\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elastic0Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elastic1Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elastic2Ip\\"\\n\\n\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elastic0Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elastic1Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elastic2Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elastic0Ip\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elastic1Ip\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elastic2Ip\\"\\n```\\n\x3c!-- {% endraw %} --\x3e\\n\\n*Small note in order to run this against the benchmark cluster you need to set the following environment variables:*\\n\\n```shell\\nexport NAMESPACE=$(kubens -c) # the namespace where the resources are located\\nexport CHAOS_SETUP=helm # indicates that the installation is done via helm, necessary since we use different labels in the helm charts and in CC.\\n```\\n\\n\x3c!-- {% raw %} --\x3e\\nRunning the script above didn\'t work as expected. We still saw records being exported. The issue was that we need to use the service IP instead of the IP\'s of the elastic search pods. The Broker uses only the service to connect with ELS. In order to get the IP we can use this `kubectl get services elasticsearch-master --template \\"{{.spec.clusterIP}}\\"`. \\n\\n\\nThe service will take care of the request routing, which means we just need to block one IP. This helps to simplify the disconnect script.\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\nelasticName=\\"elasticsearch-master\\"\\n\\nbroker0=$(getBroker 0)\\nbroker2=$(getBroker 2)\\nbroker1=$(getBroker 1)\\n\\nelasticServiceIp=$(kubectl get services elasticsearch-master --template \\"{{.spec.clusterIP}}\\")\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elasticServiceIp\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elasticServiceIp\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elasticServiceIp\\"\\n```\\n\x3c!-- {% endraw %} --\x3e\\n\\n### Actual\\n\\nIn this section we will describe how we experienced the chaos experiment and what we observed. \\n\\n#### First Try\\nWe run the disconnect script and were able to observe that the exporting stopped. \\n\\n![elastic-disconnect](elastic-disconnect.png)\\n\\nAs expected we were no longer able to compact, which cause an increasing of log segments.\\n\\n![increase-segments](increase-segments.png)\\n\\nWe realized that our current disk size might be too big (it would take a while until we fill it), so we decided to setup a new benchmark with smaller size and different watermarks.\\n\\n```yaml\\n # ...\\n    env:\\n    - name: ZEEBE_BROKER_DATA_DISKUSAGECOMMANDWATERMARK\\n      value: \\"0.6\\"\\n    - name: ZEEBE_BROKER_DATA_DISKUSAGEREPLICATIONWATERMARK\\n      value: \\"0.8\\"\\n\\n\\n# PVC\\npvcAccessMode: [\\"ReadWriteOnce\\"]\\npvcSize: 16Gi\\npvcStorageClassName: ssd\\n```\\n\\nWe thought that we might have different performance, because of such a smaller disk size, but this was not the case. We were able to reach the same level, might be worth to think about reducing the disk sizes more in our benchmarks.\\n\\n![base](next-try-base.png)\\n\\n#### Second Try\\n\\n![base1](next-try-base1.png)\\n\\n##### Disconnecting\\n\\nAfter running our disconnect script we can immediately see that the exporting is stopping.\\n\\n![drop-base](next-try-drop-base.png)\\n\\nInteresting is the elastic section where we see one of our new panels in actions which shows the failure rate. There are two dots, which show the 100% failure rate.\\n\\n![drop-elastic](next-try-drop-elastic-section.png)\\n\\nAfter reaching our disk watermark we can see that the processing stops, and we no longer accept commands.\\n\\n![drop-base-2](next-try-drop-base-2.png)\\n\\nThe cluster turns to unhealthy, which is expected.\\n\\nInteresting is that we have no metrics at all on the gateway side after reaching the watermark.\\n\\n![drop-gw](next-try-drop-gw.png)\\n\\nWe were able to verify that snapshots are still taken, but no compaction.\\n\\n![drop-snapshot](next-try-drop-snapshot.png)\\n\\nNo segments are deleted during this time.\\n\\n![drop-segments](next-try-drop-segments.png)\\n\\nIf we take a look at the processing section we can see that the exporters lag way behind, which of course makes sense.\\n\\n![drop-processing](next-try-drop-processing.png)\\n\\n##### Connecting\\n\\nLuckily we were able to reuse on of our already written reconnect scripts for this experiment, see [connect-leaders.sh](https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/scripts/connect-leaders.sh).\\n\\nAfter removing the ip route (connecting the Brokers with ELS again) we can see that it immediately starts to export again.\\n\\n![connect-base](next-try-connect-base.png)\\n\\nWhen we went under the disk watermarks the processing started again and we accepted new commands.\\n\\n### Result\\n\\n![connect-base2](next-try-connect-base2.png)\\n\\n**The experiment was successful, our system was able to recover after an elastic network outage and handled it properly.** :white_check_mark: :muscle:\\n\\nWe noted several issues with the Dashboard, during the chaos experiment observation. For example the Brokers, which went OOD, never went back to the `Healthy` state again.\\n\\n![connect-healthy](next-try-connect-healthy.png)\\n\\nFurthermore, the not exported panel seems to be broken, depending on the selected time frame.\\n\\n![connect-not-exported](next-try-connect-not-exported.png)\\n\\nThere have been also other issues with the panels and sections which we should take a look at. I have listed them below.\\n\\n### Possible Improvements\\n\\nWe observed several issues with the grafana dashboard which I wrote down here. I will create issues or PR\'s to resolve them.\\n\\n#### General Metric Section\\n\\nIf we take a look at the screenshot where we reach our disk watermarks, and the processing stops, the backpressure metrics are not correctly updated. We would expect that the backpressure shows 100%, since all requests are rejected.\\n\\n![drop-base-2](next-try-drop-base-2.png)\\n\\nAfter the cluster actually becomes healthy again (it accepts new commands) it is not shown as healthy in the panels. The metrics seems not to be updated.\\n\\nAnother possible improvement would be to make it more visible that the exporting stopped. One idea is to split the processing into exporting and processing, so having two graphs might help.\\n\\n#### Processing Section\\n\\nDepending on the time frame the panel `Number of records not exported`, seems to show quite high values.\\n\\n![connect-not-exported](next-try-connect-not-exported.png)\\n\\nIf we take a look at other metrics, this doesn\'t make any sense. If we had such a backlog we wouldn\'t expect to compact to one segment for example. Furthemore the tables on the right side show numbers which are quite close to each other.\\n\\n#### Elastic Metrics Section\\n\\nWe probably can improve the failure panel, such that it shows a graph, and the limit is not set to 130%.\\n\\n![drop-elastic](next-try-drop-elastic-section.png)\\n\\n#### GRPC Metric Section\\n\\nThe panel `gRPC requests` should have an dec ordering on the tooltip and should be renamed to `Total gRPC requests` it was a bit confusing to us.\\n\\n#### Throughput Section\\n\\nThe StreamProcessor vs Exporter panel has no data.\\n\\n#### Snapshot Section\\n\\nWe saw that snapshots are created, but the Snapshot replication panel show no data. It seem to be broken (again?).\\n\\n#### Resources Section\\n\\nThe JVM panel has the legend on the right side, which makes it hard to see the metrics if you have multiple windows open on one screen."},{"id":"/2021/05/25/Reset-Clock","metadata":{"permalink":"/zeebe-chaos/2021/05/25/Reset-Clock","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-05-25-Reset-Clock/index.md","source":"@site/blog/2021-05-25-Reset-Clock/index.md","title":"Time travel Experiment","description":"Recently we run a Game day where a lot of messages with high TTL have been stored in the state. This was based on an earlier incident, which we had seen in production. One suggested approach to resolve that incident was to increase the time, such that all messages are removed from the state. This and the fact that summer and winter time shifts can cause in other systems evil bugs, we wanted to find out how our system can handle time shifts. Phil joined me as participant and observer. There was a related issue which covers this topic as well, zeebe-chaos#3.","date":"2021-05-25T00:00:00.000Z","formattedDate":"May 25, 2021","tags":[{"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":8.205,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Time travel Experiment","date":"2021-05-25T00:00:00.000Z","categories":["chaos_experiment","broker","time"],"tags":["data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Full Disk Recovery","permalink":"/zeebe-chaos/2021/06/08/Full-Disk"},"nextItem":{"title":"Corrupted Snapshot Experiment Investigation","permalink":"/zeebe-chaos/2021/04/29/Corrupted-Snapshot"}},"content":"[Recently we run a Game day](https://confluence.camunda.com/display/ZEEBE/Game+Day+18.05.2021) where a lot of messages with high TTL have been stored in the state. This was based on an earlier incident, which we had seen in production. One suggested approach to resolve that incident was to increase the time, such that all messages are removed from the state. This and the fact that summer and winter time shifts can cause in other systems evil bugs, we wanted to find out how our system can handle time shifts. [Phil](https://github.com/saig0) joined me as participant and observer. There was a related issue which covers this topic as well, [zeebe-chaos#3](https://github.com/zeebe-io/zeebe-chaos/issues/3).\\n\\n**TL;DR;** Zeebe is able to handle time shifts back and forth, without observable issues. Operate seems to dislike it.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nAs part of the experiment we had to define what we expect, if we change the time. In order to keep it simple we decided to experiment with one-hour move forward and backwards. We wanted to run the experiment first against our normal benchmark cluster and afterwards against a Production - S Cluster on INT. Furthermore, we decided to test the time shift only on leaders, for now.\\n\\n### Expected\\n\\n*If we move the time one-hour forward we expect:*\\n\\n * in general, timers should be triggered (snapshot, message TTL, job timeouts, timers etc.)\\n * the system should operate normal, means zeebe and operate should be healthy and continue working\\n\\n*If we move the time one-hour backwards we expect:*\\n\\n * timers should not be triggered, until the deadline + 1 hour is reached\\n * the system should operate normal\\n * with operate we were not sure whether it has issues with exported records on the same time\\n\\nBefore we started, with running the experiment, we had to find out how we can change the time in a docker container.\\n\\n### Changing Time\\n\\n***Note:** If you\'re not interested in how we change the time you can jump to the [next section](#experiment-on-benchmark-cluster)*\\n\\nIf you search for it, you find quite quickly answers in how to change the time in a docker container. For example, we found this [answer](https://serverfault.com/a/898842/283059), which was quite useful.\\n\\nIn order to apply this, we first need to make sure that we have the right [linux capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html) to change the system time. For that we need the `SYS_TIME` capability. In our benchmarks this is quite easy to do, we just need to change [this line](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/zeebe-values.yaml#L16) and add `SYS_TIME`.\\n\\n```yaml\\npodSecurityContext:\\n  capabilities:\\n        add: [\\"NET_ADMIN\\", \\"SYS_TIME\\"]\\n```\\n\\nAfter changing this, we can set the time via [`date -s`](https://man7.org/linux/man-pages/man1/date.1.html). Since we were not sure whether this really works for our java process, we started a [`jshell`](https://docs.oracle.com/javase/9/tools/jshell.htm#JSWOR-GUID-C337353B-074A-431C-993F-60C226163F00) to verify that. *Note:* the jshell is only available if you use an container image with jdk. This is available, if you build your own zeebe docker image via [zeebe/createBenchmark.sh](https://github.com/camunda-cloud/zeebe/blob/develop/createBenchmark.sh).\\n\\nChanging the time:\\n```sh\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# date\\nTue May 25 10:29:33 UTC 2021\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# date +%T -s \\"09:29:00\\"\\n09:29:00\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# date\\nTue May 25 09:29:01 UTC 2021\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# jshell\\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -Xlog:gc*:file=/usr/local/zeebe/data/gc.log:time:filecount=7,filesize=8M\\nMay 25, 2021 9:29:03 AM java.util.prefs.FileSystemPreferences$1 run\\nINFO: Created user preferences directory.\\n|  Welcome to JShell -- Version 11.0.11\\n|  For an introduction type: /help intro\\n\\njshell> new Date()\\n$1 ==> Tue May 25 09:29:08 UTC 2021\\n```\\n\\nAfter we found out how we can actually change the time we moved forward and run the described chaos experiment.\\n\\n### Experiment on Benchmark Cluster\\n\\nAs usual, we have set up a normal benchmark cluster with three nodes, three partitions and replication factor three. We run 200 PI/s and 12 workers against that cluster.\\n\\n#### Move Time Forward\\n\\nAfter setting up the cluster we had to find out who is the Leader and picked the one who is Leader for the most of the partitions.\\n\\n```shell\\n$ k exec -it zell-phil-chaos-zeebe-0 -- zbctl status --insecure\\nBrokers:\\n  Broker 0 - zell-phil-chaos-zeebe-0.zell-phil-chaos-zeebe.zell-phil-chaos.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy\\n    Partition 2 : Leader, Healthy\\n    Partition 3 : Leader, Healthy\\n    ....\\n```\\n\\n**On Broker 0 we increased the time by one hour.** After doing this we observed the metrics, but we haven\'t noticed any issues. We verified logs in stackdriver, but no errors were thrown.\\n![inc-1-hour-general](inc-1-hour-general.png)\\n\\nWe noticed, looking at the metrics, that the snapshot was triggered when we moved the time forward. This was what we actually expected. Plus also new scheduled snapshot have been triggered and created.\\n![inc-1-hour-snapshot](inc-1-hour-snapshot.png)\\n\\nOn the elastic exporter we can see that flushing has happened earlier than usual, because we increased the time. This was also expected.  \\n![inc-1-hour-export](inc-1-hour-export.png)\\n\\n**Experiment succeeded** :heavy_check_mark:\\n\\n#### Move Time Backwards\\n\\nIn order to run the experiment again, with moving the timer backwards, we set up a new benchmark cluster. This time we run the experiment on Broker 1, since he was the leader of the most partitions.\\n\\n```sh\\nBroker 1 - zell-phil-chaos-zeebe-1.zell-phil-chaos-zeebe.zell-phil-chaos.svc.cluster.local:26501\\n  Version: 1.1.0-SNAPSHOT\\n  Partition 1 : Follower, Healthy\\n  Partition 2 : Leader, Healthy\\n  Partition 3 : Leader, Healthy\\n```\\n\\nNo general issues have been detected, as expected no longer snapshots were taken.\\n![dec-1-hour-general](dec-1-hour-general.png)\\n\\nWe have run the benchmark for a bit longer, to see whether the snapshot will be triggered later, which was indeed the case.\\n![dec-1-hour-snapshot-later](dec-1-hour-snapshot-later.png)\\n\\nWe could also observe how the journal segments increased until we took the next snapshot.\\n![dec-1-hour-snapshot-segments](dec-1-hour-snapshot-segments.png)\\n\\n**Experiment succeeded** :heavy_check_mark:\\n\\n### Experiment on INT\\n\\nAfter running the experiment against our benchmark clusters we were confident to run it against a Production S cluster on INT.\\nWe have set up a Production S cluster in our chaos cluster and run the [cloud benchmark](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/newCloudBenchmark.sh) against it. It starts starter and worker against that Production S cluster, turned out not with the same load (luckily this doesn\'t matter for this experiment). The starter and workers are deployed in the Zeebe Team gke cluster.\\n\\n#### How to change the time on INT\\n\\nOn INT, it is not that simple to get the `SYS_TIME` capability, which we need to change the time. Luckily we already have experience, with getting the necessary capability we need to have.\\nOn a previous chaos day we have added `NET_ADMIN` capability to a running zeebe container in order to experiment with network partitioning, you can read about that [here](/2021-03-23-camunda-cloud-network-partition/index.md).\\n\\nThe following patch adds the `SYS_TIME` cap to our linux container.\\n```yaml\\nspec:\\n  template:\\n    spec:\\n      containers:\\n        - name: \\"zeebe\\"\\n          securityContext:\\n            capabilities:\\n              add:\\n                - \\"SYS_TIME\\"\\n```\\n\\nThe following script applies the patch to our Zeebe cluster.\\n\\n```shell\\n#!/bin/bash\\nset -euo pipefail\\n\\nscriptPath=$( cd \\"$(dirname \\"${BASH_SOURCE[0]}\\")\\" ; pwd -P )\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\n\\nCLUSTERID=${namespace%-zeebe}\\n\\nkubectl patch zb \\"$CLUSTERID\\" --type merge --patch=\'{\\"spec\\":{\\"controller\\":{\\"reconcileDisabled\\":true}}}\'\\nkubectl patch statefulset zeebe -n \\"$namespace\\" --patch \\"$(cat $scriptPath/sys_time_patch.yaml)\\"\\nkubectl delete pod -l \\"$(getLabel)\\" -n \\"$namespace\\"\\n```\\n\\n#### Move Time Forward on INT\\n\\nAfter we patched our resources we can now change the time as before.\\n\\n```shell\\nroot@zeebe-0:/usr/local/zeebe# date\\nTue May 25 09:55:33 UTC 2021\\nroot@zeebe-0:/usr/local/zeebe# date +%T -s \\"10:55:00\\"\\n10:55:00\\nroot@zeebe-0:/usr/local/zeebe# date\\nTue May 25 10:55:01 UTC 2021\\n```\\n\\nIn the general section of our Grafana Dashboard everything looks fine. We can\'t see any issues here, except that the exporting is much less than the processing.\\n![int-inc-1-hour-general](int-inc-1-hour-general.png)\\n\\nWe took a closer look at the processing panels and saw that the exporter lag a lot behind, which causes Operate lagging behind and that fewer segments are deleted.\\n![int-inc-1-hour-exporting](int-inc-1-hour-exporting.png)\\n\\nOn moving the time forward we see as expected the snapshotting has been triggered.\\n![int-inc-1-hour-snapshot](int-inc-1-hour-snapshot.png)\\n\\nStackdriver shows no errors for the Zeebe service. But later, in operate we observed that no new data was imported after 11:50, we moved the time at 11:55 forward. In the logs we found the following exceptions, which need to be investigated further.\\n\\n```shell\\nException occurred when importing data: io.camunda.operate.exceptions.PersistenceException: Update request failed for [operate-import-position-1.0.0_] and id [1-process-instance] with the message [Elasticsearch exception [type=version_conflict_engine_exception, reason=[1-process-instance]: version conflict, required seqNo [1681], primary term [1]. current document has seqNo [1688] and primary term [1]]].\\n```\\n\\nThe exception in more detail:\\n```java\\nio.camunda.operate.exceptions.PersistenceException: Update request failed for [operate-import-position-1.0.0_] and id [2-job] with the message [Elasticsearch exception [type=version_conflict_engine_exception, reason=[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]]].\\n\\tat io.camunda.operate.util.ElasticsearchUtil.executeUpdate(ElasticsearchUtil.java:271) ~[operate-els-schema-1.0.0.jar!/:?]\\n\\tat io.camunda.operate.zeebeimport.ImportPositionHolder.recordLatestLoadedPosition(ImportPositionHolder.java:100) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\tat io.camunda.operate.zeebeimport.ImportJob.call(ImportJob.java:86) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\tat io.camunda.operate.zeebeimport.RecordsReader.lambda$scheduleImport$1(RecordsReader.java:217) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\\n\\tat java.lang.Thread.run(Unknown Source) [?:?]\\nCaused by: org.elasticsearch.ElasticsearchStatusException: Elasticsearch exception [type=version_conflict_engine_exception, reason=[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]]\\n\\tat org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:176) ~[elasticsearch-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1933) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1910) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1667) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1624) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1594) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.update(RestHighLevelClient.java:1061) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat io.camunda.operate.util.ElasticsearchUtil.executeUpdate(ElasticsearchUtil.java:266) ~[operate-els-schema-1.0.0.jar!/:?]\\n\\t... 7 more\\n\\tSuppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://elasticsearch:9200], URI [/operate-import-position-1.0.0_/_update/2-job?refresh=true&timeout=1m], status line [HTTP/1.1 409 Conflict]\\n{\\"error\\":{\\"root_cause\\":[{\\"type\\":\\"version_conflict_engine_exception\\",\\"reason\\":\\"[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]\\",\\"index_uuid\\":\\"7jsKYxw7RxWQhba-UIG5Wg\\",\\"shard\\":\\"0\\",\\"index\\":\\"operate-import-position-1.0.0_\\"}],\\"type\\":\\"version_conflict_engine_exception\\",\\"reason\\":\\"[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]\\",\\"index_uuid\\":\\"7jsKYxw7RxWQhba-UIG5Wg\\",\\"shard\\":\\"0\\",\\"index\\":\\"operate-import-position-1.0.0_\\"},\\"status\\":409}\\n\\t\\tat org.elasticsearch.client.RestClient.convertResponse(RestClient.java:326) ~[elasticsearch-rest-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestClient.performRequest(RestClient.java:296) ~[elasticsearch-rest-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestClient.performRequest(RestClient.java:270) ~[elasticsearch-rest-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1654) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1624) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1594) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.update(RestHighLevelClient.java:1061) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat io.camunda.operate.util.ElasticsearchUtil.executeUpdate(ElasticsearchUtil.java:266) ~[operate-els-schema-1.0.0.jar!/:?]\\n\\t\\tat io.camunda.operate.zeebeimport.ImportPositionHolder.recordLatestLoadedPosition(ImportPositionHolder.java:100) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\t\\tat io.camunda.operate.zeebeimport.ImportJob.call(ImportJob.java:86) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\t\\tat io.camunda.operate.zeebeimport.RecordsReader.lambda$scheduleImport$1(RecordsReader.java:217) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\t\\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\\n\\t\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\\n\\t\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\\n\\t\\tat java.lang.Thread.run(Unknown Source) [?:?]\\n```\\n\\n**Experiment failed** :x: \\n\\nOperate was not operating normal, the exceptions were not expected.\\n\\nIt seems that changing the time on INT, causes some unexpected problems for Operate. We need to investigate that further and resolve them before we can continue here and make that an automated test. Furthermore, we need to investigate how problematic it is that our exporting lags behind, which is related to [zeebe#6747](https://github.com/camunda-cloud/zeebe/issues/6747), and how we can resolve that.\\n\\nIn general, we saw that Zeebe has no real issues with time shifts and that timers can be triggered by changing the underlying system time. Still we should make sure that our containers are running on UTC time nodes (which we do), such that we avoid issues with daylight saving time."},{"id":"/2021/04/29/Corrupted-Snapshot","metadata":{"permalink":"/zeebe-chaos/2021/04/29/Corrupted-Snapshot","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-04-29-Corrupted-Snapshot/index.md","source":"@site/blog/2021-04-29-Corrupted-Snapshot/index.md","title":"Corrupted Snapshot Experiment Investigation","description":"A while ago we have written an experiment, which should verify that followers are not able to become leader, if they have a corrupted snapshot. You can find that specific experiment here. This experiment was executed regularly against Production-M and Production-S Camunda Cloud cluster plans. With the latest changes, in the upcoming 1.0 release, we changed some behavior in regard to detect snapshot corruption on followers.","date":"2021-04-29T00:00:00.000Z","formattedDate":"April 29, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.195,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Corrupted Snapshot Experiment Investigation","date":"2021-04-29T00:00:00.000Z","categories":["chaos_experiment","broker","snapshots"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Time travel Experiment","permalink":"/zeebe-chaos/2021/05/25/Reset-Clock"},"nextItem":{"title":"BPMN meets Chaos Engineering","permalink":"/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering"}},"content":"A while ago we have written an experiment, which should verify that followers are not able to become leader, if they have a corrupted snapshot. You can find that specific experiment [here](https://github.com/zeebe-io/zeebe-chaos/tree/master/chaos-experiments/helm/snapshot-corruption). This experiment was executed regularly against Production-M and Production-S Camunda Cloud cluster plans. With the latest changes, in the upcoming 1.0 release, we changed some behavior in regard to detect snapshot corruption on followers. \\n\\n**NEW** If a follower is restarted and has a corrupted snapshot it will detect it on bootstrap and will refuse to\\nstart related services and crash. This means the pod will end in a crash loop, until this is manually fixed.\\n\\n**OLD** The follower only detects the corrupted snapshot on becoming leader when opening the database. On the restart of a follower this will not be detected.\\n\\nThe behavior change caused to fail our automated chaos experiments, since we corrupt the snapshot on followers and on a later experiment we restart followers. For this reason we had to disable the execution of the snapshot corruption experiment, see related issue\\n[zeebe-io/zeebe-cluster-testbench#303](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/303).\\n\\nIn this chaos day we wanted to investigate whether we can improve the experiment and bring it back. For reference, I also opened a issue to discuss the current corruption detection approach [zeebe#6907](https://github.com/camunda-cloud/zeebe/issues/6907)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nThis time we look at an already existing experiment. I will run our normal setup and execute the experiment (each step) manually and observe what happens.\\n\\n### Experiment\\n\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe can recover from corrupted snapshots\\",\\n    \\"description\\": \\"Zeebe should be able to detect and recover from corrupted snapshot\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            },\\n            {\\n                \\"name\\": \\"Should be able to create process instances on partition 3\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-steady-state.sh\\",\\n                    \\"arguments\\": \\"3\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Corrupt snapshots on followers\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"corruptFollowers.sh\\",\\n                \\"arguments\\": \\"3\\"\\n            }\\n        },\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Terminate leader of partition 3\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"shutdown-gracefully-partition.sh\\",\\n                \\"arguments\\": [ \\"Leader\\", \\"3\\" ]\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\nAs written before we have our normal benchmark setup and I will run the referenced scripts manually and observe the behavior via grafana. The panels below show the base or steady state.\\n![before-general](before-general.png)\\n![before-snap](before-snap.png)\\n\\n\\nThe first script we will run, corrupts for a certain partition the snapshots of all followers. It does it via just simply deleting some `*.sst` files.\\n\\n```shell\\n[zell scripts/ cluster: zeebe-cluster ns:zell-chaos]$ ./corruptFollowers.sh 3\\n+ ...\\n+ leader=zell-chaos-zeebe-1\\n+ followers=\'zell-chaos-zeebe-0\\nzell-chaos-zeebe-2\'\\n+ ...\\n+ kubectl -n zell-chaos exec zell-chaos-zeebe-0 -- ./corrupting.sh 3\\n+ partition=3\\n+ partitionDir=data/raft-partition/partitions/3\\n+ snapshotDir=(\\"$partitionDir\\"/snapshots/*)\\n++ find data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619 -name \'*.sst\' -print -quit\\n+ fileName=data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000110.sst\\n+ rm data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000110.sst\\n+ ...\\n+ kubectl -n zell-chaos exec zell-chaos-zeebe-2 -- ./corrupting.sh 3\\n+ partition=3\\n+ partitionDir=data/raft-partition/partitions/3\\n+ snapshotDir=(\\"$partitionDir\\"/snapshots/*)\\n++ find data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619 -name \'*.sst\' -print -quit\\n+ fileName=data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000112.sst\\n+ rm data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000112.sst\\n```\\n\\nThe second script just terminated the Leader for the referenced partition.\\n\\n```shell\\n[zell scripts/ cluster: zeebe-cluster ns:zell-chaos]$ ./terminate-partition.sh \\"Leader\\" 3\\npod \\"zell-chaos-zeebe-1\\" deleted\\n```\\n\\nOn the first try we had the \\"luck\\" that there was a snapshot replication in between, such that the follower was able to become Leader, since it had again a valid snapshot. \\n\\n![luck-general](luck-general.png)\\n![luck-replication](luck-replication.png)\\n\\nOn the second try we were actually able to reproduce the behavior we want to see. We have corrupted the snapshot and restarted the Leader and the partition can make only progress - if the previous leader comes back. This is because the others have a corrupted snapshot and can\'t take over.\\n\\n![no-luck-restart-general](no-luck-restart-general.png)\\n![no-luck-restart](no-luck-restart.png)\\n\\nIn all cases above we were able to make progress. The issue now arises when one of the affected follower is restarted. For our experiment I restarted Broker-2, which was at this point in time follower for Partition 3. After I restarted the follower it first looked like the partition one was completely down.\\n\\n![follower-restart-partition-1](follower-restart-partition-1.png)\\n\\nAfter serveral minutes the system recovered and continued.\\n\\n![follower-restart-partition-1-cont](follower-restart-partition-1-cont.png)\\n\\nVia Grafana but also via `kubectl` we can see that the pod doesn\'t become ready again.\\n\\n```shell\\nzell-chaos-zeebe-0                          1/1     Running     0          13m\\nzell-chaos-zeebe-1                          1/1     Running     0          17m\\nzell-chaos-zeebe-2                          0/1     Running     0          5m4s\\nzell-chaos-zeebe-gateway-854dd5dd5c-b8cpl   1/1     Running     0          45m\\n```\\n\\n```shell\\n  Warning  Unhealthy               4m59s               kubelet                  Readiness probe failed: HTTP probe failed with statuscode: 503\\n  Warning  Unhealthy               9s (x30 over 5m9s)  kubelet                  Readiness probe failed: Get http://10.0.29.26:9600/ready: dial tcp 10.0.29.26:9600: connect: connection refused\\n```\\n\\nIn camunda cloud this will end in a crash loop, because we restart pods after 15 minutes if they are not ready in time. It is interesting to see that it seems not to restart by itself, **which I had expected**. After checking the logs we can also see why.\\n\\n```md\\nI 2021-04-29T11:20:44.205851Z RaftServer{raft-partition-partition-1} - Server join completed. Waiting for the server to be READY \\n**W 2021-04-29T11:20:44.220338Z Cannot load snapshot in /usr/local/zeebe/data/raft-partition/partitions/3/snapshots/1387684-4-4014493-4014014. The checksum stored does not match the checksum calculated.** \\nD 2021-04-29T11:20:44.737724Z Loaded disk segment: 33 (raft-partition-partition-3-33.log) \\nD 2021-04-29T11:20:44.738579Z Found segment: 33 (raft-partition-partition-3-33.log) \\nD 2021-04-29T11:20:45.024028Z Loaded disk segment: 34 (raft-partition-partition-3-34.log) \\nD 2021-04-29T11:20:45.024688Z Found segment: 34 (raft-partition-partition-3-34.log) \\nE 2021-04-29T11:20:45.025826Z Bootstrap Broker-2 [6/13]: cluster services failed with unexpected exception. \\nI 2021-04-29T11:20:45.038260Z Closing Broker-2 [1/5]: subscription api \\nD 2021-04-29T11:20:45.040467Z Closing Broker-2 [1/5]: subscription api closed in 1 ms \\nI 2021-04-29T11:20:45.040926Z Closing Broker-2 [2/5]: command api handler \\nD 2021-04-29T11:20:45.042116Z Closing Broker-2 [2/5]: command api handler closed in 1 ms \\nI 2021-04-29T11:20:45.042524Z Closing Broker-2 [3/5]: command api transport \\nD 2021-04-29T11:20:46.163435Z Created segment: JournalSegment{id=2, index=1556849} \\nI 2021-04-29T11:20:47.065203Z Stopped \\nD 2021-04-29T11:20:47.066007Z Closing Broker-2 [3/5]: command api transport closed in 2023 ms \\nI 2021-04-29T11:20:47.066552Z Closing Broker-2 [4/5]: membership and replication protocol \\nE 2021-04-29T11:20:47.067664Z Closing Broker-2 [4/5]: membership and replication protocol failed to close. \\nI 2021-04-29T11:20:47.069050Z Closing Broker-2 [5/5]: actor scheduler \\nD 2021-04-29T11:20:47.069503Z Closing actor thread ground \'Broker-2-zb-fs-workers\' \\nD 2021-04-29T11:20:47.071276Z Closing actor thread ground \'Broker-2-zb-fs-workers\': closed successfully \\nD 2021-04-29T11:20:47.071642Z Closing actor thread ground \'Broker-2-zb-actors\' \\nD 2021-04-29T11:20:47.073287Z Closing actor thread ground \'Broker-2-zb-actors\': closed successfully \\nD 2021-04-29T11:20:47.074101Z Closing Broker-2 [5/5]: actor scheduler closed in 4 ms \\nI 2021-04-29T11:20:47.074468Z Closing Broker-2 succeeded. Closed 5 steps in 2036 ms. \\n**E 2021-04-29T11:20:47.074845Z Failed to start broker 2!** \\nI 2021-04-29T11:20:47.078669Z \\n\\nError starting ApplicationContext. To display the conditions report re-run your application with \'debug\' enabled. \\n**E 2021-04-29T11:20:47.093828Z Application run failed** \\nI 2021-04-29T11:20:47.120419Z Shutting down ExecutorService \'applicationTaskExecutor\' \\nI 2021-04-29T11:20:47.132016Z RaftServer{raft-partition-partition-1}{role=FOLLOWER} - No heartbeat from null in the last PT2.926S (calculated from last 2926 ms), sending poll requests \\nI 2021-04-29T11:20:47.260582Z RaftServer{raft-partition-partition-1} - Found leader 0 \\nI 2021-04-29T11:20:47.262407Z RaftServer{raft-partition-partition-1} - Setting firstCommitIndex to 1507899. RaftServer is ready only after it has committed events upto this index \\nI 2021-04-29T11:20:47.263428Z RaftPartitionServer{raft-partition-partition-1} - Successfully started server for partition PartitionId{id=1, group=raft-partition} in 10098ms \\nD 2021-04-29T11:21:05.128572Z Created segment: JournalSegment{id=3, index=1570617} \\nD 2021-04-29T11:21:26.376398Z Created segment: JournalSegment{id=4, index=1584886} \\nI 2021-04-29T11:21:28.129677Z RaftPartitionServer{raft-partition-partition-2} - Successfully started server for partition PartitionId{id=2, group=raft-partition} in 51572ms \\n```\\n\\nWe can see in the logs that the corruption is detected and the broker seems to stop, but actually it doesn\'t.\\nAfter `Failed to start broker 2` the process should normally end. It looks like the thread for the other raft partitions are still running and continuing. This is a bug which I reported in one of my last chaos days,\\nsee [camunda-cloud/zeebe#6702](https://github.com/camunda-cloud/zeebe/issues/6702).\\n\\nWith this current behavior we could easily fix the snapshot corruption experiments, since we just need a separate clean up step. It could look like this:\\n\\n```shell\\n k exec -it zell-chaos-zeebe-2 -- rm -r data/raft-partition/partitions/3 # remove partition three data\\n k delete pod zell-chaos-zeebe-2 # restart broker 2\\n```\\n\\nI tried this and it worked, after some minutes the Broker-2 was able to come back.\\n\\n![success-after-fix.png](success-after-fix.png)\\n\\nWhy does it work you may ask. If we remove the corrupted partition data from the follower and restart it, then it will join the cluster with an empty state. The Leader for that partition will immediately replicate the snapshot for that partition, such that the follower is up to date again. This allows the follower then to bootstrap without issues again.\\n\\nOne problem with this approach we have if we actually fix the bug above, where we\'re not shutting down, then we have the issue that we might not be able to access the data, since the pod is not up. I need to do some more research to solve this, but one possible solution I can think of would be to patch the *StatefulSet*, such that we can claim the PV via multiple pods. This would allow us to start a separate POD in order to access the data and delete it.\\n\\n## Found Bugs\\n\\n * Broker is not shutting down properly [camunda-cloud/zeebe#6702](https://github.com/camunda-cloud/zeebe/issues/6702)"},{"id":"/2021/04/03/bpmn-meets-chaos-engineering","metadata":{"permalink":"/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-04-03-bpmn-meets-chaos-engineering/index.md","source":"@site/blog/2021-04-03-bpmn-meets-chaos-engineering/index.md","title":"BPMN meets Chaos Engineering","description":"On the first of April (2021) we ran our Spring Hackday at Camunda. This is an event where the developers at camunda come together to work on projects they like or on new ideas/approaches they want to try out. This time we (Philipp and me) wanted to orchestrate our Chaos Experiments with BPMN. If you already know how we automated our chaos experiments before, you can skip the next section","date":"2021-04-03T00:00:00.000Z","formattedDate":"April 3, 2021","tags":[{"label":"tools","permalink":"/zeebe-chaos/tags/tools"}],"readingTime":7.615,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"BPMN meets Chaos Engineering","date":"2021-04-03T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["tools"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Corrupted Snapshot Experiment Investigation","permalink":"/zeebe-chaos/2021/04/29/Corrupted-Snapshot"},"nextItem":{"title":"Set file immutable","permalink":"/zeebe-chaos/2021/03/30/set-file-immutable"}},"content":"On the first of April (2021) we ran our Spring Hackday at Camunda. This is an event where the developers at camunda come together to work on projects they like or on new ideas/approaches they want to try out. This time we ([Philipp](https://github.com/saig0) and [me](https://github.com/zelldon)) wanted to orchestrate our Chaos Experiments with BPMN. If you already know how we automated our chaos experiments before, you can skip the next section\\nand jump directly to the [Hackday Project section](#hackday-project).\\n\\nIn order to understand this blogpost make sure that you have a little understanding of Zeebe, Camunda Cloud and Chaos Engineering. Read the following resources to get a better understanding.\\n\\n * [Get Started with Camund cloud](https://docs.camunda.io/docs/guides/)\\n * [Quickstart Guide](https://docs.camunda.io/docs/product-manuals/clients/cli-client/get-started)\\n * [Camunda Cloud](https://camunda.com/de/products/cloud/)\\n * [Zeebe Process Engine](https://docs.camunda.io/docs/product-manuals/zeebe/zeebe-overview/)\\n * [BPMN 2.0](https://www.omg.org/spec/BPMN/2.0/About-BPMN/)\\n * [Principles of Chaos](https://principlesofchaos.org/)\\n\\n\x3c!--truncate--\x3e\\n\\n## Previous Chaos Automation \\n\\nIn the previous Chaos Day summaries I described that we use [ChaosToolkit](https://chaostoolkit.org/) to run our chaos experiments. The chaos experiments have as prerequisite that an Zeebe cluster is already running, on which they should be executed. ChaosToolkit needs/uses a specific DSL to describe and execute Chaos Experiments. An example experiment looks like the following:\\n\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe follower restart non-graceful experiment\\",\\n    \\"description\\": \\"Zeebe should be fault-tolerant. Zeebe should be able to handle followers terminations.\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            },\\n            {\\n                \\"name\\": \\"Should be able to create workflow instances on partition 1\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-steady-state.sh\\",\\n                    \\"arguments\\": \\"1\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Terminate follower of partition 1\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"terminate-partition.sh\\",\\n                \\"arguments\\": [ \\"Follower\\", \\"1\\"]\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\nThis JSON describes a chaos experiment where a follower of partition one is terminated, and where we expect that we can create a new process instance before and after this termination. The follower termination should not affect our steady state.\\n\\nIn the JSON structure we can see the defined steady state of the Zeebe Cluster and the method/action which should be executed (the chaos which should be injected). The defined steady state is verified at the beginning of the experiment and at the end, after the methods are executed. The execution logic is quite simple. You can also define rollback actions, which should be executed if the experiment fails. Timeouts can be defined for each action and probe. Since the `chaosToolkit` is written in Python you can reference python modules/functions, which should be called during execution. Additionally, it supports bash scripts, which we normally use. Unfortunately bash scripts are sometimes not easy to understand and to maintain. This is one of the reason why we already thought more than once to replace the `chaosToolkit` with something different.\\n\\nThe `chaosToolkit` has more features and extensions, but these are not used by us. \\n\\n### List of Chaos Experiments\\n\\nThe experiment above is just one experiment of our continuous growing collection of chaos experiments, which we have already defined. There exist chaos experiments for the helm charts, but also for camunda cloud, for each cluster plan separately. You can find them [here](https://github.com/zeebe-io/zeebe-chaos/tree/master/chaos-experiments).\\n\\n### Automated Chaos Experiments\\n\\nChaos experiments need to be executed continously, not only once. For that we have build an automated pipeline, which runs the chaos experiments every night or if requested. We did that with help of the [Zeebe Cluster Testbench](https://github.com/zeebe-io/zeebe-cluster-testbench), we call it just `testbench`. The `testbench` creates for each cluster plan, in camunda cloud, a Zeebe cluster and runs the corresponding experiments against these clusters. The process model looks quite simple.\\n\\n![chaos-test](chaos-test.png)\\n\\nIt is executed via a [zbctl chaos worker](https://github.com/zeebe-io/zeebe-cluster-testbench/tree/develop/core/chaos-workers), which is part of the `testbench`. The `chaos worker` polls for new jobs at the `testbench`. On new jobs it executes, based on the cluster plan, against the given/created Zeebe cluster the chaos experiments, via the `chaostoolkit`.\\n\\nIn general this was a good first solution, which is quite extensible since we just needed to add new experiments in the [zeebe-chaos](https://github.com/zeebe-io/zeebe-chaos) repository and on the next run the experiments are executed, without any further adjustments. \\n\\n### Challenges\\n\\nStill, we had some challenges with this approach. \\n\\n#### Additional Dependency\\n\\nWith the `chaosToolkit` we had an additional dependency. If you want to implement/write new chaos experiments you need to set up the `chaosToolkit` on your machine, with the correct Python etc. In general the setup guide was straight forward, but still it was something you need to have. It made the adoption harder.\\n\\n#### Root Cause Analysis\\n\\nDue to our setup it was a bit more challenging todo the root cause analysis.\\n\\n![ChaosOutput](ChaosOutput.png)\\n\\nWe run a `zbctl` worker in a docker image, which picks up the `chaos` typed jobs. An `zbctl` worker will complete jobs with the output of the called handler script. This means that everything, which you want to log, needs to be logged in a separate file. The `chaosToolkit` will print its output into an own log file. The output of the bash scripts, which are executed by the `chaosToolkit`, will also end in that `chaosToolkit.log` file. I tried to visualize this a bit with the image above.\\n\\n![chaos-test](failed-chaos-experiment-testbench.png)\\n\\nIf the chaos worker completes a job, the process instance in `testbench` is continued. If a chaos experiment fails, then the job is still completed normally, but with an error result. In the process instance execution this means that a different path is taken. The `testbench` will write a slack notification to a specific channel, such that the current Zeebe medic can look at it. \\n\\n![run-test](run-test-in-camunda-cloud.png)\\n\\nAfter the notification the medic needs to find out which experiment has failed, this is part of the payload of the completed job at least, but he also needs to find out why the experiment failed. For this root cause analysis he has to check the log of the `chaostoolkit`, which is stored somewhere in the chaos worker pod (`data/chaostoolkit.log` it is an ever growing log).\\n\\n## Hackday Project\\n\\nWith our Hackday Project we had two goals:\\n\\n 1. lower the bar for team adoption \\n 2. make root cause analysis easier\\n    \\nFor that we wanted to replace `chaosToolkit` with a BPMN Process, which should be executed by Zeebe. We wanted to stick with the experiment description (the `chaosToolkit`/openchaos DSL) for our chaos experiments.\\n\\nWe modeled two processes. One root process, which reads for a given cluster plan all experiments and runs then each experiment. This is done via a [multi instance](https://docs.camunda.io/docs/0.25/product-manuals/zeebe/bpmn-workflows/multi-instance/multi-instance/) [call activity](https://docs.camunda.io/docs/reference/bpmn-workflows/call-activities/call-activities/).\\n\\n![ChaosOutput](chaosToolkit.png)\\n\\nThe other process model is used for the real chaos experiment execution. As the `chaosToolkit` execution itself was quite simple, the resulting BPMN model is as well. All activities are\\nsequential multi instances, since we can have multiple probes/actions for the steady state, but also for the injection of chaos. On the root level of the process we have an interrupting [event sub process](https://docs.camunda.io/docs/reference/bpmn-workflows/event-subprocesses/event-subprocesses/) to timeout the chaos experiment if the experiment takes to long.\\n\\n![ChaosExperiment](chaosExperiment.png)\\n\\nAs payload of the process instances we have the defined chaos experiment in JSON, which we have seen earlier. In this JSON we have all information we need to orchestrate this experiment.\\n\\nWe have implemented two Kotlin workers, one to read all experiment JSON files and one to execute the bash scripts, which are referenced in the chaos experiment descriptions. You can find the code [here](https://github.com/zeebe-io/zeebe-chaos/tree/master/chaos-model/chaos-worker), it is just 100 lines long.\\n\\n### Results\\n\\nWe orchestrated the follower/leader termination and graceful shutdown experiments via Camunda Cloud and the created process models. The experiments have been executed against another Zeebe cluster successfully.\\n\\n![success](success-run.png)\\n\\nTo see how we improved the observability, we provoked an experiment to fail. Operate shows use via an incident that an experiment failed, and exactly at which step. \\n\\n![failure](fail-run.png)\\n\\nWe can even see the standard output and error output of the executed script in operate, without searching different log files.\\n\\n![failuremsg](fail-run-output.png)\\n\\nAn timeout of an experiment will look like this:\\n\\n![timeout](timeout-run.png)\\n\\nAs we can see the observability improved here a lot. We are able to understand why it failed based on the error message, since the complete error output is printed, and we can see the progress of the process instance on running the chaos experiment. \\n\\nWith these models we were able to completely replace the `chaosToolkit` usage, so in the end we can remove an additional dependency.\\n\\n### Further Work\\n\\nNext step would be to integrate this in `testbench`, such that we can replace the old `chaos worker`.\\n\\nFurthermore, we plan to replace step by step the new worker, which calls the scripts, by workers which have the script logic inside. For example with workers written in go or kotlin. This should improve the adoption and maintainability further.\\n\\nFor simplicity and to make progress we modeled quite generic process models, which are feed with the chaos experiment DSL. In the future we can also think of modeling the chaos experiments directly as BPMN model.\\n\\n**Thanks to [Peter](https://github.com/pihme) for giving the impulse and his awesome work on `testbench` and [Philipp](https://github.com/saig0) which worked with me on this Hackday project.**"},{"id":"/2021/03/30/set-file-immutable","metadata":{"permalink":"/zeebe-chaos/2021/03/30/set-file-immutable","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-03-30-set-file-immutable/index.md","source":"@site/blog/2021-03-30-set-file-immutable/index.md","title":"Set file immutable","description":"This chaos day was a bit different. Actually I wanted to experiment again with camunda cloud and verify that our high load chaos experiments are now working with the newest cluster plans, see zeebe-cluster-testbench#135.","date":"2021-03-30T00:00:00.000Z","formattedDate":"March 30, 2021","tags":[],"readingTime":6.5,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Set file immutable","date":"2021-03-30T00:00:00.000Z","categories":["chaos_experiment","filesystem","immutable"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"BPMN meets Chaos Engineering","permalink":"/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering"},"nextItem":{"title":"Camunda Cloud network partition","permalink":"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition"}},"content":"This chaos day was a bit different. Actually I wanted to experiment again with camunda cloud and verify that our high load chaos experiments are now working with the newest cluster plans, see [zeebe-cluster-testbench#135](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/135). \\nUnfortunately I found out that our test chaos cluster was in a way broken, that we were not able to create new clusters. Luckily this was fixed at the end of the day, thanks to @immi :) \\n\\nBecause of these circumstances I thought about different things to experiment with, and I remembered that in the [last chaos day](/2021-03-23-camunda-cloud-network-partition/index.md) we worked with patching running deployments, in order to add more capabilities.\\nThis allowed us to create ip routes and experiment with the zeebe deployment distribution. During this I have read the [capabilities list of linux](https://man7.org/linux/man-pages/man7/capabilities.7.html), and found out that we can mark files as immutable, which might be interesting for a chaos experiment.\\n\\nIn this chaos day I planned to find out how marking a file immutable affects our brokers and I made the hypothesis that: *If a leader has a write error, which is not recoverable, it will step down and another leader should take over.* I put this in our hypothesis backlog ([zeebe-chaos#52](https://github.com/zeebe-io/zeebe-chaos/issues/52)). \\n\\nIn order to really run this kind of experiment I need to find out whether marking a file immutable will cause any problems and if not how I can cause write errors such that affects the broker.\\nUnfortunately it turned out that immutable files will not cause issues on already opened file channels, but I found some other bugs/issues, which you can read below.\\n\\nIn the next chaos days I will search for a way to cause write errors proactively, so we can verify that our system can handle such issues.\\n\\n\x3c!--truncate--\x3e\\n\\n### Immutable File\\n\\nIn order to [mark a file as immutable](https://delightlylinux.wordpress.com/2012/12/11/file-immutable-attribute/) we can use the command `chattr +i`. For that we need a specific capability called `LINUX_IMMUTABLE`. Since we were at this time not able to create new clusters I tested it with the helm charts, where it is anyway easier to give us these capabilities.\\n\\nWe just need to add in our *values* files the following:\\n\\n```shell\\npodSecurityContext:\\n  capabilities:\\n        add: [ \\"LINUX_IMMUTABLE\\" ]\\n```\\n\\nSince we want to experiment with leaders I need to get the current topology from the gateway, here I found an issue on our latest zbctl build and the human output ([camunda-cloud/zeebe#6692](https://github.com/camunda-cloud/zeebe/issues/6692)). This is already fixed, thanks to [Miguel](https://github.com/MiguelPires) :rocket:!\\n\\nAfter finding the correct leader we can execute on the corresponding broker/pod following commands to mark a file as immutable.\\n\\n```shell\\n[zell ns:zell-chaos]$ k exec -it zell-chaos-zeebe-1 -- bash # opens an interactive shell on zell-chaos-zeebe-1\\nroot@zell-chaos-zeebe-1# cd data/raft-partition/partitions/1/\\nroot@zell-chaos-zeebe-1# chattr +i raft-partition-partition-1-1.log # marks the log file immutable\\nroot@zell-chaos-zeebe-1# lsattr raft-partition-partition-1-1.log\\n----i---------e---- raft-partition-partition-1-1.log\\n```\\n\\nWith the last command `lsattr` we print out the file attributes, where we see that the immutable flag is set (the `i` in the output).\\nAfter setting the log file to immutable nothing really happens. I checked again the manual page and read that this not affects already created file channels.\\n\\n```shell\\nCHATTR(1)                           General Commands Manual                          CHATTR(1)\\n\\nNAME\\n       chattr - change file attributes on a Linux file system\\n\\n      ...\\n      i      A  file with the \'i\' attribute cannot be modified: it cannot be deleted or renamed, no link can be created to this file, most of the file\'s metadata can not be modified, and the\\n              file can not be opened in write mode.  Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear this attribute.\\n      ...\\n      \\n      BUGS AND LIMITATIONS\\n       The \'c\', \'s\',  and \'u\' attributes are not honored by the ext2, ext3, and ext4 filesystems as implemented in the current mainline Linux kernels.  Setting \'a\' and \'i\' attributes will not\\n       affect the ability to write to already existing file descriptors.\\n```\\n\\nSo we actually are not able to use it to cause any write errors on a running leader, but interesting was what happened later when the pod was restarted. The pod was not able to restart, since the log was still immutable. We can see the following bootstrap sequence and the related errors:\\n\\n```shell\\nD 2021-03-30T09:11:02.433467Z Found segment: 1 (raft-partition-partition-2-1.log) \\nI 2021-03-30T09:11:02.499079Z RaftServer{raft-partition-partition-2} - Transitioning to FOLLOWER \\nI 2021-03-30T09:11:02.500847Z RaftPartitionServer{raft-partition-partition-1} - Starting server for partition PartitionId{id=1, group=raft-partition} \\nI 2021-03-30T09:11:02.506876Z RaftServer{raft-partition-partition-2} - Server join completed. Waiting for the server to be READY \\nE 2021-03-30T09:11:02.508400Z Bootstrap Broker-1 [6/13]: cluster services failed with unexpected exception. \\nI 2021-03-30T09:11:02.523239Z Closing Broker-1 [1/5]: subscription api \\nD 2021-03-30T09:11:02.525497Z Closing Broker-1 [1/5]: subscription api closed in 2 ms \\nI 2021-03-30T09:11:02.526484Z Closing Broker-1 [2/5]: command api handler \\nD 2021-03-30T09:11:02.528108Z Closing Broker-1 [2/5]: command api handler closed in 1 ms \\nI 2021-03-30T09:11:02.528740Z Closing Broker-1 [3/5]: command api transport \\nI 2021-03-30T09:11:03.519309Z RaftServer{raft-partition-partition-2} - Found leader 2 \\nI 2021-03-30T09:11:03.521376Z RaftServer{raft-partition-partition-2} - Setting firstCommitIndex to 2. RaftServer is ready only after it has committed events upto this index \\nI 2021-03-30T09:11:03.522206Z RaftPartitionServer{raft-partition-partition-2} - Successfully started server for partition PartitionId{id=2, group=raft-partition} in 1171ms \\nI 2021-03-30T09:11:04.553825Z Stopped \\nD 2021-03-30T09:11:04.555166Z Closing Broker-1 [3/5]: command api transport closed in 2026 ms \\nI 2021-03-30T09:11:04.556177Z Closing Broker-1 [4/5]: membership and replication protocol \\nI 2021-03-30T09:11:04.558282Z RaftServer{raft-partition-partition-2} - Transitioning to INACTIVE \\nE 2021-03-30T09:11:04.558408Z Closing Broker-1 [4/5]: membership and replication protocol failed to close. \\nI 2021-03-30T09:11:04.560776Z Closing Broker-1 [5/5]: actor scheduler \\nD 2021-03-30T09:11:04.561558Z Closing actor thread ground \'Broker-1-zb-fs-workers\' \\nD 2021-03-30T09:11:04.563600Z Closing segment: JournalSegment{id=1, version=1, index=1} \\nD 2021-03-30T09:11:04.563881Z Closing actor thread ground \'Broker-1-zb-fs-workers\': closed successfully \\nD 2021-03-30T09:11:04.564448Z Closing actor thread ground \'Broker-1-zb-actors\' \\nD 2021-03-30T09:11:04.566157Z Closing actor thread ground \'Broker-1-zb-actors\': closed successfully \\nD 2021-03-30T09:11:04.567716Z Closing Broker-1 [5/5]: actor scheduler closed in 6 ms \\nI 2021-03-30T09:11:04.568366Z Closing Broker-1 succeeded. Closed 5 steps in 2045 ms. \\nE 2021-03-30T09:11:04.568908Z Failed to start broker 1! \\nI 2021-03-30T09:11:04.574482Z \\n\\nError starting ApplicationContext. To display the conditions report re-run your application with \'debug\' enabled. \\nE 2021-03-30T09:11:04.595919Z Application run failed \\nI 2021-03-30T09:11:04.627321Z Shutting down ExecutorService \'applicationTaskExecutor\' \\n```\\n\\nThe following exception occurred on opening the log:\\n\\n```java\\njava.util.concurrent.CompletionException: io.zeebe.journal.JournalException: java.nio.file.FileSystemException: /usr/local/zeebe/data/raft-partition/partitions/1/raft-partition-partition-1-1.log: Operation not permitted\\n\\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]\\n\\tat java.lang.Thread.run(Unknown Source) ~[?:?]\\nCaused by: io.zeebe.journal.JournalException: java.nio.file.FileSystemException: /usr/local/zeebe/data/raft-partition/partitions/1/raft-partition-partition-1-1.log: Operation not permitted\\n\\tat io.zeebe.journal.file.SegmentedJournal.openChannel(SegmentedJournal.java:468) ~[zeebe-journal-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n\\tat io.zeebe.journal.file.SegmentedJournal.loadSegments(SegmentedJournal.java:490) ~[zeebe-journal-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n```\\n\\nAfter this exception on bootstrap the broker tries to close itself, and we see an error on closing a step `2021-03-30 11:11:04.558 CEST Closing Broker-1 [4/5]: membership and replication protocol failed to close.` This seems to be caused by a NPE.\\n```shell\\n\\njava.lang.NullPointerException: null\\n\\tat io.atomix.raft.partition.impl.RaftPartitionServer.stop(RaftPartitionServer.java:141) ~[atomix-cluster-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n\\tat io.atomix.raft.partition.RaftPartition.closeServer(RaftPartition.java:165) ~[atomix-cluster-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n\\tat io.atomix.raft.partition.RaftPartition.close(RaftPartition.java:155) ~[atomix-cluster-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n```\\n\\nThe problem now here is that the broker never comes back. It is not restarted, which is a bit confusing. Furthermore it hasn\'t retried on opening, which is also unexpected, since it might be a temporary exception. \\n\\nWe can see in stackdriver that the new leader is not able to connect, which is expected. But we also see that the other Broker never comes back which is unexpected!\\n\\n```shell\\n\\nW 2021-03-30T09:11:05.006825Z RaftServer{raft-partition-partition-2} - AppendRequest{term=2, leader=2, prevLogIndex=2, prevLogTerm=2, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:05.256830Z RaftServer{raft-partition-partition-2} - AppendRequest{term=2, leader=2, prevLogIndex=2, prevLogTerm=2, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:08.450683Z RaftServer{raft-partition-partition-3} - AppendRequest{term=2, leader=2, prevLogIndex=1, prevLogTerm=1, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:12.897492Z RaftServer{raft-partition-partition-1} - AppendRequest{term=2, leader=2, prevLogIndex=1, prevLogTerm=1, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:25.950363Z RaftServer{raft-partition-partition-3} - AppendRequest{term=2, leader=2, prevLogIndex=1, prevLogTerm=1, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message\\n```\\n\\nWe need to investigate this issue [camunda-cloud/zeebe#6702](https://github.com/camunda-cloud/zeebe/issues/6702) further, and I need to find out how we can cause write errors proactively.\\n\\n#### Found Bugs\\n\\n * Int Console was not able to create new clusters in ultrachaos \\n * Zbctl human output looks broken [#6692](https://github.com/camunda-cloud/zeebe/issues/6692)\\n * Broker is not correctly shutdown [#6702](https://github.com/camunda-cloud/zeebe/issues/6702)"},{"id":"/2021/03/23/camunda-cloud-network-partition","metadata":{"permalink":"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-03-23-camunda-cloud-network-partition/index.md","source":"@site/blog/2021-03-23-camunda-cloud-network-partition/index.md","title":"Camunda Cloud network partition","description":"This time Deepthi was joining me on my regular Chaos Day.","date":"2021-03-23T00:00:00.000Z","formattedDate":"March 23, 2021","tags":[],"readingTime":7.18,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Camunda Cloud network partition","date":"2021-03-23T00:00:00.000Z","categories":["chaos_experiment","camunda_cloud","network"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Set file immutable","permalink":"/zeebe-chaos/2021/03/30/set-file-immutable"},"nextItem":{"title":"Fault-tolerant processing of process instances","permalink":"/zeebe-chaos/2021/03/09/cont-workflow-instance"}},"content":"This time [Deepthi](https://github.com/deepthidevaki) was joining me on my regular Chaos Day. :tada:\\n\\n[In the second last chaos day](/2021-02-23-automate-deployments-dist/index.md) I created an automated chaos experiment, which verifies that the deployments are distributed after a network partition. Later it turned out that this doesn\'t work for camunda cloud, only for our helm setup. [The issue](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/237) was that on our camunda cloud zeebe clusters we had no [NET_ADMIN](https://man7.org/linux/man-pages/man7/capabilities.7.html) capability to create ip routes (used for the network partitions). After discussing with our SRE\'s they proposed a good way to overcome this. On running chaos experiments, which are network related, we will patch our target cluster to add this capability. This means we don\'t need to add such functionality in camunda cloud and the related zeebe operate/controller. Big thanks to [Immi](https://github.com/hisImminence) and [David](https://github.com/Faffnir) for providing this fix.\\n\\n\\n**TL;DR;**\\n\\nWe were able to enhance the deployment distribution experiment and run it in the camunda cloud via testbench. We have enabled the experiment for Production M and L cluster plans. We had to adjust the rights for the testbench service account to make this work.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe already had a [prepared chaos experiment](https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/helm/deployment-distribution/experiment.json), but we needed to enhance that. Deepthi was so kind to create [PR](https://github.com/zeebe-io/zeebe-chaos/pull/50) for that.\\n\\n### Enhancement\\nThe changes contain a new step before creating the network partition on the deployment distribution experiment, see [here](https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/camunda-cloud/production-l/deployment-distribution/experiment.json#L25-L35).\\n\\n```json\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Enable net_admin capabilities\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"apply_net_admin.sh\\"\\n            },\\n            \\"pauses\\": {\\n                \\"after\\": 180\\n            }\\n        }\\n```\\n\\nThe `apply_net_admin.sh` contains the following code:\\n\\n```shell\\n\\n#!/bin/bash\\nset -euo pipefail\\n\\nscriptPath=$( cd \\"$(dirname \\"${BASH_SOURCE[0]}\\")\\" ; pwd -P )\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\n\\nCLUSTERID=${namespace%-zeebe}\\n\\nkubectl patch zb \\"$CLUSTERID\\" --type merge --patch=\'{\\"spec\\":{\\"controller\\":{\\"reconcileDisabled\\":true}}}\'\\nkubectl patch statefulset zeebe -n \\"$namespace\\" --patch \\"$(cat $scriptPath/net_admin_patch.yaml)\\"\\n```\\n\\nIt disables reconciliation for the target zeebe cluster and applies the following patch, which adds the `NET_ADMIN` capability:\\n\\n```yaml\\nspec:\\n  template:\\n    spec:\\n      containers:\\n        - name: \\"zeebe\\"\\n          securityContext:\\n            capabilities:\\n              add:\\n                - \\"NET_ADMIN\\"\\n```\\n\\nBig thanks to [Immi](https://github.com/hisImminence) and [David](https://github.com/Faffnir) for providing this fix.\\n\\nAfter we applied the patch, we need to make sure that the all pods are restarted and have the requested change. This is the reason we wait after the action for some minutes. This was the easiest way for us to think of, but ideally we find a better way here to make sure the patch was applied and we can continue.\\n\\n### Verification\\n\\nWe run this experiment with a Production L cluster (v1.0.0-alpha2) and it succeeded. This is quite nice, because this also contains already the rewritten [deployment distribution](https://github.com/camunda-cloud/zeebe/issues/6173) logic.\\n\\nTo verify whether the experiment really does something we checked the metrics and the logs. In the metrics we were not really able to tell that there was a network partition going on. There were no heart beats missing. The reason for that was that in the experiment the Leader for partition 3 (Node 1) and Leader for partition 1 (Node 0) have been disconnected. If we check the [partition distribution](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/docs/scripts/partitionDistribution.sh) we can see that they have no partitions in common. \\n\\n```shell\\n$ ./partitionDistribution.sh 6 8 3\\nDistribution:\\nP\\\\N|\\tN 0|\\tN 1|\\tN 2|\\tN 3|\\tN 4|\\tN 5\\nP 0|\\tL  |\\tF  |\\tF  |\\t-  |\\t-  |\\t-  \\nP 1|\\t-  |\\tL  |\\tF  |\\tF  |\\t-  |\\t-  \\nP 2|\\t-  |\\t-  |\\tL  |\\tF  |\\tF  |\\t-  \\nP 3|\\t-  |\\t-  |\\t-  |\\tL  |\\tF  |\\tF  \\nP 4|\\tF  |\\t-  |\\t-  |\\t-  |\\tL  |\\tF  \\nP 5|\\tF  |\\tF  |\\t-  |\\t-  |\\t-  |\\tL  \\nP 6|\\tL  |\\tF  |\\tF  |\\t-  |\\t-  |\\t-  \\nP 7|\\t-  |\\tL  |\\tF  |\\tF  |\\t-  |\\t-  \\n\\nPartitions per Node:\\nN 0: 4\\nN 1: 5\\nN 2: 5\\nN 3: 4\\nN 4: 3\\nN 5: 3\\n```\\n\\nFortunately we saw in the logs that the Node 1, was retrying to send the deployments and at some point it succeeds.\\n```shell\\n2021-03-23 13:11:54.163 CET\\nFailed to receive deployment response for partitions [2, 4, 7, 8] (topic \'deployment-response-2251799813685304\'). Retrying\\n2021-03-23 13:11:54.164 CET\\nPushed deployment 2251799813685304 to all partitions.\\n2021-03-23 13:11:54.164 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 2\\n2021-03-23 13:11:54.164 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 4\\n2021-03-23 13:11:54.164 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 8\\n2021-03-23 13:11:54.165 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 7\\n2021-03-23 13:11:54.175 CET\\nDeployment 2251799813685304 distributed to all partitions successfully.\\n2021-03-23 13:11:54.763 CET\\nFailed to receive deployment response for partitions [2, 4, 7, 8] (topic \'deployment-response-2251799813685306\'). Retrying\\n2021-03-23 13:11:54.764 CET\\nPushed deployment 2251799813685306 to all partitions.\\n2021-03-23 13:11:54.764 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 4\\n2021-03-23 13:11:54.764 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 2\\n2021-03-23 13:11:54.765 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 8\\n2021-03-23 13:11:54.765 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 7\\n2021-03-23 13:11:54.773 CET\\nDeployment 2251799813685306 distributed to all partitions successfully.\\n```\\n\\n### Testbench\\n\\nAfter the experiment has succeeded and we had verified the execution we run this again on testbench.\\n\\nWe saw a non completing chaos worker after checking the chaostoolkit logs, we saw that it was still in the phase of disconnecting the nodes, which was the same issue as before [#237](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/237).\\n\\n```shell\\n[2021-03-23 12:40:24 INFO] [activity:161] Action: Enable net_admin capabilities\\n[2021-03-23 12:40:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/apply_net_admin.sh\']\\n[2021-03-23 12:40:24 WARNING] [process:66] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2021-03-23 12:40:24 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 12:40:24 DEBUG] [activity:180]   => succeeded with \'{\'status\': 1, \'stdout\': \'\', \'stderr\': \'Error from server (Forbidden): zeebeclusters.cloud.camunda.io \\"cc108db7-768c-45cc-a6c5-098dc28f260c\\" is forbidden: User \\"system:serviceaccount:zeebe-chaos:zeebe-chaos-sa\\" cannot get resource \\"zeebeclusters\\" in API group \\"cloud.camunda.io\\" at the cluster scope\\\\n\'}\'\\n[2021-03-23 12:40:24 INFO] [activity:198] Pausing after activity for 180s...\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 INFO] [activity:161] Probe: All pods should be ready\\n[2021-03-23 12:43:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/verify-readiness.sh\']\\n[2021-03-23 12:43:24 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 12:43:24 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 12:43:24 DEBUG] [activity:180]   => succeeded with \'{\'status\': 0, \'stdout\': \'pod/zeebe-0 condition met\\\\npod/zeebe-1 condition met\\\\npod/zeebe-2 condition met\\\\npod/zeebe-3 condition met\\\\npod/zeebe-4 condition met\\\\npod/zeebe-5 condition met\\\\n\', \'stderr\': \\"+ source utils.sh\\\\n++ CHAOS_SETUP=cloud\\\\n++ getNamespace\\\\n+++ kubectl config view --minify --output \'jsonpath={..namespace}\'\\\\n++ namespace=cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n++ echo cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n+ namespace=cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n+ \'[\' cloud == cloud \']\'\\\\n+ kubectl wait --for=condition=Ready pod -l app.kubernetes.io/app=zeebe --timeout=900s -n cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n\\"}\'\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 INFO] [activity:161] Action: Create network partition between leaders\\n[2021-03-23 12:43:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/disconnect-leaders.sh\']\\n```\\n\\nIn the logs we also saw that the patch didn\'t worked as expected with the used service account, so we need to fix here the permission and after that it should hopefully work. :crossed_fingers:\\n\\n```shell\\n\'Error from server (Forbidden): zeebeclusters.cloud.camunda.io \\"XXX\\" is forbidden: User \\"system:serviceaccount:zeebe-chaos:zeebe-chaos-sa\\" cannot get resource \\"zeebeclusters\\" in API group \\"cloud.camunda.io\\" at the cluster scope\\\\n\'\\n```\\n\\nAfter checking with Immi, we were sure that we need to adjust the [serviceaccount roles](https://github.com/zeebe-io/zeebe-cluster-testbench/blob/develop/core/chaos-workers/deployment/service-account/zeebe-chaos-role.yaml#L9). After changing the apiGroups to `[\\"*\\"]` the experiments are running in testbench and the patch can be applied. We can now see in the log the following:\\n\\n```shell\\n[2021-03-23 14:21:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/apply_net_admin.sh\']\\n[2021-03-23 14:21:25 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 14:21:25 DEBUG] [activity:180]   => succeeded with \'{\'status\': 0, \'stdout\': \'zeebecluster.cloud.camunda.io/cc108db7-768c-45cc-a6c5-098dc28f260c patched\\\\nstatefulset.apps/zeebe patched\\\\n\', \'stderr\': \'\'}\'\\n```\\n\\nThanks for participating [Deepthi](https://github.com/deepthidevaki).\\n\\n#### Found Bugs\\n\\n##### Re-connecting might fail\\n\\nWe realized during testing the experiment that the re-connecting might fail, because the pod can be rescheduled and then a ip route can\'t be delete since it no longer exist. [This is now fixed](https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/scripts/connect-leaders.sh#L45-L48). We check for existence of the command `ip`, if this doesn\'t exist we know the pod was restarted and we ignore it.\\n\\n\\n*Before:*\\n\\n```shell\\nfunction connect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route del unreachable \\"$targetIp\\"\\n\\n}\\n```\\n\\n*After:*\\n\\n```shell\\n\\nfunction connect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n if command -v ip\\n then\\n     kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route del unreachable \\"$targetIp\\"\\n fi\\n}\\n```"},{"id":"/2021/03/09/cont-workflow-instance","metadata":{"permalink":"/zeebe-chaos/2021/03/09/cont-workflow-instance","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-03-09-cont-workflow-instance/index.md","source":"@site/blog/2021-03-09-cont-workflow-instance/index.md","title":"Fault-tolerant processing of process instances","description":"Today I wanted to add another chaos experiment, to increase our automated chaos experiments collection. This time we will deploy a process model (with timer start event), restart a node and complete the process instance via zbctl.","date":"2021-03-09T00:00:00.000Z","formattedDate":"March 9, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":5.98,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Fault-tolerant processing of process instances","date":"2021-03-09T00:00:00.000Z","categories":["chaos_experiment","broker","processing"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Camunda Cloud network partition","permalink":"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition"},"nextItem":{"title":"Automating Deployment Distribution Chaos Experiment","permalink":"/zeebe-chaos/2021/02/23/automate-deployments-dist"}},"content":"Today I wanted to add another chaos experiment, to increase our automated chaos experiments collection. This time we will deploy a process model (with timer start event), restart a node and complete the process instance via `zbctl`.\\n\\n**TL;DR;**\\n\\nI was able to create the chaos toolkit experiment. It shows us that we are able to restore our state after fail over, which means we can trigger timer start events to create process instances even if they have been deployed before fail-over. Plus we are able to complete these instances.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor testing, I have run our normal setup of three nodes, three partitions and replication factor three in our zeebe gke cluster.\\nLater I want to automate the experiment against the production cluster plans.\\n\\n### Expected\\n\\nWe want to verify whether the processing of process instances continues even if we restart a leader in between. For that we do the following experiment:\\n\\n1. Verify Steady State: All Pods are ready\\n  2. Introduce Chaos:\\n      1. *Action:* Deploy process with timer start event (`PT1M`)\\n      2. *Action:* Restart leader of partition one\\n      3. *Probe:* We can activate and complete an job of a specific type\\n3. Verify Steady State: All Pods are ready\\n\\n**Note:** *We know that timer start events currently only scheduled on partition one, which means it is enough to just restart the leader of partition one for our experiment.* We use this property to reduce the blast radius. Later we could introduce an intermediate timer catch event and start many workflow instances on multiple partitions to verify whether this works on all partitions.\\n\\nModel will look like this:\\n\\n![model](chaosTimerStart.png))\\n\\n### Experiment Definition\\n\\nThe experiment definition looks like the following:\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe process instance continuation\\",\\n    \\"description\\": \\"Zeebe processing of process instances should be fault-tolerant. Zeebe should be able to handle fail-overs and continue process instances after a new leader starts with processing.\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Deploy process model\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"deploy-specific-model.sh\\", \\n                \\"arguments\\": [ \\"chaosTimerStart.bpmn\\" ]\\n            }\\n        },\\n         {\\n              \\"type\\": \\"action\\",\\n              \\"name\\": \\"Restart partition leader\\",\\n              \\"provider\\": {\\n                   \\"type\\": \\"process\\",\\n                   \\"path\\": \\"shutdown-gracefully-partition.sh\\",\\n                   \\"arguments\\": [ \\"Leader\\", \\"1\\" ]\\n              }\\n         },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Complete process instance\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"complete-instance.sh\\",\\n                \\"arguments\\": [\\"chaos\\"],\\n                \\"timeout\\": 900\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\n#### Timer start events\\n\\nIn order to trigger the timer start event, after 1 minute, after successful deployment. I used the following [feel expression](https://docs.camunda.io/docs/reference/feel/what-is-feel): `=now()+ duration(\\"PT1M\\")`. This is necessary, since only date and cycle are supported for [timer start events](https://docs.camunda.io/docs/reference/bpmn-workflows/timer-events/timer-events/#timer-start-events).\\n\\n#### Continue process instance\\n\\nTo continue and finish the process instance, we will activate the job with the `chaos` job type and complete that job. If there is no job to activate/complete we will loop here until we reach the timeout. With this we can make sure that the timer was scheduled and the process instance was created even after restart. We are not using here an job worker, since the activate-complete commands make it currently easier to mark it as success or fail. With the job worker we would introduce another concurrency layer. \\n\\n```shell\\n#!/bin/bash\\nset -euox pipefail\\n\\nsource utils.sh\\n\\njobType=$1\\nnamespace=$(getNamespace)\\npod=$(getGateway)\\n\\nfunction completeJob() {\\n  # we want to first activate the job with the given job type and then complete it with the given job key\\n  # if we are not able to activate an job with the given type the function will return an error\\n  # and retried from outside\\n  jobs=$(kubectl exec -it \\"$pod\\" -n \\"$namespace\\" -- zbctl --insecure activate jobs \\"$jobType\\")\\n  key=$(echo \\"$jobs\\" | jq -r \'.jobs[0].key\')\\n  kubectl exec -it \\"$pod\\" -n \\"$namespace\\" -- zbctl --insecure complete job \\"$key\\"\\n}\\n\\nretryUntilSuccess completeJob\\n```\\n\\n### Outcome\\n\\nAfter running this experiment we get the following output, which shows us that the experiment **SUCCEEDED**.\\n\\n```shell\\n(chaostk) [zell helm/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run process-continuation/experiment.json \\n[2021-03-09 13:16:34 INFO] Validating the experiment\'s syntax\\n[2021-03-09 13:16:34 INFO] Experiment looks valid\\n[2021-03-09 13:16:34 INFO] Running experiment: Zeebe process instance continuation\\n[2021-03-09 13:16:34 INFO] Steady-state strategy: default\\n[2021-03-09 13:16:34 INFO] Rollbacks strategy: default\\n[2021-03-09 13:16:34 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-03-09 13:16:34 INFO] Probe: All pods should be ready\\n[2021-03-09 13:16:35 INFO] Steady state hypothesis is met!\\n[2021-03-09 13:16:35 INFO] Playing your experiment\'s method now...\\n[2021-03-09 13:16:35 INFO] Action: Deploy process model\\n[2021-03-09 13:16:37 INFO] Action: Restart partition leader\\n[2021-03-09 13:16:46 INFO] Probe: Complete process instance\\n[2021-03-09 13:17:38 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-03-09 13:17:38 INFO] Probe: All pods should be ready\\n[2021-03-09 13:17:38 INFO] Steady state hypothesis is met!\\n[2021-03-09 13:17:38 INFO] Let\'s rollback...\\n[2021-03-09 13:17:38 INFO] No declared rollbacks, let\'s move on.\\n```\\n\\n#### Testbench\\n\\nTbd.\\n\\nWe have currently some problems with running the chaos experiments against camunda cloud, like [testbench#247](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/247) or [testbench#248](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/248). This is the reason why I postponed it.\\n\\n#### Found Bugs\\n\\n##### Chaos Experiments not working correctly\\nI realized that most of the experiments are no longer run correctly, since they referring to `\\"Leader\\"` as the raft role, where the raft role in the topology is `LEADER`. This causes that on some experiments pods are not really restarted.\\n\\nAlmost everywhere we use constructs like:\\n\\n```shell\\n\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Terminate leader of partition 3\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"shutdown-gracefully-partition.sh\\",\\n                \\"arguments\\": [ \\"Leader\\", \\"3\\" ]\\n            }\\n        }\\n```\\n\\nIn the utils script we use a `jq` expression to get the node which is in a certain state for a certain partition.\\n\\nThe `jq` expression looks like this:\\n```shell\\n  index=$(echo \\"$topology\\" | jq \\"[.brokers[]|select(.partitions[]| select(.partitionId == $partition) and .role == \\\\\\"$state\\\\\\")][0].nodeId\\")\\n```\\n`jq` is not able to find the raft `state` which is returned by topology, if we use not capital letters.\\n\\nWhen we run the chaos experiment we see warnings like:\\n\\n```shell\\n(chaostk) [zell helm/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run process-continuation/experiment.json \\n[2021-03-09 13:27:01 INFO] Validating the experiment\'s syntax\\n[2021-03-09 13:27:01 INFO] Experiment looks valid\\n[2021-03-09 13:27:01 INFO] Running experiment: Zeebe process instance continuation\\n[2021-03-09 13:27:01 INFO] Steady-state strategy: default\\n[2021-03-09 13:27:01 INFO] Rollbacks strategy: default\\n[2021-03-09 13:27:01 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-03-09 13:27:01 INFO] Probe: All pods should be ready\\n[2021-03-09 13:27:02 INFO] Steady state hypothesis is met!\\n[2021-03-09 13:27:02 INFO] Playing your experiment\'s method now...\\n[2021-03-09 13:27:02 INFO] Action: Deploy process model\\n[2021-03-09 13:27:03 INFO] Action: Restart partition leader\\n[2021-03-09 13:27:05 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n\\n```\\n\\nThe experiment still continues (in effect it does nothing, since it was not able to find the right pod to restart).\\nConverting the state into capital letters fixes this issue easily.\\n\\n```shell\\nfunction getIndexOfPodForPartitionInState()\\n{\\n  partition=\\"$1\\"\\n  # expect caps for raft roles\\n  state=${2^^}\\n```\\n\\n##### Redeployment causes retrigger timer\\n\\nDuring running the chaos experiment and testing the scripts I realized that the timer start event is retriggered everytime I redeployed the exact deployment, which was kind of unexpected.\\nI created a bug issue for that [zeebe#6515](https://github.com/camunda-cloud/zeebe/issues/6515)."},{"id":"/2021/02/23/automate-deployments-dist","metadata":{"permalink":"/zeebe-chaos/2021/02/23/automate-deployments-dist","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-02-23-automate-deployments-dist/index.md","source":"@site/blog/2021-02-23-automate-deployments-dist/index.md","title":"Automating Deployment Distribution Chaos Experiment","description":"This time I wanted to automate a chaos experiment via the ChaosToolkit, which I did on the last chaos day. For a recap check out the last chaos day summary.","date":"2021-02-23T00:00:00.000Z","formattedDate":"February 23, 2021","tags":[{"label":"tests","permalink":"/zeebe-chaos/tags/tests"}],"readingTime":6.855,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Automating Deployment Distribution Chaos Experiment","date":"2021-02-23T00:00:00.000Z","categories":["chaos_experiment","broker","network","deployment"],"tags":["tests"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Fault-tolerant processing of process instances","permalink":"/zeebe-chaos/2021/03/09/cont-workflow-instance"},"nextItem":{"title":"Deployment Distribution","permalink":"/zeebe-chaos/2021/01/26/deployments"}},"content":"This time I wanted to automate a chaos experiment via the [ChaosToolkit](https://chaostoolkit.org/), which I did on the last chaos day. For a recap check out the [last chaos day summary](/2021-01-26-deployments/index.md).\\n\\n**TL;DR;**\\n\\nI was able to automate the deployment distribution chaos experiment successfully and deployed it on testbench for a `Production - M` cluster plan.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor testing, I have run our normal setup of three nodes, three partitions and replication factor three.\\nLater I wanted to automate the experiment against the production cluster plans.\\n\\n### Expected\\n\\nWe want to verify whether deployments are still distributed after a network partition, for that we will write the following chaos experiment:\\n\\n1. Verify Steady State: All Pods are ready\\n  2. Introduce Chaos:\\n      1. *Action:* Disconnect two leaders (Leader of partition one and another leader)\\n      2. *Action:* Deploy multiple versions of a workflow\\n      3. *Action:* Connect two leaders again\\n      4. *Probe*: I can create workflow instance on all partitions with the last workflow version\\n3. Verify Steady State: All Pods are ready\\n\\n### Experiment Definition\\n\\nThe experiment definition looks like the following:\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe deployment distribution\\",\\n    \\"description\\": \\"Zeebe deployment distribution should be fault-tolerant. Zeebe should be able to handle network outages and fail-overs and distribute the deployments after partitions are available again.\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Create network partition between leaders\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"disconnect-leaders.sh\\"\\n            }\\n        },\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Deploy different deployment versions.\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"deploy-different-versions.sh\\",\\n                \\"arguments\\": [\\"Follower\\", \\"3\\"]\\n            }\\n        },\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Delete network partition\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"connect-leaders.sh\\"\\n            }\\n        },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Create workflow instance of latest version on partition one\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"start-instance-on-partition-with-version.sh\\",\\n                \\"arguments\\": [\\"1\\", \\"10\\"],\\n                \\"timeout\\": 900\\n            }\\n        },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Create workflow instance of latest version on partition two\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"start-instance-on-partition-with-version.sh\\",\\n                \\"arguments\\": [\\"2\\", \\"10\\"],\\n                \\"timeout\\": 900\\n            }\\n        },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Create workflow instance of latest version on partition three\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"start-instance-on-partition-with-version.sh\\",\\n                \\"arguments\\": [\\"3\\", \\"10\\"],\\n                \\"timeout\\": 900\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\n\\n#### Create network partition between leaders\\n\\nWe reuse a script which we introduce in earlier chaos days. It needed to be improved a bit, since we haven\'t handled clusters where one node leads multiple partitions.\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\n# determine leader for partition one\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"LEADER\\")\\nleader=$(getBroker \\"$index\\")\\nleaderIp=$(kubectl get pod \\"$leader\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n# determine leader for partition three\\nindex=$(getIndexOfPodForPartitionInState \\"3\\" \\"LEADER\\")\\nleaderTwo=$(getBroker \\"$index\\")\\nleaderTwoIp=$(kubectl get pod \\"$leaderTwo\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\nif [ \\"$leaderTwo\\" == \\"$leader\\" ]\\nthen\\n  # determine leader for partition two\\n  index=$(getIndexOfPodForPartitionInState \\"2\\" \\"LEADER\\")\\n  leaderTwo=$(getBroker \\"$index\\")\\n  leaderTwoIp=$(kubectl get pod \\"$leaderTwo\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n  if [ \\"$leaderTwo\\" == \\"$leader\\" ]\\n  then\\n    # We could try to kill the pod and hope that he is not able to become leader again,\\n    # but there is a high chance that it is able to do so after restart. It can make our test fragile,\\n    # especially if we want to connect again, which is the reason why we do nothing in that case.\\n    exit\\n  fi\\nfi\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$leader\\" \\"$leaderTwoIp\\"\\nretryUntilSuccess disconnect \\"$leaderTwo\\" \\"$leaderIp\\" \\n```\\n\\n\\n#### Delete network partition\\n\\nLooks similar to the disconnect script.\\n\\n##### Deploy different deployment versions\\n\\nThis script is interesting. My first approach was to have one workflow model, where I replace an comment via `sed` before redeploying. Later I realized it is much easier to just have two versions of the same worfklow model in the repository and deploy them in an alternating manner. This reduced the dependecy of an extra tool (`sed`), which might not be available everywhere or work differently on different linux distributions. \\n\\n```shell\\n#!/bin/bash\\n\\nset -exuo pipefail\\n\\nscriptPath=$( cd \\"$(dirname \\"${BASH_SOURCE[0]}\\")\\" ; pwd -P )\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\npod=$(getGateway)\\n\\nbpmnPath=\\"$scriptPath/../bpmn/\\"\\n\\n# we put both together in one function to retry both, because it might be that pod has been restarted\\n# then the model is not longer on the node, which cause endless retries of deployments\\nfunction deployModel() {\\n  kubectl cp \\"$bpmnPath\\" \\"$pod:/tmp/\\" -n \\"$namespace\\"\\n\\n  for i in {1..5}\\n  do\\n    # the models differ in one line, the share the same name and process id\\n    # if we deploy them after another it will create two different deployment versions\\n    # the deploy command only compares the last applied deployment - so we can do that in a loop to cause\\n    # multiple deployments\\n    kubectl exec \\"$pod\\" -n \\"$namespace\\" -- sh -c \\"zbctl deploy /tmp/bpmn/multi-version/multiVersionModel.bpmn --insecure\\"\\n    kubectl exec \\"$pod\\" -n \\"$namespace\\" -- sh -c \\"zbctl deploy /tmp/bpmn/multi-version/multiVersionModel_v2.bpmn --insecure\\"\\n  done\\n}\\n\\nretryUntilSuccess deployModel\\n```\\n\\nWhen running this script we deploy `10` new versions of the workflow `multiVersion`.\\n\\n#### Create workflow instance of latest version on partition X\\n\\nThe following script allows us to be sure that we can create a workflow instance on a specific partition with the given version of the `multiVersion` workflow.\\nThis means we can verify that on all partitions the last deployment version is deployed/distributed.\\n\\n```shell\\n#!/bin/bash\\nset -xo pipefail\\n\\nif [ -z \\"$1\\" ]\\nthen\\n  echo \\"Please provide an required partition!\\"\\n  exit 1\\nfi\\n\\nif [ -z \\"$2\\" ]\\nthen\\n  echo \\"Please provide an required deployment version!\\"\\n  exit 1\\nfi\\n\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\npod=$(getGateway)\\n\\nrequiredPartition=$1\\nrequiredDeploymentVersion=$2\\nprocessId=\\"multiVersion\\"\\n\\n# we put all into one function because we need to make sure that even after preemption the\\n# dependency are installed, which is in this case is the deployment\\nfunction startInstancesOnPartition() {\\n\\n  partition=0\\n  until [[ \\"$partition\\" -eq \\"$requiredPartition\\" ]]; do\\n    workflowInstanceKey=$(kubectl exec \\"$pod\\" -n \\"$namespace\\" -- zbctl create instance \\"$processId\\" --version \\"$requiredDeploymentVersion\\" --insecure)\\n    workflowInstanceKey=$(echo \\"$workflowInstanceKey\\" | jq \'.workflowInstanceKey\')\\n    partition=$(( workflowInstanceKey >> 51 ))\\n    echo \\"Started workflow with key $workflowInstanceKey, corresponding partition $partition\\"\\n  done\\n}\\n\\nretryUntilSuccess startInstancesOnPartition\\n```\\n\\n### Outcome\\n\\nAfter running this experiment we get the following output, which shows us that the experiment **SUCCEEDED**.\\n\\n```shell\\n(chaostk) [zell camunda-cloud/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run production-m/deployment-distribution/experiment.json \\n[2021-02-23 14:15:06 INFO] Validating the experiment\'s syntax\\n[2021-02-23 14:15:06 INFO] Experiment looks valid\\n[2021-02-23 14:15:06 INFO] Running experiment: Zeebe deployment distribution\\n[2021-02-23 14:15:06 INFO] Steady-state strategy: default\\n[2021-02-23 14:15:06 INFO] Rollbacks strategy: default\\n[2021-02-23 14:15:06 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-02-23 14:15:06 INFO] Probe: All pods should be ready\\n[2021-02-23 14:15:07 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2021-02-23 14:15:10 INFO] Steady state hypothesis is met!\\n[2021-02-23 14:15:10 INFO] Playing your experiment\'s method now...\\n[2021-02-23 14:15:10 INFO] Action: Create network partition between leaders\\n[2021-02-23 14:15:26 INFO] Action: Deploy thousand different deployment versions.\\n[2021-02-23 14:15:31 INFO] Action: Delete network partition\\n[2021-02-23 14:15:43 INFO] Probe: Create workflow instance of latest version on partition one\\n[2021-02-23 14:15:43 INFO] Probe: Create workflow instance of latest version on partition two\\n[2021-02-23 14:15:51 INFO] Probe: Create workflow instance of latest version on partition three\\n[2021-02-23 14:15:52 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-02-23 14:15:52 INFO] Probe: All pods should be ready\\n[2021-02-23 14:15:52 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2021-02-23 14:15:55 INFO] Steady state hypothesis is met!\\n[2021-02-23 14:15:55 INFO] Let\'s rollback...\\n[2021-02-23 14:15:55 INFO] No declared rollbacks, let\'s move on.\\n[2021-02-23 14:15:55 INFO] Experiment ended with status: completed\\n```\\n\\n#### Testbench\\n\\nI executed the new experiment via zeebe testbench, to verify that this works with the `Production - M` cluster plan and it was successful :muscle:\\n\\n![operate](operate-success.png)\\n\\n```json\\n{\\"results\\":[\\"production-m/deployment-distribution/experiment.json completed successfully\\",\\"production-m/follower-restart/experiment.json completed successfully\\",\\"production-m/follower-terminate/experiment.json completed successfully\\",\\"production-m/leader-restart/experiment.json completed successfully\\",\\"production-m/leader-terminate/experiment.json completed successfully\\",\\"production-m/msg-correlation/experiment.json completed successfully\\",\\"production-m/multiple-leader-restart/experiment.json completed successfully\\",\\"production-m/snapshot-corruption/experiment.json completed successfully\\",\\"production-m/stress-cpu-on-broker/experiment.json completed successfully\\",\\"production-m/worker-restart/experiment.json completed successfully\\"]}\\n```"},{"id":"/2021/01/26/deployments","metadata":{"permalink":"/zeebe-chaos/2021/01/26/deployments","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-01-26-deployments/index.md","source":"@site/blog/2021-01-26-deployments/index.md","title":"Deployment Distribution","description":"On this chaos day we wanted to experiment a bit with deployment\'s and there distribution.","date":"2021-01-26T00:00:00.000Z","formattedDate":"January 26, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":10.855,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Deployment Distribution","date":"2021-01-26T00:00:00.000Z","categories":["chaos_experiment","broker","network"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Automating Deployment Distribution Chaos Experiment","permalink":"/zeebe-chaos/2021/02/23/automate-deployments-dist"},"nextItem":{"title":"Network partitions","permalink":"/zeebe-chaos/2021/01/19/network-partition"}},"content":"On this chaos day we wanted to experiment a bit with deployment\'s and there distribution.\\n\\nWe run a chaos experiment with deploying multiple workflows and disconnecting two leaders. We verified whether deployments are distributed later. The chaos experiment was successful and showed a bit how fault tolerant deployment distribution is. :muscle:\\n\\n\x3c!--truncate--\x3e\\n\\n## Deployments\\n\\nIn order to continue here we need to explain how the workflow deployment and distribution actually works, if you know it then you can skip this section :wink:.\\n\\n### Deployment Distribution\\n\\nIf you deploy a workflow and you have multiple partitions, Zeebe needs to make sure that all partitions eventually have the same version of the deployment. This is done with via the deployment distribution.\\n\\n![distribution](distribution.png)\\n\\nOn deploying a workflow via a client, like the java client, we send a deployment command to the gateway. The gateway sends the received deployment to the \\"deployment partition\\", which is partition one. The partition one is in charge of distributing the deployment. When the client receives a response for the deployment command, this means that the deployment is written/created on partition one. It doesn\'t mean that it is distributed to all other partitions. The distribution is done asynchronously. \\n\\nThis can cause issues, if you want to create workflow instances immediately after the deployment. If you try to create a workflow instance on a partition which hasn\'t received the deployment yet, then this creation will fail. The gateway sends commands, like workflow instance creation, in a round-robin fashion and if you have multiple partitions, then the chance that you hit another partition is quite high.\\n\\n#### Reasoning\\n\\nYou may ask why we build it like that. Let me explain this a bit more.\\n\\n##### Why isn\'t the gateway in charge of distributing the deployment?\\n\\nBecause the gateway is stateless. If the gateway restarts it has no state it can replay, so it is not able to retry the deployment distributions. If the gateway failed during deployment distribution some partition might lose the deployment update. In the broker we replay the state, which means we can detect whether we distributed the deployment already to a certain partition if not we can retry it.\\n\\n##### Why the response isn\'t send after the distribution is done. Why it is build in an asynchronous way?\\n\\nThe simple answer would be, because it can take a long time until the deployment is distributed to all partitions. In a distributed system it is likely that a service fail, which means if one partition is not available the distribution is not complete. With the current approach you can already start creating instances at least at partition one and you can retry the requests if you get an rejection.\\n\\nAfter the small excursion of how deployment distribution look like and why, we can start with our experiment to verify that it works as expected.\\n\\n## Chaos Experiment\\n\\nWe have a standard setup of three nodes, three partitions and replication factor three.\\n\\n### Expected\\n\\nWe deploy multiple versions (1000+) of a deployment and assume that at some point all deployments are distributed on all partitions and that we are able to create a workflow instance with the latest version on all partitions. The system should remain stable during distributing the deployments. This can be seen as the steady state.\\n\\nIf we now disconnect a leader of a different partition (different from partition one) with the leader of partition one, then the deployments can\'t be distributed. If we try to create workflow instances on that partition we should receive rejection\'s. After we connect them again the deployments should be distributed and we should be able to create workflow instances on that specific partition.\\n\\n### Actual\\n\\n#### Steady State\\n\\nFollowing Java code was used to verify the steady state. In order to find out on which partition the workflow instances was created I used the `Protocol#decodePartitionId` method, which is available in the zeebe protocol module. This functionality and the property of the key\'s was already quite useful in the past on doing chaos experiments.\\n\\n\\n```java\\nimport io.zeebe.client.ZeebeClient;\\nimport io.zeebe.model.bpmn.Bpmn;\\nimport io.zeebe.protocol.Protocol;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\npublic class ChaosDayMain {\\n\\n\\n  private static final Logger LOG = LoggerFactory.getLogger(ChaosDayMain.class.getName());\\n\\n  public static void main(String[] args) {\\n    final var zeebeClient = ZeebeClient.newClientBuilder().usePlaintext().gatewayAddress(\\"localhost:26500\\").build();\\n\\n    final var topology = zeebeClient.newTopologyRequest().send().join();\\n    LOG.info(\\"{}\\", topology);\\n\\n    var lastVersion = 0;\\n    for (int i = 0; i < 1_000; i++) {\\n\\n      final var modelInstance = Bpmn.createExecutableProcess(\\"workflow\\").startEvent().endEvent().done();\\n\\n      final var workflow = zeebeClient.newDeployCommand()\\n          .addWorkflowModel(modelInstance, \\"workflow\\").send().join();\\n      lastVersion = workflow.getWorkflows().get(0).getVersion();\\n    }\\n    LOG.info(\\"Last version deployed: {}\\", lastVersion);\\n\\n    final var partitions = new ArrayList<>(List.of(1, 2, 3));\\n\\n    while (!partitions.isEmpty()) {\\n      try {\\n        final var workflowInstanceEvent = zeebeClient.newCreateInstanceCommand()\\n            .bpmnProcessId(\\"workflow\\")\\n            .version(lastVersion).send().join();\\n        final var workflowInstanceKey = workflowInstanceEvent.getWorkflowInstanceKey();\\n        final var partitionId = Protocol.decodePartitionId(workflowInstanceKey);\\n\\n        partitions.remove(Integer.valueOf(partitionId));\\n        LOG.info(\\"Created workflow instance on partition {}, {} partitions left ({}).\\", partitionId, partitions.size(), partitions);\\n      } catch (Exception e) {\\n        // retry\\n        LOG.info(\\"Failed to create workflow instance\\", e);\\n      }\\n\\n    }\\n  }\\n}\\n```\\n\\n**Small Note** the line: `Bpmn.createExecutableProcess(\\"workflow\\").startEvent().endEvent().done()` will always create a new version of a workflow, since internally new id\'s are generated.\\n\\nAfter running the code above we can see following output:\\n\\n```\\n13:45:00.606 [] [main] INFO  ChaosDayMain - TopologyImpl{brokers=[BrokerInfoImpl{nodeId=0, host=\'zell-chaos-zeebe-0.zell-chaos-zeebe.zell-chaos.svc.cluster.local\', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=LEADER, health=HEALTHY}]}, BrokerInfoImpl{nodeId=2, host=\'zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc.cluster.local\', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=LEADER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=LEADER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=FOLLOWER, health=HEALTHY}]}, BrokerInfoImpl{nodeId=1, host=\'zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc.cluster.local\', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=FOLLOWER, health=HEALTHY}]}], clusterSize=3, partitionsCount=3, replicationFactor=3, gatewayVersion=0.27.0-SNAPSHOT}\\n13:46:04.384 [] [main] INFO  ChaosDayMain - Last version deployed: 6914\\n13:46:04.434 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 2 partitions left ([2, 3]).\\n13:46:04.505 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n13:46:04.571 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 3, 0 partitions left ([]).\\n```\\n\\nAs you can see in the version count I run it already multiple times :). With this we are able to verify our steady state, that the deployments are distributed to all partitions and that we are able to create workflow instances with the specific (last) version on all partitions.\\n\\n*Side note, I needed multiple runs because there was a leader change (of partition one) in between and I had to adjust the code etc. Needs to be investigated whether the deployment distribution caused that.*\\n\\n#### Chaos Injection (Method)\\n\\nThe following topology we used to determined who to disconnect:\\n\\n```yaml\\nCluster size: 3\\nPartitions count: 3\\nReplication factor: 3\\nGateway version: 0.27.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - zell-chaos-zeebe-0.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\\n    Version: 0.27.0-SNAPSHOT\\n    Partition 1 : Leader, Healthy\\n    Partition 2 : Leader, Healthy\\n    Partition 3 : Follower, Healthy\\n  Broker 1 - zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\\n    Version: 0.27.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy\\n    Partition 2 : Follower, Healthy\\n    Partition 3 : Follower, Healthy\\n  Broker 2 - zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\\n    Version: 0.27.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy\\n    Partition 2 : Follower, Healthy\\n    Partition 3 : Leader, Healthy\\n```\\n\\nBased on the work of the last chaos days we are able to disconnect brokers easily.\\n```sh\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\n# determine leader for partition one\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"LEADER\\")\\nleader=$(getBroker \\"$index\\")\\nleaderIp=$(kubectl get pod \\"$leader\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n# determine leader for partition three\\n\\nindex=$(getIndexOfPodForPartitionInState \\"3\\" \\"FOLLOWER\\")\\nleaderTwo=$(getBroker \\"$index\\")\\nleaderTwoIp=$(kubectl get pod \\"$leaderTwo\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$leader\\" \\"$leaderTwoIp\\"\\nretryUntilSuccess disconnect \\"$leaderTwo\\" \\"$leaderIp\\" \\n\\n```\\n\\nWe used partition three here, since the leader of partition one and two are the same node. After disconnecting and running the Java code from above, we get the following output:\\n\\n```\\n14:11:56.655 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:11:56.713 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:11:56.777 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\\nio.zeebe.client.api.command.ClientStatusException: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\t... 1 more\\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\\n\\tat java.lang.Thread.run(Thread.java:834) ~[?:?]\\n14:11:56.846 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:11:56.907 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:11:56.971 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\\nio.zeebe.client.api.command.ClientStatusException: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\t... 1 more\\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\\n\\tat java.lang.Thread.run(Thread.java:834) ~[?:?]\\n```\\nWhich is of course expected, since the deployment is not available at the third partition.\\n\\nIn the stackdriver we see also warnings regarding the deployment distribution:\\n```\\n2021-01-26 14:11:49.439 CET\\nFailed to push deployment to node 2 for partition 3\\n2021-01-26 14:11:49.439 CET\\nFailed to push deployment to node 2 for partition 3\\n2021-01-26 14:11:49.439 CET\\nFailed to push deployment to node 2 for partition 3\\n```\\n\\nAfter we connected the leaders again (deleting the ip route), we can see in the application log that the exception changed to deadline exceeded and at some point we are able to create a workflow instance on partition three.\\n\\n```\\n14:16:21.958 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:16:22.020 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:16:32.032 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\\nio.zeebe.client.api.command.ClientStatusException: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\\n\\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\t... 1 more\\nCaused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\\n\\tat java.lang.Thread.run(Thread.java:834) ~[?:?]\\n14:16:32.062 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:16:32.125 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:16:37.596 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 3, 0 partitions left ([]).\\n```\\n\\nWe see the deadline exceeded, because the processor is not able to send the response\'s in time. The reason for that is probably because we have so many deployments to process, which have been pushed by the partition one.\\n\\nThe resource consumption around that time look ok. We can see that the memory spikes a bit, but it recovers later. On the CPU graph we can see when we connected the nodes again.\\n\\n![res](res.png))\\n\\n### Result\\n\\nThe chaos experiment was successful. The deployment was distributed even after a network disconnect and we were able to create workflow instance of the latest version on all partitions at the end.\\n\\nWith this experiment we were able to show that the deployment distribution is fault tolerant in way that it can handle unavailability of other partitions. This means eventually all partitions will receive there deployment\'s and we are able to create workflow instances on these partitions. \\n\\n#### Further work\\n\\nFurther possible experiments would be to restart the leader of partition one to see that even after restart we re-distribute the deployments. It is probably also interesting to see how the distribution behaves on more partitions than three.\\n\\nDuring the experiment we have observed some leader changes. It needs to be investigated further, whether this was related to the deployments or something different. It is probably also interesting to see how it behaves with larger deployments, also resource consumption wise."},{"id":"/2021/01/19/network-partition","metadata":{"permalink":"/zeebe-chaos/2021/01/19/network-partition","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-01-19-network-partition/index.md","source":"@site/blog/2021-01-19-network-partition/index.md","title":"Network partitions","description":"As you can see, I migrated the old chaos day summaries to github pages, for better readability.","date":"2021-01-19T00:00:00.000Z","formattedDate":"January 19, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.03,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Network partitions","date":"2021-01-19T00:00:00.000Z","categories":["chaos_experiment","broker","network"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Deployment Distribution","permalink":"/zeebe-chaos/2021/01/26/deployments"},"nextItem":{"title":"Disconnect Leader and one Follower","permalink":"/zeebe-chaos/2021/01/07/disconnect-leader-and-follower"}},"content":"As you can see, I migrated the old chaos day summaries to github pages, for better readability. \\nI always wanted to play around with github pages and jekyll so this was a good opportunity. I hope you like it. :smile:\\n\\nOn the last Chaos Day, we experimented with disconnecting a Leader and *one* follower. We expected no bigger disturbance, since we still have quorum and can process records. Today I want to experiment with bigger network partitions.\\n\\n * In the first chaos experiment: I had a cluster of 5 nodes and split that into two groups, the processing continued as expected, since we had still quorum. :muscle:\\n * In the second chaos experiment: I split the cluster again into two groups, but this time we added one follower of the bigger group to the smaller group after snapshot was taken and compaction was done. The smaller group needed to keep up with the newer state, before new processing can be started again, but everything worked fine.\\n\\n\x3c!--truncate--\x3e\\n\\n## First Chaos Experiment\\n\\nSay we have cluster of 5 nodes, one partition with replication factor 3 and we split the cluster in two parts (2 nodes and 3 nodes).\\n\\n### Expected\\n\\nWe expect if we partition two followers away that one part of the cluster can still continue, since it has quorum. Quorum is defined as `quorum=floor(nodes/2) + 1`\\n\\n### Actual\\n\\n![general](general.png)\\n\\nWhen partitioning two followers, this means we would have two groups. First group would be Broker-0 and Broker-1, the second group contains then Broker-2, Broker-3 and Broker-4. I adjusted the disconnect script from the last chaos day a bit. It looks now like this:\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\n# this scripts expects a setup of 5 nodes with replication factor 5 or higher\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\nbroker0=$(getBroker \\"0\\")\\nbroker0Ip=$(kubectl get pod \\"$broker0\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker1=$(getBroker \\"1\\")\\nbroker1Ip=$(kubectl get pod \\"$broker1\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker2=$(getBroker \\"2\\")\\nbroker2Ip=$(kubectl get pod \\"$broker2\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker3=$(getBroker \\"3\\")\\nbroker3Ip=$(kubectl get pod \\"$broker3\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker4=$(getBroker \\"4\\")\\nbroker4Ip=$(kubectl get pod \\"$broker4\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\n# Broker 0 and 1 is one group\\n\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$broker2Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$broker3Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$broker4Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$broker2Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$broker3Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$broker4Ip\\"\\n\\n# Broker 2, 3 and 4 is the other group\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$broker0Ip\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$broker1Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker3\\" \\"$broker0Ip\\"\\nretryUntilSuccess disconnect \\"$broker3\\" \\"$broker1Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker4\\" \\"$broker0Ip\\"\\nretryUntilSuccess disconnect \\"$broker4\\" \\"$broker1Ip\\"\\n\\n```\\n\\n\\n![general-network-partition](general-network-partition-0.png)\\n\\nIt works quite well, we can see that another broker took over the leadership and continues with processing. We reach almost the same throughput, interesting is that the activate job requests seem to scale up, which is totally unexpected! We drop now 82% of our requests because we are overloaded with activate job requests.\\n\\n![growing-request-network-partition](growing-requests-network-partition-0.png)\\n\\nIn the atomix section we can see that the both nodes, which are partitioned away, miss a lot of heatbeats and we can see the leader change, which has happened earlier.\\n![atomix-network-partition](atomix-network-partition-0.png)\\n\\nQuite early after the network partition a node preemption happened.\\n\\n![node-died](node-died-0.png)\\n\\nWe see that the processing completely stops, two reasons here: one is that the gateway was restarted and another is that the leader was restarted and we lost quourum, since we already have the network partition in place. After the restart the Broker-4 actually should know again the other nodes, which is why the heartbeat misses stopped.\\n\\n![atomix-after-restart](atomix-after-restart.png)\\n\\nAfter the Broker comes back the processing started again.\\n\\n![new-start-broker-4](new-start-broker-4.png)\\n\\nAs mentioned earlier the grpc requests increased significantly, we now drop 100% of the requests. We have ~3k incoming activate job requests.\\n\\n![grpc-after-restart](grpc-after-restart.png)\\n\\nSome time later we can see that the grpc requests has stabilized again.\\n\\n![grpc-stabilized](grpc-stabilized.png)\\n\\nThis should be investigated further, but we will stop here with this experiment since it worked as expected that we kept processing even if we partition two brokers away.\\n\\n## Second Chaos Experiment\\n\\nAfter the first experiment succeeded, I wanted to experiment how the cluster behaves if we add one follower back to group one and remove it from the second group. As you might remember we have in the first group (Broker-0, Broker-1) and in the second group (Broker-2, Broker-3, Broker-4).\\n\\n### Expected \\n\\nWhen the network partition is created and we continue with processing at some point a snapshot is taken and the log is compacted. The first group will not receive any events, which means it has the old state. If we now add Broker-2 to the first group we would expect that the first group now can take over, since it has quorum, and the second will stop working. Before it can start with further processing the Broker-0 and Broker-1 need to get the latest state of Broker-2. We expect that Broker-2 becomes leader in the first group, since it has the longer (latest) log.\\n\\n### Actual\\n\\nAgain same set up, 5 nodes, one partition and replication factor 3. I\'m using the same script as above. I will wait until a snapshot is taken, we could also trigger it now via an end point.\\n\\nWe can see no difference in processing throughput after setting up the network partition again.\\n\\n![new-partition](new-partition.png)\\n\\nFurthermore, the grpc requests seem to be stable, so it must be something related to the gateway or leader restart.\\n\\n![new-grpc](new-grpc.png)\\n\\nWhen we take a look at the atomix metrics we see that both brokers are missing heartbeats, which is expected. \\n\\n![new-heartbeats](new-heartbeats.png)\\n\\nNode preemption wanted to annoy me again... Broker 2 was restarted, because of node preemption. Since we had no quorum, a new election was started. Broker-2 came back voted for Broker-3, but missed soon also heartbeats, so it started an election again and became leader, because it was able to talk with all nodes again. This was not what we wanted to test, but it is nice to know that it works :laughing:\\n\\n![second-restart](second-restart.png)\\n\\nSo again, I re-deployed the cluster and created a snapshot by hand (via API).\\n\\nFor that I port-forwarded our admin port (9600)\\n```shell\\n$ k port-forward zell-chaos-zeebe-1 9600\\n```\\n\\nOn the leader we send the POST request to take a snapshot.\\n```sh\\n[zell zell-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ curl -X POST http://localhost:9600/actuator/partitions/takeSnapshot\\n{\\"1\\":{\\"role\\":\\"LEADER\\",\\"snapshotId\\":\\"591299-1-1611061561457-718421-717758\\",\\"processedPosition\\":723973,\\"processedPositionInSnapshot\\":718421,\\"streamProcessorPhase\\":\\"PROCESSING\\",\\"exporterPhase\\":\\"EXPORTING\\",\\"exportedPosition\\":722841}}\\n```\\n\\nWe can see in the metrics that a snapshot was taken (probably two, because I accidently executed the command twice).\\n\\n![snapshot.png](snapshot.png)\\n\\nFor the Broker 2 we check whether it already received the snapshot:\\n\\n```shell\\n[zell zell-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ curl -X GET http://localhost:9600/actuator/partitions\\n{\\"1\\":{\\"role\\":\\"FOLLOWER\\",\\"snapshotId\\":\\"595599-1-1611061566584-723972-722841\\",\\"processedPosition\\":null,\\"processedPositionInSnapshot\\":null,\\"streamProcessorPhase\\":null,\\"exporterPhase\\":null,\\"exportedPosition\\":null}}\\n```\\n\\nWe also verify that Broker-0 hasn\'t received any snapshots nor events.\\n\\n```shell\\n[zell zell-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ curl -X GET http://localhost:9600/actuator/partitions\\n{\\"1\\":{\\"role\\":\\"FOLLOWER\\",\\"snapshotId\\":\\"44199-1-1611061147163-76565-53432\\",\\"processedPosition\\":null,\\"processedPositionInSnapshot\\":null,\\"streamProcessorPhase\\":null,\\"exporterPhase\\":null,\\"exportedPosition\\":null}}\\n```\\n\\nAfter that, we start with the disconnection to group two and connect Broker-2 to group one (Broker-0 and Broker-1).\\n\\n```shell\\n retryUntilSuccess disconnect \\"$broker2\\" \\"$broker3Ip\\"\\n retryUntilSuccess disconnect \\"$broker2\\" \\"$broker4Ip\\"\\n \\n retryUntilSuccess connect \\"$broker2\\" \\"$broker0Ip\\"\\n retryUntilSuccess connect \\"$broker2\\" \\"$broker1Ip\\"   \\n```\\n\\nWe can see that we now have no leader at all, because I missed to connect the first group with Broker-2 in the reverse and disconnecting group 2 from Broker-2.\\n\\n![network-partition-happening.png](network-partition-happening.png)\\n\\nAfter doing so:\\n\\n```shell\\n\\nretryUntilSuccess disconnect \\"$broker3\\" \\"$broker2Ip\\"\\nretryUntilSuccess disconnect \\"$broker4\\" \\"$broker2Ip\\"\\n\\nretryUntilSuccess connect \\"$broker0\\" \\"$broker2Ip\\"\\nretryUntilSuccess connect \\"$broker1\\" \\"$broker2Ip\\"    \\n```\\n\\nWe can see in the logs but also in the metrics that snapshots are replicated to Broker-0 and Broker-1.\\n\\n![snapshot-metrics.png](snapshot-metrics.png)\\n\\nI would expect that we also see something in the atomix snapshot panels, but here it looks like only the duration is published.\\n\\n![atomix-snapshot-metrics.png](atomix-snapshot-metrics.png)\\n\\nAfter connecting the Broker\'s we see that Broker-0 and Broker-1 are not missing heartbeats anymore and that a new leader has been chosen, Broker-2 which was the expected leader! \\n\\n![after-connect-all.png](after-connect-all.png)\\n\\nThe processing started and cluster seem to look healthy again.\\n\\n![healed.png](healed.png)\\n\\nExperiment was successful! :+1:\\n\\n## New Issues\\n\\n * Unexpected request count on network partition/node restart\\n * Snapshot metrics are unclear, which show what and Atomix snapshot metrics are not showing values\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2021/01/07/disconnect-leader-and-follower","metadata":{"permalink":"/zeebe-chaos/2021/01/07/disconnect-leader-and-follower","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2021-01-07-disconnect-leader-and-follower/index.md","source":"@site/blog/2021-01-07-disconnect-leader-and-follower/index.md","title":"Disconnect Leader and one Follower","description":"Happy new year everyone","date":"2021-01-07T00:00:00.000Z","formattedDate":"January 7, 2021","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.64,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Disconnect Leader and one Follower","date":"2021-01-07T00:00:00.000Z","categories":["chaos_experiment","broker","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Network partitions","permalink":"/zeebe-chaos/2021/01/19/network-partition"},"nextItem":{"title":"Message Correlation after Failover","permalink":"/zeebe-chaos/2020/11/24/message-correlation-after-failover"}},"content":"Happy new year everyone :tada:\\n\\nThis time I wanted to verify the following hypothesis `Disconnecting Leader and one Follower should not make cluster disruptive` ([#45](https://github.com/zeebe-io/zeebe-chaos/issues/45)).\\nBut in order to do that we need to extract the Leader and Follower node for a partition from the Topology. Luckily in December we got an [external contribution](https://github.com/zeebe-io/zeebe/pull/5943) which allows us to print `zbctl status` as json.\\nThis gives us now more possibilities, since we can extract values much better out of it.\\n\\n**TL;DR** The experiment was successful :+1:\\n\\n\x3c!--truncate--\x3e\\n\\n## Preparation\\n\\nBefore we start with the experiment I wanted to extract the right node id\'s for the follower\'s and leader from the `zbctl status` output via `jq`. If we have that we can use this for other use cases.\\n\\nI stored the `zbctl` json output in a file, to make the lines a bit more readable and that I can focus on the jq stuff. The tested output looks like this:\\n\\n```shell\\n$ cat test.json \\n{\\n  \\"brokers\\": [\\n    {\\n      \\"nodeId\\": 1,\\n      \\"host\\": \\"zeebe-chaos-zeebe-1.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local\\",\\n      \\"port\\": 26501,\\n      \\"partitions\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 2,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 3,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 4,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        }\\n      ],\\n      \\"version\\": \\"0.27.0-SNAPSHOT\\"\\n    },\\n    {\\n      \\"nodeId\\": 2,\\n      \\"host\\": \\"zeebe-chaos-zeebe-2.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local\\",\\n      \\"port\\": 26501,\\n      \\"partitions\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 2,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 3,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 4,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        }\\n      ],\\n      \\"version\\": \\"0.27.0-SNAPSHOT\\"\\n    },\\n    {\\n      \\"nodeId\\": 0,\\n      \\"host\\": \\"zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local\\",\\n      \\"port\\": 26501,\\n      \\"partitions\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 2,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 3,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 4,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        }\\n      ],\\n      \\"version\\": \\"0.27.0-SNAPSHOT\\"\\n    }\\n  ],\\n  \\"clusterSize\\": 3,\\n  \\"partitionsCount\\": 4,\\n  \\"replicationFactor\\": 3,\\n  \\"gatewayVersion\\": \\"0.27.0-SNAPSHOT\\"\\n}\\n```\\n\\nI had a really hard time to find the correct `jq` expression, but here is it:\\n\\n```shell\\n$ cat test.json | jq \\".brokers[]|select(.partitions[]| select(.partitionId == 3) and .role == \\\\\\"LEADER\\\\\\")\\"\\n```\\n\\nYou may ask why there are multiple [selects](https://stedolan.github.io/jq/manual/#select(boolean_expression)). I tried it previous with one and the issue is that it then works like an cartesian-product. It takes broker objects, which take part of the partition 3 and it will take broker objects, which are leader for an partition into the output. This is obviously not that what I want.\\nThe current expression filters brokers for partitions which have the partitionId and are leader for that partition. This [gist comment](https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4#gistcomment-3257810) helped me here.\\n\\nExamples:\\n\\n```shell\\n$ cat test.json | jq \\".brokers[]|select(.partitions[]| select(.partitionId == 3) and .role == \\\\\\"LEADER\\\\\\")|.nodeId\\"\\n1\\n$ cat test.json | jq \\".brokers[]|select(.partitions[]| select(.partitionId == 2) and .role == \\\\\\"LEADER\\\\\\")|.nodeId\\"\\n2\\n```\\n\\nLater I realized that this doesn\'t work for followers, since you can have multiple ones, BUT also this can be solved. [Just put it in an array and get the first entry](\\nhttps://stackoverflow.com/questions/38500363/get-the-first-or-nth-element-in-a-jq-json-parsing).\\n\\n```shell\\njq \\"[.brokers[]|select(.partitions[]| select(.partitionId == $partition) and .role == \\\\\\"$state\\\\\\")][0].nodeId\\n```\\n\\nAs you can see `jq` is quite powerful and I learned a lot about it this day. If you interested you can also check [the manual](https://stedolan.github.io/jq/manual/) which has ton\'s of examples.\\n\\n### Resources\\n\\n* [https://stackoverflow.com/questions/18592173/select-objects-based-on-value-of-variable-in-object-using-jq](https://stackoverflow.com/questions/18592173/select-objects-based-on-value-of-variable-in-object-using-jq)\\n* [https://unix.stackexchange.com/questions/404699/using-multiple-wildcards-in-jq-to-select-objects-in-a-json-file](https://unix.stackexchange.com/questions/404699/using-multiple-wildcards-in-jq-to-select-objects-in-a-json-file)\\n* [https://stedolan.github.io/jq/manual/#Builtinoperatorsandfunctions](https://stedolan.github.io/jq/manual/#Builtinoperatorsandfunctions)\\n* [https://stackoverflow.com/questions/33057420/jq-select-multiple-conditions](https://stackoverflow.com/questions/33057420/jq-select-multiple-conditions)\\n* [https://github.com/stedolan/jq/issues/319](https://github.com/stedolan/jq/issues/319)\\n* [https://unix.stackexchange.com/questions/491669/jq-get-attribute-of-nested-object](https://unix.stackexchange.com/questions/491669/jq-get-attribute-of-nested-object)\\n* [https://stackoverflow.com/questions/27562424/jq-nested-object-extract-top-level-id-and-lift-a-value-from-internal-object](https://stackoverflow.com/questions/27562424/jq-nested-object-extract-top-level-id-and-lift-a-value-from-internal-object)\\n* _false track_ [https://stackoverflow.com/questions/28615174/jq-filter-on-sub-object-value](https://stackoverflow.com/questions/28615174/jq-filter-on-sub-object-value)\\n* final key: [https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4#gistcomment-3257810](https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4#gistcomment-3257810)\\n\\n### Script\\n\\nI was able to replace the old utility:\\n\\n```shell\\nfunction getIndexOfPodForPartitionInState()\\n{\\n  partition=\\"$1\\"\\n  state=\\"$2\\"\\n  pod=$(getGateway)\\n  namespace=$(getNamespace)\\n\\n  # To print the topology in the journal\\n  until topology=\\"$(kubectl exec \\"$pod\\" -n \\"$namespace\\" -- zbctl status --insecure)\\"\\n  do\\n    true;\\n  done\\n\\n\\n  # For cluster size 3 and replication factor 3\\n  # we know the following partition matrix\\n  # partition \\\\ node  0    1     2\\n  #     1             L    F     F\\n  #     2             F    L     F\\n  #     3             F    F     L\\n  #    etc.\\n  # This means broker 1, 2 or 3 participates on partition 3\\n  # BE AWARE the topology above is just an example and the leader can every node participating node.\\n\\n  index=$(($(echo \\"$topology\\" \\\\\\n    | grep \\"Partition $partition\\" \\\\\\n    | grep -n \\"$state\\" -m 1 \\\\\\n    | sed \'s/\\\\([0-9]*\\\\).*/\\\\1/\') - 1))\\n  echo \\"$index\\"\\n}\\n```\\n\\nWith this:\\n\\n```shell\\nfunction getIndexOfPodForPartitionInState()\\n{\\n  partition=\\"$1\\"\\n  state=\\"$2\\"\\n  pod=$(getGateway)\\n  namespace=$(getNamespace)\\n\\n  # To print the topology in the journal\\n  until topology=\\"$(kubectl exec \\"$pod\\" -n \\"$namespace\\" -- zbctl status --insecure -o json)\\"\\n  do\\n    true;\\n  done\\n\\n  index=$(echo \\"$topology\\" | jq \\"[.brokers[]|select(.partitions[]| select(.partitionId == $partition) and .role == \\\\\\"$state\\\\\\")][0].nodeId\\")\\n  echo \\"$index\\"\\n}\\n```\\n\\nThe previous function worked only with homogeneous clusters, which means where the partitions are equally distributed. This caused issues on experiments on Production L clusters, where partitions are heterogeneous distributed, see related issue [zeebe-io/zeebe-cluster-testbench#154](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/154). With this new utility we can create some new experiments also for Production - L clusters.\\n\\nI wrote a new script based on the [older disconnect/connect gateway scripts](https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-experiments/scripts/disconnect-standalone-gateway.sh), where we disconnect the gateway with the brokers. The new one disconnects an leader for an partition with the follower and vice-versa.\\n\\nDisconnect Leader-Follower:\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\n# determine leader for partition\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"LEADER\\")\\nleader=$(getBroker \\"$index\\")\\nleaderIp=$(kubectl get pod \\"$leader\\" -n \\"$namespace\\" --template=\\"{ {.status.podIP} }\\")\\n\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"FOLLOWER\\")\\nfollower=$(getBroker \\"$index\\")\\nfollowerIp=$(kubectl get pod \\"$follower\\" -n \\"$namespace\\" --template=\\"{ {.status.podIP} }\\")\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$leader\\" \\"$followerIp\\"\\nretryUntilSuccess disconnect \\"$follower\\" \\"$leaderIp\\" \\n```\\n\\n## Chaos Experiment\\n\\nWe want to disconnect a leader and a follower from a specific partition.\\n\\n### Hypothesis \\n\\nWe expect that even if the leader and the follower can\'t talk with each other the follower is not able to disrupt the cluster and no new election is started, such that he becomes the leader.\\nOn reconnect we expect that the follower keeps up again and is eventually on the same page with the other follower and leader.\\n\\n### Actual\\n\\nWe deployed a cluster with one partition for simplicity. We run the above posted script to disconnect one leader with a follower and the same follower with the leader.\\n\\n#### Disconnect\\n\\nAfter running the disconnect script we see in general no disruption. The processing is still continuing.\\n\\n![](general.png)\\n\\nWe can see that the followers misses a lot of heartbeats, which is expected.\\n\\n![](heartbeats.png)\\n\\nThis is also visible in the logs:\\n\\n```shell\\n2021-01-07 20:22:28.320 CET\\nzeebe-chaos-zeebe-0\\nRaftServer{raft-partition-partition-1}{role=FOLLOWER} - No heartbeat from null in the last PT2.98S (calculated from last 2980 ms), sending poll requests\\n2021-01-07 20:22:28.321 CET\\nzeebe-chaos-zeebe-0\\nRaftServer{raft-partition-partition-1}{role=FOLLOWER} - Poll request to 1 failed: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-1.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.0.7:26502\\n2021-01-07 20:22:28.625 CET\\nzeebe-chaos-zeebe-1\\nRaftServer{raft-partition-partition-1} - AppendRequest{term=1, leader=1, prevLogIndex=2643199, prevLogTerm=1, entries=0, checksums=0, commitIndex=2755920} to 0 failed: java.util.concurrent.CompletionException: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.7.13:26502\\n2021-01-07 20:22:28.977 CET\\nzeebe-chaos-zeebe-1\\nRaftServer{raft-partition-partition-1} - AppendRequest{term=1, leader=1, prevLogIndex=2643199, prevLogTerm=1, entries=0, checksums=0, commitIndex=2756276} to 0 failed: java.util.concurrent.CompletionException: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.7.13:26502\\n2021-01-07 20:22:29.382 CET\\nzeebe-chaos-zeebe-1\\nRaftServer{raft-partition-partition-1} - AppendRequest{term=1, leader=1, prevLogIndex=2643199, prevLogTerm=1, entries=0, checksums=0, commitIndex=2756571} to 0 failed: java.util.concurrent.CompletionException: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.7.13:26502\\n```\\n\\nThe follower is failing  to send poll requests to Broker-1, which is the leader. I assume we don\'t see that the follower sends the other follower poll requests because our log level is to high. \\nFurthermore we can see that the leader is not able to send append requests. We have a panel where we can see how many entries the follower lags behind.\\n\\n![](slow-follower.png)\\n\\nInteresting that the java heap of the follower is growing.\\n\\n![](resources-follower.png)\\n\\nBut after some time GC steps in and it goes back to normal.\\n\\n![](later-gc.png)\\n\\n#### Connect\\n\\nAfter running the connect script we can see in the log that almost immediately a snapshot is send to the follower.\\n\\n```shell\\nD 2021-01-07T19:26:24.042908Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000333.log of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.045690Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000334.sst of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.052229Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000335.log of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.068270Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000336.sst of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.076135Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk CURRENT of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.081880Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk MANIFEST-000003 of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.089900Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk OPTIONS-000090 of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\n``` \\n\\nThis is also visible in the metrics\\n\\n![](raft-snap.png)\\n\\nWe see a healed raft.\\n\\n![](healed-raft.png)\\n\\nWhat I was wondering is why the metric which shows the lag of the follower is not really recovering.\\n\\n![](metrics-is-not-correct.png)\\n\\nEven after almost 12 hours it is still showing ~4K\\n\\n![](failing-metric.png)\\n\\n## Result\\n\\nAs we can see the experiment was successful, we were able to verify our hypothesis. The new extraction of the leader and follower from the topology gives us new possibilities for new chaos experiments.\\nI think we can also experiment a bit more with disconnecting different nodes, to see how the cluster behaves.\\n\\n## New Issues\\n\\n * Metric: Follower lag doesn\'t recover\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/11/24/message-correlation-after-failover","metadata":{"permalink":"/zeebe-chaos/2020/11/24/message-correlation-after-failover","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-11-24-message-correlation-after-failover/index.md","source":"@site/blog/2020-11-24-message-correlation-after-failover/index.md","title":"Message Correlation after Failover","description":"Today I wanted to finally implement an experiment which I postponed for long time, see #24.","date":"2020-11-24T00:00:00.000Z","formattedDate":"November 24, 2020","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.38,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Message Correlation after Failover","date":"2020-11-24T00:00:00.000Z","categories":["chaos_experiment","broker","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Disconnect Leader and one Follower","permalink":"/zeebe-chaos/2021/01/07/disconnect-leader-and-follower"},"nextItem":{"title":"Many Job Timeouts","permalink":"/zeebe-chaos/2020/11/11/job-timeouts"}},"content":"Today I wanted to finally implement an experiment which I postponed for long time, see [#24](https://github.com/zeebe-io/zeebe-chaos/issues/24).\\nThe problem was that previous we were not able to determine on which partition the message was published, so we were not able to assert that it was published on the correct partition. With this [#4794](https://github.com/zeebe-io/zeebe/issues/4794) it is now possible, which was btw an community contribution. :tada:\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe want to publish a message to a specific partition. After publishing the message we want to restart the corresponding leader of that partition, deploy and create a workflow instance to which the message should correlate to. \\n\\n### Hypothesis\\n\\nWe expect that even due to a leader change messages can be correlated to a workflow instance, after a new leader comes up for that partition.\\n\\n### Actual\\n\\n#### Implementation\\n The experiment should ideally work on all cluster plans, since we run the chaos experiments now with all existing cluster plans. For that we want to publish the message on partition one. In Zeebe the messages are distributed over the partitions via the correlation key. Our current cluster plans have 1, 4 or 8 partitions. In order to always reach the same partition we need a correlation key which is modulo the partition count always the same number. Ideally it is just one character, which makes the calculation easier. If we take a look at the ASCII table we see that for example `48 mod 1, 4 or 8` is always `0`. This would correspond then to partition one, since in the partition calculation we add 1. If we use \\"0\\" as correlation key we can be sure this will end up in the production clusters on partition one. For more information about the calculate you can check the [SubscriptionUtil](https://github.com/zeebe-io/zeebe/blob/develop/protocol-impl/src/main/java/io/zeebe/protocol/impl/SubscriptionUtil.java) class.\\n\\nThe process is quite simple, we just have one intermediate message catch event and we will create an new instance and await the result. With that we make sure that the message was correlated correctly.\\n\\n![oneReceiveMsgEvent](oneReceiveMsgEvent.png)\\n\\nOn testing the separate scripts I had at the begining problems with the `awaitResult`. I got always timeouts.\\n\\n```sh\\nError: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request type command-api-4 timed out in 8999 milliseconds\\ncommand terminated with exit code 1\\n+ echo \'Failed to execute: \'\\\\\'\'awaitInstance\'\\\\\'\'. Retry.\'\\n```\\n\\n![operate](operate.png)\\n\\nVia operate nor via zbctl it is easy to find out what is the real issue. I\'m not able to see any details regarding the intermediate message catch event in operate. With help of [zdb](https://github.com/Zelldon/zdb) I was able to track down the issue. The time to live was to small. The published messages have been already deleted before I created the corresponding workflow instancs. Per default the time to live is `5s` with `zbctl`. It is not easy to find out why the message doesn\'t correlate. After setting the `ttl` quite high it works and I can run my experiment successfully.\\n\\n#### Result\\n\\n```sh\\n$ chaos run production-m/msg-correlation/experiment.json \\n[2020-11-24 14:56:28 INFO] Validating the experiment\'s syntax\\n[2020-11-24 14:56:28 INFO] Experiment looks valid\\n[2020-11-24 14:56:28 INFO] Running experiment: Zeebe message correlation experiment\\n[2020-11-24 14:56:28 INFO] Steady-state strategy: default\\n[2020-11-24 14:56:28 INFO] Rollbacks strategy: default\\n[2020-11-24 14:56:28 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-11-24 14:56:28 INFO] Probe: All pods should be ready\\n[2020-11-24 14:56:28 INFO] Steady state hypothesis is met!\\n[2020-11-24 14:56:28 INFO] Playing your experiment\'s method now...\\n[2020-11-24 14:56:28 INFO] Action: Publish message to partition one\\n[2020-11-24 14:56:29 INFO] Action: Terminate leader of partition 1 non-gracefully\\n[2020-11-24 14:56:34 INFO] Probe: Should be able to create a workflow and await the message correlation\\n[2020-11-24 14:56:39 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-11-24 14:56:39 INFO] Probe: All pods should be ready\\n[2020-11-24 14:57:16 INFO] Steady state hypothesis is met!\\n[2020-11-24 14:57:16 INFO] Let\'s rollback...\\n[2020-11-24 14:57:16 INFO] No declared rollbacks, let\'s move on.\\n[2020-11-24 14:57:16 INFO] Experiment ended with status: completed\\n```\\n\\nExperiment added to all cluster plans:\\n * https://github.com/zeebe-io/zeebe-chaos/commit/adeab53915e12b4a76fd4d49bb359684619b117f\\n * https://github.com/zeebe-io/zeebe-chaos/commit/93daf11864fdd851267dae67fdfc31e0ea78b407\\n\\n\\n## New Issues\\n\\n * Operate: Show details of an intermediate catch event [OPE-1165](https://jira.camunda.com/browse/OPE-1165)\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/11/11/job-timeouts","metadata":{"permalink":"/zeebe-chaos/2020/11/11/job-timeouts","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-11-11-job-timeouts/index.md","source":"@site/blog/2020-11-11-job-timeouts/index.md","title":"Many Job Timeouts","description":"In the last game day (on friday 06.11.2020) I wanted to test whether we can break a partition if many messages time out at the same time. What I did was I send many many messages with a decreasing TTL, which all targeting a specific point in time, such that they will all timeout at the same time. I expected that if this happens that the processor will try to time out all at once and break because the batch is to big. Fortunately this didn\'t happen, the processor was able to handle this.","date":"2020-11-11T00:00:00.000Z","formattedDate":"November 11, 2020","tags":[{"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.885,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Many Job Timeouts","date":"2020-11-11T00:00:00.000Z","categories":["chaos_experiment","broker"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Message Correlation after Failover","permalink":"/zeebe-chaos/2020/11/24/message-correlation-after-failover"},"nextItem":{"title":"Investigate failing Chaos Tests","permalink":"/zeebe-chaos/2020/11/03/investigate-failing-tests"}},"content":"In the last game day (on friday 06.11.2020) I wanted to test whether we can break a partition if many messages time out at the same time. What I did was I send many many messages with a decreasing TTL, which all targeting a specific point in time, such that they will all timeout at the same time. I expected that if this happens that the processor will try to time out all at once and break because the batch is to big. Fortunately this didn\'t happen, the processor was able to handle this.\\n\\nI wanted to verify the same with job time out\'s.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nI setup an Production S cluster in camunda cloud. Deployed an normal starter, which starts 20 workflow instances per second. I used similar code to activate jobs with a decreasing timeout, such that they all timeout at the same time. The target time was 3 PM. I started the test ~11 am.\\n\\n**Code:**\\n```csharp\\n            var now = DateTime.Now;\\n            var today3PM = now.Date.AddHours(15);\\n            int count = 0;\\n            var totalMilli = (today3PM - now).TotalMilliseconds;\\n            do\\n            {\\n                try\\n                {\\n\\n                    await client\\n                        .NewActivateJobsCommand()\\n                        .JobType(\\"benchmark-task\\")\\n                        .MaxJobsToActivate(100)\\n                        .Timeout(TimeSpan.FromMilliseconds(totalMilli))\\n                        .WorkerName(\\"lol\\")\\n                        .Send(TimeSpan.FromSeconds(30));\\n                    count++;\\n\\n                    totalMilli = (today3PM - DateTime.Now).TotalMilliseconds;\\n                    if (count % 10 == 0)\\n                    {\\n                        Console.WriteLine(\\"Activated next 1000, count:\\" + count);\\n                        Console.WriteLine(\\"Total \\" + totalMilli + \\" ms until 3 am\\");\\n                    }\\n                }\\n                catch (Exception ex)\\n                {\\n                    Console.WriteLine(\\"Failed to activate job, because of \\" + ex.Message);\\n                }\\n            } while (totalMilli > 0);\\n\\n```\\n\\nI experienced a lot of pod restarts during the experiment, but at 3 pm the processor seems to handle the situation correctly and has no problems with so many events.\\n\\n\\n![timebomb](timeout-bomb.png)\\n![timebomb-general](timeout-bomb-general.png)\\n\\n## Related issues\\n\\n### No worker name\\n\\nFirst I missed the `.WorkerName` in the activation command and this broke somehow the activation.\\nIn the client I got either timeouts or resource exhausted responses, but in the gateway I saw that the worker name is missing.\\n\\n**Gateway output:**\\n\\n```\\nio.zeebe.gateway.cmd.BrokerRejectionException: Command (ACTIVATE) rejected (INVALID_ARGUMENT): Expected to activate job batch with worker to be present, but it was blank\\n\\tat io.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:185) ~[zeebe-gateway-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:137) ~[zeebe-gateway-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorJob.invoke(ActorJob.java:76) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorJob.execute(ActorJob.java:39) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorTask.execute(ActorTask.java:122) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorThread.executeCurrentTask(ActorThread.java:107) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorThread.doWork(ActorThread.java:91) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorThread.run(ActorThread.java:204) [zeebe-util-0.25.0.jar:0.25.0]\\n```\\n\\n**Client output:**\\n\\n```\\nFailed to activate job, because of Status(StatusCode=\\"DeadlineExceeded\\", Detail=\\"Deadline Exceeded\\", DebugException=\\"Grpc.Core.Internal.CoreErrorDetailException: {\\"created\\":\\"@1605092309.325960242\\",\\"description\\":\\"Error received from peer ipv4:35.205.156.246:443\\",\\"file\\":\\"/var/local/git/grpc/src/core/lib/surface/call.cc\\",\\"file_line\\":1062,\\"grpc_message\\":\\"Deadline Exceeded\\",\\"grpc_status\\":4}\\")\\n```\\n\\nAfter adding the worker name it works, but begins with lot of resource exhausted. I created a new issue for it https://github.com/zeebe-io/zeebe/issues/5812 .\\n\\n## Pod restarts\\n\\n![preempt](preemptions.png)\\n\\nEvery 10 min it seems to be a node dying, which causes resource exhausted then.\\n\\nAfter looking at the [gke events](https://console.cloud.google.com/logs/viewer?interval=PT1H&authuser=1&organizationId=669107107215&project=camunda-cloud-240911&minLogLevel=0&expandAll=false&timestamp=2020-11-11T14:04:53.000000000Z&customFacets=&limitCustomFacetWidth=true&advancedFilter=jsonPayload.kind%3D%22Event%22%0Aresource.labels.cluster_name%3D%22ultrachaos%22%0AjsonPayload.involvedObject.namespace%3D%2299c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe%22%0AjsonPayload.involvedObject.name:%22zeebe%22&scrollTimestamp=2020-11-11T13:49:52.000000000Z&dateRangeEnd=2020-11-11T14:06:47.813Z&dateRangeStart=2020-11-11T13:06:47.813Z) I saw now evidence that this is caused by node preemptions. \\n\\nI checked the pods directly and saw no heap dumps in the data folder. After describing the pod I can see:\\n\\n```\\n[zell zeebe-cluster-testbench/ ns:99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe]$ k describe pod zeebe-1\\nName:         zeebe-1\\nNamespace:    99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe\\nPriority:     0\\nNode:         gke-ultrachaos-compute-fea23edf-tqbm/10.132.0.58\\nStart Time:   Wed, 11 Nov 2020 14:49:52 +0100\\nLabels:       app.kubernetes.io/app=zeebe\\n              app.kubernetes.io/component=gateway\\n              controller-revision-hash=zeebe-66b694fbfc\\n              statefulset.kubernetes.io/pod-name=zeebe-1\\nAnnotations:  <none>\\nStatus:       Running\\nIP:           10.56.7.16\\nIPs:\\n  IP:           10.56.7.16\\nControlled By:  StatefulSet/zeebe\\nContainers:\\n  zeebe:\\n    Container ID:   docker://a55fe90d3184bea064aec29d845680241096b0d971d66b05a35495857c5d7427\\n    Image:          camunda/zeebe:0.25.0\\n    Image ID:       docker-pullable://camunda/zeebe@sha256:1286086e786975837dcbf664daa29d41d2666af4daf4abd3192fff1426804dd6\\n    Ports:          9600/TCP, 26500/TCP, 26501/TCP, 26502/TCP, 26503/TCP, 26504/TCP\\n    Host Ports:     0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP\\n    State:          Waiting\\n      Reason:       CrashLoopBackOff\\n    Last State:     Terminated    <=====\\n      Reason:       OOMKilled    <====\\n      Exit Code:    137   \\n      Started:      Wed, 11 Nov 2020 16:06:59 +0100\\n      Finished:     Wed, 11 Nov 2020 16:16:19 +0100\\n    Ready:          False\\n```\\n\\nFurthermore we can see the `JAVA_OPTIONS`, which are:\\n\\n```\\nJAVA_TOOL_OPTIONS:                                -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError\\n```\\n\\nI was wondering why we not setting any path for the heap dump. @npepinpe mentioned that this is done in the start up script.\\n\\nIt is true this is part of the script:\\n\\n```sh\\n[zell zeebe-cluster-testbench/ ns:99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe]$ k describe configmaps zeebe-configmap \\nName:         zeebe-configmap\\nNamespace:    99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe\\nLabels:       cloud.camunda.io/channel=Internal_Dev\\n              cloud.camunda.io/clusterPlan=Production_S_v1\\n              cloud.camunda.io/clusterPlanType=Production_S\\n              cloud.camunda.io/generation=Zeebe_0_25_0\\n              cloud.camunda.io/internalSalesPlan=false\\n              cloud.camunda.io/orgName=the_org_with_the_big_cluster\\n              cloud.camunda.io/salesPlan=Paid\\nAnnotations:  <none>\\n\\nData\\n====\\nstartup.sh:\\n----\\n# ...\\n# append datestamped heapdump path\\nexport JAVA_TOOL_OPTIONS=\\"${JAVA_TOOL_OPTIONS} -XX:HeapDumpPath=/usr/local/zeebe/data/java_started_$(date +%s).hprof\\"\\n\\nenv\\nexec /usr/local/zeebe/bin/broker\\n```\\n\\nUnfortunately this is not visible outside of this context, which is why I thought it is not set.\\n\\n```sh\\nroot@zeebe-0:/usr/local/zeebe# java -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -version\\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError\\n\\n#...\\n\\n    bool HeapDumpBeforeFullGC                     = false                                  {manageable} {default}\\n     bool HeapDumpOnOutOfMemoryError               = true                                   {manageable} {environment}\\n    ccstr HeapDumpPath                             =                                        {manageable} {default}\\n    uintx HeapFirstMaximumCompactionCount          = 3                                         {product} {default}\\n\\n#...\\n```\\n\\n```sh\\nroot@zeebe-0:/usr/local/zeebe# env | grep JAVA\\nJAVA_HOME=/usr/local/openjdk-11\\nJAVA_TOOL_OPTIONS=-XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError\\nJAVA_VERSION=11.0.8\\nroot@zeebe-0:/usr/local/zeebe# \\n\\n```\\n\\nI think we should give the production s cluster plans a bit more memory, currently has 2 gig and java can use only 1 gig. It is currently quite easy to overload the brokers and kill them with a small load.\\n\\n## New Issues\\n\\n * [#5812](https://github.com/zeebe-io/zeebe/issues/5812) \\n \\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/11/03/investigate-failing-tests","metadata":{"permalink":"/zeebe-chaos/2020/11/03/investigate-failing-tests","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-11-03-investigate-failing-tests/index.md","source":"@site/blog/2020-11-03-investigate-failing-tests/index.md","title":"Investigate failing Chaos Tests","description":"Today as part of the Chaos Day I wanted to investigate why our current Chaos Tests are failing and why our targeting cluster has been broken by them,","date":"2020-11-03T00:00:00.000Z","formattedDate":"November 3, 2020","tags":[{"label":"tests","permalink":"/zeebe-chaos/tags/tests"}],"readingTime":4.57,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Investigate failing Chaos Tests","date":"2020-11-03T00:00:00.000Z","categories":["investigation"],"tags":["tests"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Many Job Timeouts","permalink":"/zeebe-chaos/2020/11/11/job-timeouts"},"nextItem":{"title":"Non-graceful Shutdown Broker","permalink":"/zeebe-chaos/2020/10/20/non-graceful-shutdown"}},"content":"Today as part of the Chaos Day I wanted to investigate why our current Chaos Tests are failing and why our targeting cluster has been broken by them,\\nsee the related issue [#5688](https://github.com/zeebe-io/zeebe/issues/5688).\\n\\n**TL;DR**\\n\\nWe found three new bugs regarding the reprocessing detection and deployment distribution, but still were not able to reproduce the real issue.\\n\\n\x3c!--truncate--\x3e\\n\\n## Investigation\\n\\nI started already yesterday with the investigation and found out that two brokers (`Broker-0` and `Broker-2`) are failing to restart on partition three, but `Broker-0` is able to start the processing on partition three. See this [comment](https://github.com/zeebe-io/zeebe/issues/5688#issuecomment-720401612) for more information. Note that partition three is receiving deployment from partition one via deployment distribution.\\n\\nTogether with @saig0 I looked at the code and we found out that reprocessing can be a bit problematic in some situations regarding the `WorkflowPersistenceCache`, see related [comment](https://github.com/zeebe-io/zeebe/issues/5688#issuecomment-721021464).\\n\\nThe cache is used in the `DeploymentCreateProcessor` to verify that the deployment is new and to add it to the state. If this deployment with the same key already exists (in-memory) then the processor rejects the `CREATE` command.\\nNow we can have situations which might be problematic. Say we have two of the `CREATE` commands on normal processing, this can happen when the first partition re-distributes the deployment. When the first one is processed we create a follow up event (`CREATED`), on the second `CREATE` we will write a rejection, since it is already in-memory. If a leader change happens, then it is crucial where the snapshot was taken. If the snapshot position is **AFTER** the first `CREATE` this means that we will handle the second `CREATE` as the first one, which means we will generate on reprocessing a `CREATE` but on the log is a rejection written. This is because the in-memory state doesn\'t reflect the real state. Opened an issue for this [#5753](https://github.com/zeebe-io/zeebe/issues/5753).\\n\\nThe bug which we found was not the real issue we currently have with the broken cluster, since we have two nodes which have the **same** snapshot, but on one Broker it fails and another it doesn\'t. To really understand what the issue is we need the related log, so we need to reproduce this issue.\\n\\n### Reproducing Chaos\\n\\nI created a benchmark with the Zeebe version `0.24.4`. Check [this](https://github.com/zeebe-io/zeebe/tree/develop/benchmarks/setup) for how to setup a benchmark. I realized that creating a benchmark for an earlier version is currently not working because we set the `useMMap` flag in the `zeebe-values.yaml` file. After removing that it works without problems.\\n\\nTo run all experiments in a loop I used in the `chaos-experiments/kubernetes` folder\\n```\\n while [ $? -eq 0 ]; do for ex in */experiment.json; do chaos run $ex; done; done\\n\\n```\\nDuring running the experiments I found a bug in our chaos experiments, where it seems that some experiments are not executed correctly, see [#43](https://github.com/zeebe-io/zeebe-chaos/issues/43).\\n\\n\\nIt took a while, but at some point the experiments start to fail. Interesting is that if you look at the pods all seem to be ready, but in the metrics we can see that one partition is unhealthy (Partition one this time).\\nChecking the logs I found this:\\n\\n```\\nio.zeebe.engine.processor.InconsistentReprocessingException: Reprocessing issue detected! Restore the data from a backup and follow the recommended upgrade procedure. [cause: \\"The key of the record on the log stream doesn\'t match to the record from reprocessing.\\", log-stream-record: {\\"partitionId\\":1,\\"value\\":{\\"errorMessage\\":\\"\\",\\"type\\":\\"benchmark-task\\",\\"errorCode\\":\\"\\",\\"variables\\":{},\\"worker\\":\\"benchmark-worker\\",\\"deadline\\":1604403149010,\\"bpmnProcessId\\":\\"benchmark\\",\\"workflowKey\\":2251799813685250,\\"customHeaders\\":{},\\"retries\\":3,\\"elementId\\":\\"task\\",\\"elementInstanceKey\\":2251799813688892,\\"workflowDefinitionVersion\\":1,\\"workflowInstanceKey\\":2251799813688864},\\"sourceRecordPosition\\":8054,\\"timestamp\\":1604403162815,\\"position\\":9274,\\"valueType\\":\\"JOB\\",\\"intent\\":\\"TIME_OUT\\",\\"recordType\\":\\"COMMAND\\",\\"rejectionReason\\":\\"\\",\\"rejectionType\\":\\"NULL_VAL\\",\\"key\\":2251799813688902}, reprocessing-record: {key=2251799813688825, sourceRecordPosition=8054, intent=DeploymentIntent:DISTRIBUTED, recordType=EVENT}]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.lambda$verifyRecordMatchesToReprocessing$12(ReProcessingStateMachine.java:400) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.verifyRecordMatchesToReprocessing(ReProcessingStateMachine.java:394) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.reprocessEvent(ReProcessingStateMachine.java:258) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.reprocessNextEvent(ReProcessingStateMachine.java:226) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorJob.invoke(ActorJob.java:73) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorJob.execute(ActorJob.java:39) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorTask.execute(ActorTask.java:118) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorThread.executeCurrentTask(ActorThread.java:107) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorThread.doWork(ActorThread.java:91) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorThread.run(ActorThread.java:204) [zeebe-util-0.24.4.jar:0.24.4]\\n```\\n\\nI think this is caused by [#3124](https://github.com/zeebe-io/zeebe/issues/3124), because if the distribution succeeds we want to write a `DISTRUBITED` event on the log, with the stream writer from the context. This can happen concurrently with our reprocessing.\\nMy assumption was that this is caused by a race condition, which might get fixed when we restart the pod. I restarted the `Broker-2`, which was leader for partition 3 and `Broker-1` took over and started the partition successfully.\\n\\n```\\n2020-11-03 14:54:32.916 CET Broker-1-StreamProcessor-1 Processor finished reprocessing at event position 18711\\n```\\n\\nI started the chaos experiments again. They are running now for a while.\\n\\nTogether with @npepinpe I discussed the open issues. We found another problematic bug with the in-memory state of the workflow cache. If on reprocessing an error/exception happens, then we retry the processing of this record, endless.\\nBefore we retry we normally rollback the current transaction to discard all changes in the state. This doesn\'t apply for the in-memory state of the workflow cache, so this can lead to our specific scenario. We checked the log but found no\\nindication that on reprocessing another exception happen, which caused an retry. In anyway we need to fix the cache to avoid the bugs we have described. We concluded that we need the log of the failing cluster to really understand what was happening. We will try to fix the bugs above soon as possible and in parallel run the experiments endless until they fail again.\\n\\n### Notes\\nTo investigate the disks I prepared the follwing commands, which I can use to download the state of the brokers to my local machine.\\n\\n```sh\\nkubectl exec zell-chaos-0244-zeebe-2 -- tar -cf data.tar.gz data/ # compress the data dir\\nkubectl cp zell-chaos-0244-zeebe-2:/usr/local/zeebe/data.tar.gz broker-2/data.tar.gz # download the tarball\\ncd broker-2/\\ntar -xvf broker-2-data.tar.gz\\n\\n```\\n\\n\\n## New Issues\\n\\n * Gateway experiments are not executed [#43](https://github.com/zeebe-io/zeebe-chaos/issues/43)\\n * Deployment Reprocessing inconsistencies [#5753](https://github.com/zeebe-io/zeebe/issues/5753)\\n \\n## Participants\\n\\n  * @saig0\\n  * @npepinpe\\n  * @zelldon"},{"id":"/2020/10/20/non-graceful-shutdown","metadata":{"permalink":"/zeebe-chaos/2020/10/20/non-graceful-shutdown","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-10-20-non-graceful-shutdown/index.md","source":"@site/blog/2020-10-20-non-graceful-shutdown/index.md","title":"Non-graceful Shutdown Broker","description":"Today I had not much time for the chaos day, because of writing Gameday Summary, Incident review, taking part of incidents etc. So enough chaos for one day :)","date":"2020-10-20T00:00:00.000Z","formattedDate":"October 20, 2020","tags":[],"readingTime":1.83,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Non-graceful Shutdown Broker","date":"2020-10-20T00:00:00.000Z","categories":["chaos_experiment","broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Investigate failing Chaos Tests","permalink":"/zeebe-chaos/2020/11/03/investigate-failing-tests"},"nextItem":{"title":"Gateway memory consumption","permalink":"/zeebe-chaos/2020/10/27/standalone-gw-memory"}},"content":"Today I had not much time for the chaos day, because of writing Gameday Summary, Incident review, taking part of incidents etc. So enough chaos for one day :)\\n\\nBut I wanted to merge the PR from Peter and test how our brokers behave if they are not gracefully shutdown. \\nI did that on Wednesday (21-10-2020).\\n\\n\x3c!--truncate--\x3e\\n\\n## PR Merge\\n\\nI tried again the new chaos experiment with a Production M cluster, before merging. It worked quite smooth.\\nPR is merged [#41](https://github.com/zeebe-io/zeebe-chaos/pull/41) :tada:\\n\\n## Non-graceful shutdown\\n\\nCurrently in our experiments we do a normal `kubectl delete pod`, which does an graceful shutdown. The application has time to stop it\'s services etc. It would be interesting how Zeebe handles non-graceful shutdowns. In order to achieve that we can use the option `--grace-period=0`. For more information you can read for example [this](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#force-deletion)\\n\\nI added additional experiments to our normal follower and leader restarts experiments, such that we have both graceful and non-graceful restarts.\\nBoth seem to work without any issues. I was also able to fix some bash script error with the help of [shellcheck](https://github.com/koalaman/shellcheck). Related issue https://github.com/zeebe-io/zeebe-chaos/issues/42.\\n\\n\\nExample output:\\n\\n```\\n(chaostk) [zell kubernetes/ ns:f45d4dee-f73a-4733-9cd4-a4aa8b022376-zeebe]$ chaos run leader-terminate/experiment.json \\n[2020-10-21 15:57:23 INFO] Validating the experiment\'s syntax\\n[2020-10-21 15:57:23 INFO] Experiment looks valid\\n[2020-10-21 15:57:23 INFO] Running experiment: Zeebe Leader restart non-graceful experiment\\n[2020-10-21 15:57:23 INFO] Steady-state strategy: default\\n[2020-10-21 15:57:23 INFO] Rollbacks strategy: default\\n[2020-10-21 15:57:23 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-21 15:57:23 INFO] Probe: All pods should be ready\\n[2020-10-21 15:57:23 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2020-10-21 15:57:27 INFO] Steady state hypothesis is met!\\n[2020-10-21 15:57:27 INFO] Playing your experiment\'s method now...\\n[2020-10-21 15:57:27 INFO] Action: Terminate leader of partition 3 non-gracefully\\n[2020-10-21 15:57:33 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-21 15:57:33 INFO] Probe: All pods should be ready\\n[2020-10-21 15:58:28 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2020-10-21 15:58:32 INFO] Steady state hypothesis is met!\\n[2020-10-21 15:58:32 INFO] Let\'s rollback...\\n[2020-10-21 15:58:32 INFO] No declared rollbacks, let\'s move on.\\n[2020-10-21 15:58:32 INFO] Experiment ended with status: completed\\n```\\n\\nRelated commits:\\n\\n * [Restart leader non-gracefully](https://github.com/zeebe-io/zeebe-chaos/commit/e6260cb8612a983c8ed74fd2a37a249987ad3d3d)\\n * [Restart follower non-gracefully](https://github.com/zeebe-io/zeebe-chaos/commit/63c481c0c7dd7026f03be4e51d61a918613b0140)\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/10/27/standalone-gw-memory","metadata":{"permalink":"/zeebe-chaos/2020/10/27/standalone-gw-memory","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-10-27-standalone-gw-memory/index.md","source":"@site/blog/2020-10-27-standalone-gw-memory/index.md","title":"Gateway memory consumption","description":"In the last weeks I check multiple benchmarks and clusters in incidents. Often I had the feeling that the memory consumption from the gateway is not ideal","date":"2020-10-20T00:00:00.000Z","formattedDate":"October 20, 2020","tags":[],"readingTime":3.775,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway memory consumption","date":"2020-10-20T00:00:00.000Z","categories":["chaos_experiment","gateway","resources"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Non-graceful Shutdown Broker","permalink":"/zeebe-chaos/2020/10/20/non-graceful-shutdown"},"nextItem":{"title":"Multiple Leader Changes","permalink":"/zeebe-chaos/2020/10/13/multiple-leader-changes"}},"content":"In the last weeks I check multiple benchmarks and clusters in incidents. Often I had the feeling that the memory consumption from the gateway is not ideal\\nor that there is a memory leak. I wanted to experiment regarding this memory consumptions. Since we saw in investigating https://github.com/zeebe-io/zeebe/issues/5641 a high memory spike\\nwhen the gateway was not able to talk to other nodes I suspected that here might be some bugs hiding\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment\\n\\nWe will run the Standalone gateway without the brokers and put load on it. \\n\\n### Expected\\n\\nAll requests are rejected by the gateway and the memory doesn\'t grow infinitly on a steady load. The memory consumption should be stable at some point.\\n\\n### Actual\\n\\nFirst I run the standalone gateway without any load. It seems that benchmarks without brokers are not shown in our dashboard namespaces. I fixed that for my experiment in my local grafana session. The issue was that we search for a `atomix_role` metric to get the related namespaces. This metrics is only published on broker side.\\n\\n![](memory-gw-no-broker-no-load.png)\\n\\nWe can see that even if there is no load the memory is already growing.\\n\\nPutting more load on it showed that it doesn\'t drastically increase the memory consumption, but still it was growing.\\n\\n![](memory-gw-no-broker-high-load.png)\\n\\nI think the issue here is that we currently have no limits set for the gateway, which means it will use as many as it can. There is also no pressure for the GC to run or reclaim memory.\\nWe probably want to limit it at somepoint. I created an issue for it [#5699](https://github.com/zeebe-io/zeebe/issues/5699) In order to find out whether we have a memory leak I used a profiler.\\n\\nI restarted the experiment with new settings:\\n\\n```\\n# JavaOpts:\\n# DEFAULTS\\nJavaOpts: >-\\n  # other options\\n  -Djava.rmi.server.hostname=127.0.0.1\\n  -Dcom.sun.management.jmxremote.port=9010\\n  -Dcom.sun.management.jmxremote.rmi.port=9010\\n  -Dcom.sun.management.jmxremote.authenticate=false\\n  -Dcom.sun.management.jmxremote.ssl=false\\n  -Dcom.sun.management.jmxremote.local.only=false\\n```\\n> This will open a remote, unauthenticated, plaintext JMX connection - do not use this configuration in production!\\nSee https://github.com/zeebe-io/zeebe/blob/develop/benchmarks/docs/debug/README.md#remote_jmx\\n\\nAfter I added a port forwarding I was able to open an JMX connection with Java Mission Control.\\n\\n### Conclusion\\n\\nI profiled the gateway with and without load but haven\'t found any memory leak so far.\\n\\n![](result.png)\\n\\nAlso with VisualVM and triggering multiple GC\'s I was not able to spot any thing problematic.\\n\\n![](visualvm.png)\\n\\nIn order to avoid that it uses too much memory and the memory continously grows we should set a limit for the Gateway ([#5699](https://github.com/zeebe-io/zeebe/issues/5699)).\\n\\n### Other Observations\\n\\n#### SerialGC usage\\n\\nJava Mission Control reported as an error that on a multi-core machine the serial garabage collector was used.\\nIf we check the JVM properties we can see that as well.\\n\\n![](gc-settings.png)\\n\\nThis is weird because we don\'t set any GC in our benchmarks, so I would suspect the G1 is used with Java 11. Unfortunately this depends on the available resources which are \\"detected\\" by the JVM.\\nRelated to that https://stackoverflow.com/questions/52474162/why-is-serialgc-chosen-over-g1gc\\nI think we should investigate that further, because we can see in Java mission control that we have GC pauses up to 8 seconds! I created a new issue for it [#5700](https://github.com/zeebe-io/zeebe/issues/5700).\\n\\n#### Unexpected responses\\n\\nWhen we start the `starters` they will first try to deploy a workflow model and loop in this state until they succeed. \\nIn the metrics we can see that the responses to the deployment commands are `NOT_FOUND` instead of `PARTITION_NOT_AVAILABLE`, which I would expect.\\n\\n![](unexepcted-result.png)\\n\\nWe can also see that in the log of the starter:\\n```\\n11:01:18.035 [main] WARN  io.zeebe.Starter - Failed to deploy workflow, retrying\\nio.zeebe.client.api.command.ClientStatusException: Expected to execute command, but this command refers to an element that doesn\'t exist.\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[app.jar:0.24.2]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[app.jar:0.24.2]\\n\\tat io.zeebe.Starter.deployWorkflow(Starter.java:128) [app.jar:0.24.2]\\n\\tat io.zeebe.Starter.run(Starter.java:55) [app.jar:0.24.2]\\n\\tat io.zeebe.App.createApp(App.java:50) [app.jar:0.24.2]\\n\\tat io.zeebe.Starter.main(Starter.java:142) [app.jar:0.24.2]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Expected to execute command, but this command refers to an element that doesn\'t exist.\\n\\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[app.jar:0.24.2]\\n\\t... 4 more\\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Expected to execute command, but this command refers to an element that doesn\'t exist.\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[app.jar:0.24.2]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:460) ~[app.jar:0.24.2]\\n\\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[app.jar:0.24.2]\\n\\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[app.jar:0.24.2]\\n\\tat me.dinowernli.grpc.prometheus.MonitoringClientCallListener.onClose(MonitoringClientCallListener.java:50) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:426) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl.access$500(ClientCallImpl.java:66) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:689) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$900(ClientCallImpl.java:577) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:751) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:740) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[app.jar :0.24.2]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\\n\\tat java.lang.Thread.run(Unknown Source) ~[?:?]\\n```\\n\\nThis doesn\'t make any sense. I created a new issue for it [#5702](https://github.com/zeebe-io/zeebe/issues/5702)\\n\\n## New Issues\\n\\n * Limit Gateway https://github.com/zeebe-io/zeebe/issues/5699\\n * SerialGC usage https://github.com/zeebe-io/zeebe/issues/5700\\n * Wrong error response on deployment command https://github.com/zeebe-io/zeebe/issues/5702\\n \\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/10/13/multiple-leader-changes","metadata":{"permalink":"/zeebe-chaos/2020/10/13/multiple-leader-changes","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-10-13-multiple-leader-changes/index.md","source":"@site/blog/2020-10-13-multiple-leader-changes/index.md","title":"Multiple Leader Changes","description":"Today I wanted to add new chaostoolkit experiment, which we can automate.","date":"2020-10-13T00:00:00.000Z","formattedDate":"October 13, 2020","tags":[],"readingTime":3.39,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Multiple Leader Changes","date":"2020-10-13T00:00:00.000Z","categories":["chaos_experiment","broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Gateway memory consumption","permalink":"/zeebe-chaos/2020/10/27/standalone-gw-memory"},"nextItem":{"title":"Play around with ToxiProxy","permalink":"/zeebe-chaos/2020/10/06/toxi-proxy"}},"content":"Today I wanted to add new chaostoolkit experiment, which we can automate.\\nWe already have experiments like restarting followers and leaders for a partition, but in the past what also caused issues was multiple restarts/leader changes\\nin a short period of time. This is the reason why I created [#39](https://github.com/zeebe-io/zeebe-chaos/issues/39). \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment: Multiple Leader Elections\\n\\nIn order to reduce the blast radius I setup an new Zeebe cluster with one partition (clusterplan: production-s). This makes it possible that we exactly restart the leader for that one partition.\\nLater we can also try it out with multiple partitions. In our automated environment it is anyway executed with multiple partitions.\\n\\n### Steady State\\n\\nAll Brokers are ready and we are able to create new workflow instances on the partition one.\\n\\n### Hypothesis\\n\\nEven if we cause multiple leader changes due to broker restarts we should still be able to start new workflow instances on the corresponding partition.\\n\\n### Method \\n\\nWe requesting the Topology, determine the leader for partition one restart that corresponding node and wait until it is up again. We repeat that multiple times (three times).\\n\\n### Result\\n\\nThe corresponding experiment was added via this [commit](https://github.com/zeebe-io/zeebe-chaos/commit/11c3a96fc87991f649fb1559363ba335b2bf42a1).\\nWe were able to prove that our hypothesis is true. we are able to handle multiple leader changes even in a short period of time.\\n\\n#### Metrics\\n\\nIn the metrics we can see the behavior during the experiment and also we can see that it becomes healthy at the end.\\n\\n![general.png](general.png)\\n\\n![atomix.png](atomix.png)\\n\\nI also run this with a cluster plan M cluster with the same results:\\n\\n![multiple.png](multiple.png)\\n\\n#### Chaostoolkit\\n\\n```\\n(chaostk) [zell kubernetes/ ns:4ac065c1-a67e-4f47-8782-38a10d67515d-zeebe]$ chaos run multiple-leader-restart/experiment.json \\n[2020-10-13 14:01:30 INFO] Validating the experiment\'s syntax\\n[2020-10-13 14:01:30 INFO] Experiment looks valid\\n[2020-10-13 14:01:30 INFO] Running experiment: Zeebe Leader restart multiple times experiment\\n[2020-10-13 14:01:30 INFO] Steady-state strategy: default\\n[2020-10-13 14:01:30 INFO] Rollbacks strategy: default\\n[2020-10-13 14:01:30 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-13 14:01:30 INFO] Probe: All pods should be ready\\n[2020-10-13 14:01:30 INFO] Probe: Should be able to create workflow instances on partition one\\n[2020-10-13 14:01:32 INFO] Steady state hypothesis is met!\\n[2020-10-13 14:01:32 INFO] Playing your experiment\'s method now...\\n[2020-10-13 14:01:32 INFO] Action: Terminate leader of partition one\\n[2020-10-13 14:01:42 INFO] Pausing after activity for 5s...\\n[2020-10-13 14:01:47 INFO] Probe: All pods should be ready\\n[2020-10-13 14:02:32 INFO] Action: Terminate leader of partition one\\n[2020-10-13 14:02:41 INFO] Pausing after activity for 5s...\\n[2020-10-13 14:02:46 INFO] Probe: All pods should be ready\\n[2020-10-13 14:03:23 INFO] Action: Terminate leader of partition one\\n[2020-10-13 14:03:33 INFO] Pausing after activity for 5s...\\n[2020-10-13 14:03:38 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-13 14:03:38 INFO] Probe: All pods should be ready\\n[2020-10-13 14:04:12 INFO] Probe: Should be able to create workflow instances on partition one\\n[2020-10-13 14:04:16 INFO] Steady state hypothesis is met!\\n[2020-10-13 14:04:16 INFO] Let\'s rollback...\\n[2020-10-13 14:04:16 INFO] No declared rollbacks, let\'s move on.\\n[2020-10-13 14:04:16 INFO] Experiment ended with status: completed\\n```\\n\\n## Chaos Experiment: High Load\\n\\nAs mentioned last week @pihme has reported voluntarily that he wants to implement another chaos experiment.\\nHe worked on #7, where we expect that even we put high load on the Zeebe cluster we will cause no leader changes. This was in the past an failure case, where high load disrupted the cluster.\\n\\n\\n### Steady State\\n\\nAll Brokers are ready and we can create workflow instances on all partitions. We store the begining topology for later.\\n\\n### Hypothesis\\n\\nWe expect that even on high load we are not able to disrupt the cluster and that this will not cause any leader changes.\\n\\n### Method\\n\\nPut high load on the cluster for several minutes, via creating workflow instances\\n\\n### Result\\n\\n@pihme create a new PR to add the experiment [#41](https://github.com/zeebe-io/zeebe-chaos/pull/41) \\n\\n\\n#### Metrics\\n\\nWe see that we already put some load on the cluster but it is not enough to exhaust the request limits and reach back pressure.\\n\\n![highload](highload.png)\\n\\nWe neeed to find a good way how put high load on the Zeebe cluster. We will continue on this.\\n\\n## Participants\\n\\n  * @pihme\\n  * @zelldon"},{"id":"/2020/10/06/toxi-proxy","metadata":{"permalink":"/zeebe-chaos/2020/10/06/toxi-proxy","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-10-06-toxi-proxy/index.md","source":"@site/blog/2020-10-06-toxi-proxy/index.md","title":"Play around with ToxiProxy","description":"First chaos day since my parental leave.","date":"2020-10-06T00:00:00.000Z","formattedDate":"October 6, 2020","tags":[{"label":"tools","permalink":"/zeebe-chaos/tags/tools"}],"readingTime":3.275,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Play around with ToxiProxy","date":"2020-10-06T00:00:00.000Z","categories":["chaos_experiment","toxiProxy"],"tags":["tools"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Multiple Leader Changes","permalink":"/zeebe-chaos/2020/10/13/multiple-leader-changes"},"nextItem":{"title":"Experiment with Camunda Cloud","permalink":"/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud"}},"content":"First chaos day since my parental leave :tada:.\\n\\nToday I wanted to play a bit with [ToxiProxy](https://github.com/Shopify/toxiproxy). Toxiproxy is a framework for simulating network conditions and ideal to do some chaos on the network.\\n\\n\x3c!--truncate--\x3e\\n\\n## Run ToxiProxy\\n\\nDownload from the [release page](https://github.com/Shopify/toxiproxy/releases) the latest version (server and cli).\\n\\nStart a broker via docker.\\n\\n```sh\\ndocker pull camunda/zeebe:SNAPSHOT\\ndocker run -p 26500:26500 camunda/zeebe:SNAPSHOT\\n```\\n\\nStart the toxi proxy server.\\n\\n```sh\\n./toxiproxy-server-linux-amd64 start\\n```\\n\\nCreate a proxy for zeebe\\n```sh\\n./toxiproxy-cli-linux-amd64 create zeebe-proxy -l localhost:26379 -u localhost:26500\\nCreated new proxy zeebe-proxy\\n```\\n\\nYou should see something in the toxy proxy server log:\\n\\n```sh\\nINFO[0031] Started proxy                                 name=zeebe-proxy proxy=127.0.0.1:26379 upstream=localhost:26500\\n```\\n\\nTry zbctl to request the topology.\\n\\n```sh\\n./zbctl --address localhost:26379 status --insecure\\n\\nCluster size: 1\\nPartitions count: 1\\nReplication factor: 1\\nGateway version: 0.25.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - 172.17.0.2:26501\\n    Version: 0.25.0-SNAPSHOT\\n    Partition 1 : Leader\\n```\\n\\nIn the toxy proxy server log it should be shown as:\\n\\n```sh\\nINFO[0149] Accepted client                               client=127.0.0.1:41510 name=zeebe-proxy proxy=127.0.0.1:26379 upstream=localhost:26500\\nWARN[0149] Source terminated                             bytes=245 err=read tcp 127.0.0.1:56178->127.0.0.1:26500: use of closed network connection name=zeebe-proxy\\n```\\n\\nAdd latency to requests\\n\\n```sh\\n$ ./toxiproxy-cli-linux-amd64 toxic add -t latency -a latency=5000 zeebe-proxy\\nAdded downstream latency toxic \'latency_downstream\' on proxy \'zeebe-proxy\'\\n```\\n\\nRunning zbctl again:\\n\\n```sh\\n ./zbctl --address localhost:26379 status --insecure\\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\\n```\\n\\nUpdating existing toxy:\\n\\n```sh\\n./toxiproxy-cli-linux-amd64 toxic update -n latency_downstream -t latency -a latency=500 zeebe-proxy\\n```\\n\\nRunning zbctl again:\\n\\n```sh\\ntime ./zbctl --address localhost:26379 status --insecure\\nCluster size: 1\\nPartitions count: 1\\nReplication factor: 1\\nGateway version: 0.25.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - 172.17.0.2:26501\\n    Version: 0.25.0-SNAPSHOT\\n    Partition 1 : Leader\\n\\nreal\\t0m1.045s\\nuser\\t0m0.012s\\nsys\\t0m0.021s\\n\\n```\\n\\nInspect existing toxics:\\n\\n```sh\\n$ ./toxiproxy-cli-linux-amd64 inspect zeebe-proxy\\nName: zeebe-proxy\\tListen: 127.0.0.1:26379\\tUpstream: localhost:26500\\n======================================================================\\nUpstream toxics:\\nProxy has no Upstream toxics enabled.\\n\\nDownstream toxics:\\nlatency_downstream:\\ttype=latency\\tstream=downstream\\ttoxicity=1.00\\tattributes=[\\tjitter=0\\tlatency=500\\t]\\n\\n```\\n\\nWith toxicity we can change whether it should be applied on all requests or only on some. It is possible to add the latency instead of downstream to upstream. There also other things we can inject, like slicing and delaying packages, dropping packages and limiting the bandwith.\\n\\n\\nPossible new experiments:\\n\\n * introduce latency between one follower and leader - if only one follower experience delays we expect that no election is started\\n * introduce latency betweem gw and broker - see whether command timeout\\n * slice packages - drop packages, but not every packages - expect that command is send correctly since requests are retried\\n\\n\\n### Slice packages\\n\\nSlices packages after 128 bytes:\\n```sh\\n./toxiproxy-cli-linux-amd64 toxic add zeebe-proxy -t slicer -a average_size=128\\n```\\n\\nPublish message seem to work:\\n```sh\\n$ time ./zbctl --address localhost:26379 publish message failing --insecure --correlationKey key --variables \\"{}\\"\\n{\\n  \\"key\\": 2251799813685253\\n}\\n```\\n\\nAfter limiting it to 32 bytes:\\n\\n```sh\\n$ ./toxiproxy-cli-linux-amd64 toxic update -n slicer_downstream -a average_size=32 zeebe-proxy\\nUpdated toxic \'slicer_downstream\' on proxy \'zeebe-proxy\'\\n```\\n\\nThe publish message seem to not work as expected.\\n\\n```sh\\n$ time ./zbctl --address localhost:26379 publish message failing --insecure --correlationKey key --variables \\"{}\\"\\nnull\\n\\nreal\\t0m0.039s\\nuser\\t0m0.007s\\nsys\\t0m0.023s\\n```\\n\\nActually I would expect here an error instead of just returning null.\\n\\n## Chaos Experiment \\n\\n### No Leader change on high load\\n\\n Peter volunteered for automating a new chaos experiment, where we put high load on a broker and expect that we have no leader change. This was previous an issue, since the leaders were not able to send heartbeats in time. Related issue #7.\\n\\n### Time reset\\nI wanted to work on the clock reset [#3](https://github.com/zeebe-io/zeebe-chaos/issues/3).\\nThis seems to be not easily possible in kubernetes or at least with our current images, since we need for that root privilges.\\n\\n```sh\\nroot@zell-time-reset-zeebe-0:/usr/local/zeebe# date -s $(date +%Y-%m-%dT%H:%M:%S)\\ndate: cannot set date: Operation not permitted\\nTue Oct  6 11:51:19 UTC 2020\\n```\\n\\nIt seems that chaos mesh supports something like that for kubernetes maybe worth to look at\\nhttps://pingcap.com/blog/simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node\\n\\n\\n## Participants\\n\\n  * @pihme\\n  * @zelldon"},{"id":"/2020/08/20/experiment-with-camunda-cloud","metadata":{"permalink":"/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-08-20-experiment-with-camunda-cloud/index.md","source":"@site/blog/2020-08-20-experiment-with-camunda-cloud/index.md","title":"Experiment with Camunda Cloud","description":"In order to make our chaos experiments more realistic we have setup a new gke cluster, which is similar to the Camunda Cloud gke cluster.","date":"2020-08-20T00:00:00.000Z","formattedDate":"August 20, 2020","tags":[],"readingTime":3.875,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment with Camunda Cloud","date":"2020-08-20T00:00:00.000Z","categories":["chaos_experiment","cloud"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Play around with ToxiProxy","permalink":"/zeebe-chaos/2020/10/06/toxi-proxy"},"nextItem":{"title":"Experiment with Low Load","permalink":"/zeebe-chaos/2020/08/06/low-load"}},"content":"In order to make our chaos experiments more realistic we have setup a new gke cluster, which is similar to the Camunda Cloud gke cluster.\\nIt allows us to test and experiment with Zeebe clusters which have the same configuration as Zeebe clusters in the Camunda cloud.\\n\\nAs part of the chaos day I run the same benchmark we normally run in our gke with our configuration against the Camunda Cloud Zeebe clusters.\\n\\n\x3c!--truncate--\x3e\\n\\n## Configurations of Zeebe Clusters\\n\\nIn the following table I want to highlight the different configurations of the different Zeebe Clusters (cluster types).\\n\\n| Name | Our Default | Prod S | Prod M | Prod L |\\n|------|-------------|--------|--------|--------|\\n|Partitions|  3      |   1    |   4    |  8     |\\n|Nodes|       3       |   3    |   3    |   6    |\\n|Replication| 3     |   3     |   3   |    3   |\\n|SnapshotPeriod| 15 | 5 | 5 | 5 |\\n|CPU_THREADS| 4 | 1 | 4 | 4 |\\n|IO_THREADS| 4 | 2 | 4 | 4 |\\n|CPU LIMIT| 5 | 1 | 4 | 4 |\\n|CPU REQUEST| 5 | 200m | 200m | 200m |\\n|RAM LIMIT| 12Gi | 2Gi | 8Gi | 8Gi |\\n|RAM REQUEST| 12Gi | 250Mi | 250Mi | 250Mi|\\n|Gateway|Standalone|Embedded|Embedded|Embedded|\\n\\n## Benchmarks\\n\\n| Name | Our Default | Prod S | Prod M | Prod L |\\n|------|-------------|--------|--------|--------|\\n|General|![base](base.png)|![prods](prod-s-general.png)|![prods](prod-m-general.png)|![prods](prod-l-general.png)|\\n|Resources|![base](base-res.png)|![prods](prod-s-res.png)|![prods](prod-m-res.png)|![prods](prod-l-res.png)|\\n|Disk usage||![prods](prod-s-disk.png)|![prods](prod-m-disk.png)|![prods](prod-l-disk.png)|\\n|Latency|![base](base-latency.png)|![prods](prod-s-latency.png)|![prods](prod-m-latency.png)|![prods](prod-l-latency.png)|\\n|Working|![base](base.png)|![prods](prod-s-working.png)|![prods](prod-m-working.png)|![prods](prod-l-working.png)|\\n\\nIn general we can see that the clusters haven\'t survived long. This is also visible in our Camunda Cloud status page.\\n![status](status.png)\\n\\nI think it is kind of related with the preemtable nodes, high load, long restarts and that pods are restarted after 15 minutes, when there are not getting ready.\\nOne of the reasons why restarting takes so long is fixed now with [#5189](https://github.com/zeebe-io/zeebe/pull/5189) so I hope that this gets better. But currently it is an issue, since you start replicating a snapshot and reprocess on start up. If this takes longer then 15 min the pod will be restarted because of this configuration: `Liveness:   http-get http://:9600/ready delay=900s timeout=1s period=15s #success=1 #failure=3` after restarting the pod you haven\'t gained any value you just need to start again the complete procedure. In k8 we can see a high restart count of the pods.\\n\\nInteresting is if we take a look at the working part of Prod S then we clearly see how often actually a pod is preemted or leader change happens.\\n\\n![prod-s](prod-s-working.png)\\n\\nIt is a known issue that currently the nodes are preemted quite often in Camunda Cloud and they working on a solution to it.\\n\\n### Throughput\\n\\nIf we take a look at the Working part we can see that we scale based on the partition count (or prod cluster sizes) in Camunda Cloud. For Prod S we reach in avg ~24 workflow instance creation/completions per second. For Prod M we reach in avg ~46 workflow instance creation/completions per second. For Prod L we reach in avg ~99 workflow instance creations and completions. To be fair I run the benchmark on these cluster sizes only with three workers, which have 8 threads an activation count of 120 and they completing an job after 150 ms delay, and an starter which starts 100 workflow instances per second. Normally we use in our benchmarks 12 workers and start 300 workflow instances per second. I tried that with the Prod L cluster, but this failed quite fast after increasing the load. Here we probably need to investigate further. If we take a look at our cluster setup then we reach in avg ~147 workflow instance creations/completions per second.\\n\\n### Latency\\n\\nIf take a look at the latency we can see that in Prod M cluster the latency seems to be a bit problematic, where in Prod S and L it seems similar. In our default cluster we get the best latency. Might be worth to take a look as well.\\n\\n### Other Observations\\n\\nDuring the benchmark observations I saw that some metrics are missing.\\n\\nFor example the Gateway metrics are not shown:\\n\\n![gw](missing-gw-metrics.png)\\n\\nFurthermore I saw that all container related and pvc related metrics are missing. I was not able to check the IO metrics nor the CPU metrics and other.\\n\\n![io](missing-io.png)\\n\\nIf we want to run more tests and chaos experiments we need to fix these missing metrics before. Opened a new issue for it [#242](https://github.com/camunda-cloud/monitoring/issues/242)\\n\\n## Running automated Chaos experiments\\n\\nIn order to run automated chaos experiments in our new gke. I had to create a new serviceaccount and rolebindings, such that our Jenkins can access the new Kubernetes cluster and our experiments can delete and create new resources.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/08/06/low-load","metadata":{"permalink":"/zeebe-chaos/2020/08/06/low-load","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-08-06-low-load/index.md","source":"@site/blog/2020-08-06-low-load/index.md","title":"Experiment with Low Load","description":"* Run a benchmark with low load","date":"2020-08-06T00:00:00.000Z","formattedDate":"August 6, 2020","tags":[],"readingTime":2.91,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment with Low Load","date":"2020-08-06T00:00:00.000Z","categories":["chaos_experiment","load broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment with Camunda Cloud","permalink":"/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud"},"nextItem":{"title":"Experiment without Exporters","permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters"}},"content":"* Run a benchmark with low load\\n * Investigate last benchmark failures\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n We currently seem to have issues with RocksDB, which sometimes generates a lot of SST files during the Broker lifetime. This causes to fail the snapshot replication at some point.\\n This is especially problematic after pod\'s get restarted, since a follower normally need either to be catched up with the log or the leader will send a snapshot to the follower.\\n If the snapshot contains a lot of files this get\'s problematic. In order to understand this better we would like to find out how we can reproduce it. We expected this happens only on low load,\\n the assumption is that RocksDB will not trigger the compaction so often, because we are not reaching a certain threshold. See the related issue [#4887](https://github.com/zeebe-io/zeebe/issues/4887) .\\n\\n### Expected\\n\\n When creating workflow instances and completing them directly afterwards we normally expect that there is nothing left and nothing should accumulate together. If we do it on low load, which means 1 workflow instance creation and completion per second then we expect the same. Furthermore we expect that the used resources are lower then on higher load.\\n\\n### Actual\\n\\n We have setup our default benchmark with three partition, three nodes and replication factor three. We starting one workflow instance at a time. We running one worker, which completes the related workflow.\\n\\nIn the general overview we can see that we start and complete one workflow per second.\\n![general](general-one-workflow.png)\\n\\nThe resource consumption looks ok.\\n![resource](resources-one-workflow.png)\\n\\nBut the RocksDB used size and snapshot files seem to be increasing.\\n![rocks1](rocks1.png)\\n![rocks2](rocks2.png)\\n![snapshot](https://user-images.githubusercontent.com/2758593/89533130-384fcc80-d7f3-11ea-83b8-69bbe75f5211.png)\\n \\n\\nI will let the benchmark run a bit longer and I think we need to investigate this issue further.\\n\\n## Investigation Last Benchmark\\n\\nEvery week I create a benchmark on the chaos day and let it run until the next chaos day. Since I had not much time for trying out other experiments today and I saw that the old benchmark has problems I decided to investigated why the processing went down. I collected here my observations and use this more as a summary, since until now it is not clear what is the issue.\\n\\nIn the general overview we can see that the processing is in avg under 100 workflow instances per second, normally we would expect something around 130. We can also see that the processing in general is not that stable.\\n![general](general.png)\\n\\nIf we take a look at the partitions separatly we can see that the partition two died quite early.\\n\\n**Partition 1 last 7 days**\\n![partition-1-general-7-days](https://user-images.githubusercontent.com/2758593/89543087-cb433380-d800-11ea-9d03-27d60a6cc49d.png)\\n**Partition 2 last 7 days**\\n![partition-2-general-7-days](https://user-images.githubusercontent.com/2758593/89543091-cb433380-d800-11ea-80ab-922c483cd039.png)\\n**Partition 3 last 7 days**\\n![partition-3-general-7-days](https://user-images.githubusercontent.com/2758593/89543093-cbdbca00-d800-11ea-9e86-d14b165e02eb.png)\\n\\nWe can see that the processing seem to be stopped for the partition two and never comes back. This seem to happen on the 31-07-2020 ~ 2 pm.\\nWith the resource panel we can also see that at this time a node preemption happen, since all pod seem to be rescheduled. This can be seen based on the different colors of the graphs.\\n\\n![resources](https://user-images.githubusercontent.com/2758593/89543095-cbdbca00-d800-11ea-962e-aee3fa95a826.png)\\n\\nIn the log we can see that all brokers are getting closed.\\n\\nInteresting is that we see the day after, an continously growing disk usage, which never gets reduced.\\n\\n![disk-usage](https://user-images.githubusercontent.com/2758593/89543083-c9797000-d800-11ea-82f3-a1b505811171.png)\\n\\nCurrently I don\'t understand what was the issue and why it never comes back. I created an issue to further investigate that [#5127](https://github.com/zeebe-io/zeebe/issues/5127) . I will keep the old benchmark running and will setup a separe one in order to reproduce this.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/30/experiment-without-exporters","metadata":{"permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-07-30-experiment-without-exporters/index.md","source":"@site/blog/2020-07-30-experiment-without-exporters/index.md","title":"Experiment without Exporters","description":"* Run a chaos experiment without exporters","date":"2020-07-30T00:00:00.000Z","formattedDate":"July 30, 2020","tags":[],"readingTime":5.755,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment without Exporters","date":"2020-07-30T00:00:00.000Z","categories":["chaos_experiment","exporters"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment with Low Load","permalink":"/zeebe-chaos/2020/08/06/low-load"},"nextItem":{"title":"Big Multi Instance","permalink":"/zeebe-chaos/2020/07/16/big-multi-instance"}},"content":"* Run a chaos experiment without exporters\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n We wanted to run a chaos experiment, which covers [#20](https://github.com/zeebe-io/zeebe-chaos/issues/20).\\n Furthermore, it was recently asked in the forum whether it makes a difference performance wise to run a broker without exporters, see [here](https://forum.zeebe.io/t/zeebe-low-performance/1356/17) \\n\\n### Expected\\n\\n Running Broker without exporter should work without any problems. We should be able to delete data on every snapshot interval.\\n Performance wise there should be no difference. \\n\\n### Actual\\n\\n We have setup three different benchmarks:\\n\\n * default with elastic and metrics exporter enabled\\n * only with metrics exporter\\n * without any exporter\\n\\n These benchmarks run overnight without bigger issues. This means all of three where able to take snapshots and compact the log. This satisfy our hypothesis of https://github.com/zeebe-io/zeebe-chaos/issues/20 .\\n\\n| Default | Without exporters |\\n|---|---|\\n| ![default-pvc](default-pvc.png) | ![without-exporter-pvc](without-exporter-pvc.png) |\\n\\nThe resource consumption seem to be kind of similar, but we still see that the memory usage increases overtime. This seems to be related to [#4812](https://github.com/zeebe-io/zeebe/issues/4812)\\n\\n| Default | Metric Exporter | Without exporters |\\n|---|---|---|\\n| ![default](default-resources.png) | ![metric](metric-exporter-resources.png) | ![without](without-exporter-resources.png) |\\n\\n Unexpected was that we see a difference in throughput. The benchmark without exporters seem to have a better throughput overall. It is able to complete ~ 30 workflow instances more per second, then the other benchmarks.\\n\\n| Default | Metric Exporter | Without exporters |\\n|---|---|---|\\n| ![default](default-general.png) | ![metric](metric-exporter-general.png) | ![without](without-exporter-general.png) |\\n\\n  We compared also other benchmarks which we have currently running, e.g. a snapshot from 24-07-2020 or from the 0.24.1 release. \\n\\n| Snapshot 24-07 | Release 0.24.1 |\\n|---|---|\\n| ![snapshot](snapshot-24-7-general.png) | ![release-241](release-0241-general.png) |\\n\\n  All benchmarks with exporters seem to have a throughput around ~140 workflow instance completions per second, but the benchmarks without exporters reaches ~170 workflow instance completions per second.\\n When we check the metrics we can see that sometimes brokers are leader for all partition and sometimes it is good distributed, but even this makes not that huge difference as the fact to having no exporter.\\n This is unexpected and we need to investigate further, created new issue for this [#5085](https://github.com/zeebe-io/zeebe/issues/5085)\\n \\n The latency seems to be not affected by this.\\n\\n#### General Observations\\n\\n##### RocksDB\\n\\nAfter taking a look at the metrics of the different benchmarks we can see that at one benchmark we have a higher live data size, which is unexepected.\\n\\n\\n  ![without-exporter-rocksdb](without-exporter-rocksdb.png)\\n  ![default-rocksdb](default-rocksdb.png)\\n  ![metric-exporter-rocksdb](default-rocksdb.png)\\n\\nCreated an issue for it [#5081](https://github.com/zeebe-io/zeebe/issues/5081)\\n\\n##### Atomix Bootstrap\\n\\nOn taking a look at the logs during starting the benchmarks we can see that the logging of atomix is not really useful.\\n\\nIt prints several statements which seem to be just noisy.\\n```\\nI 2020-07-30T10:17:54.198527Z TCP server listening for connections on 0.0.0.0:26502 \\nI 2020-07-30T10:17:54.199468Z Started \\nI 2020-07-30T10:17:54.223547Z UDP server listening for connections on 0.0.0.0:26502 \\nI 2020-07-30T10:17:54.224941Z Joined \\nI 2020-07-30T10:17:54.228644Z Started \\nI 2020-07-30T10:17:54.229384Z Started \\nI 2020-07-30T10:17:54.229856Z Started \\nI 2020-07-30T10:17:54.231121Z Started \\n```\\n\\nCreated an issue for it [#5080](https://github.com/zeebe-io/zeebe/issues/5080)\\n\\n##### Merged log statement\\n\\nAnother issue we can see during startup is that from time the log statements are merged together in stackdriver.\\n\\nLooks like this:\\n\\n```\\nI 2020-07-30T09:16:47.053339Z Bootstrap Broker-1 partitions succeeded. Started 3 steps in 166 ms. \\nI 2020-07-30T09:16:47.053352763Z {\\"severity\\":\\"DEBUG\\",\\"logging.googleapis.com/sourceLocation\\":{\\"function\\":\\"startStepByStep\\",\\"file\\":\\"StartProcess.java\\",\\"line\\":63},\\"message\\":\\"Bootstrap Broker-1 partitions [3/3]: partition 1 started in 7 ms\\",\\"serviceContext\\":{\\"service\\":\\"zeebe-broker\\",\\"version\\":\\"zeebe-chaos-metric-exporter\\"},\\"context\\":{\\"threadId\\":1,\\"broker-id\\":\\"Broker-1\\",\\"threadPriority\\":5,\\"loggerName\\":\\"io.zeebe.broker.system\\",\\"threadName\\":\\"main\\"},\\"timestampSeconds\\":1596100607,\\"timestampNanos\\":52869000,\\"logger\\":\\"io.zeebe.broker.system\\",\\"thread\\":\\"main\\"}{\\"severity\\":\\"DEBUG\\",\\"logging.googleapis.com/sourceLocation\\":{\\"function\\":\\"calculateHealth\\",\\"file\\":\\"CriticalComponentsHealthMonitor.java\\",\\"line\\":91},\\"message\\":\\"The components are healthy. The current health status of components: {ZeebePartition-1=HEALTHY}\\",\\"serviceContext\\":{\\"service\\":\\"zeebe-broker\\",\\"version\\":\\"zeebe-chaos-metric-exporter\\"},\\"context\\":{\\"threadId\\":263,\\"threadPriority\\":5,\\"loggerName\\":\\"io.zeebe.broker.system\\",\\"threadName\\":\\"Broker-1-zb-actors-3\\",\\"actor-name\\":\\"Broker-1-ZeebePartition-1\\"},\\"timestampSeconds\\":1596100607,\\"timestampNanos\\":52860000,\\"logger\\":\\"io.zeebe.broker.system\\",\\"thread\\":\\"Broker-1-zb-actors-3\\"}\\n \\nI 2020-07-30T09:16:47.053379451Z \\n\\n```\\n\\nCreated an issue for it [#5079](https://github.com/zeebe-io/zeebe/issues/5079)\\n\\n##### Wrong Exporter Configuration\\nWhen Exporter is configured falsely it breaks the start up, which means the exporter can\'t be loaded and we see an exception.\\n\\n```\\n{\\n insertId: \\"1mp0o7821rhxwebj4\\"  \\n jsonPayload: {\\n  context: {\u2026}   \\n  exception: \\"java.lang.IllegalStateException: Failed to load exporter with configuration: ExporterCfg{, jarPath=\'null\', className=\'\', args={index={ignoreVariablesAbove=32767}, url=http://elasticsearch-master:9200}}\\n\\tat io.zeebe.broker.system.partitions.ZeebePartition.<init>(ZeebePartition.java:145) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.lambda$partitionsStep$22(Broker.java:344) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.lambda$startStepByStep$2(StartProcess.java:60) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.startStepByStep(StartProcess.java:58) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.start(StartProcess.java:43) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.partitionsStep(Broker.java:352) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.lambda$initStart$10(Broker.java:184) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.lambda$startStepByStep$2(StartProcess.java:60) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.startStepByStep(StartProcess.java:58) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.start(StartProcess.java:43) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.internalStart(Broker.java:135) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.util.LogUtil.doWithMDC(LogUtil.java:21) [zeebe-util-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.start(Broker.java:115) [zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:65) [zeebe-distribution-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:795) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:779) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:322) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat io.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:52) [zeebe-distribution-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\nCaused by: io.zeebe.broker.exporter.repo.ExporterLoadException: Cannot load exporter [elasticsearch]: cannot load specified class\\n\\tat io.zeebe.broker.exporter.repo.ExporterRepository.load(ExporterRepository.java:81) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.system.partitions.ZeebePartition.<init>(ZeebePartition.java:143) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\t... 23 more\\nCaused by: java.lang.ClassNotFoundException: \\n\\tat jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source) ~[?:?]\\n\\tat jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source) ~[?:?]\\n\\tat java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]\\n\\tat io.zeebe.broker.exporter.repo.ExporterRepository.load(ExporterRepository.java:78) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.system.partitions.ZeebePartition.<init>(ZeebePartition.java:143) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\t... 23 more\\n\\"   \\n  logger: \\"io.zeebe.broker.system\\"   \\n  message: \\"Bootstrap Broker-1 [11/12]: zeebe partitions failed with unexpected exception.\\"   \\n  serviceContext: {\u2026}   \\n  thread: \\"main\\"   \\n }\\n labels: {\u2026}  \\n logName: \\"projects/zeebe-io/logs/stdout\\"  \\n receiveTimestamp: \\"2020-07-30T04:58:07.561999605Z\\"  \\n resource: {\u2026}  \\n severity: \\"INFO\\"  \\n sourceLocation: {\u2026}  \\n timestamp: \\"2020-07-30T04:58:04.221614Z\\"  \\n}\\n```\\n\\n\\nThe Broker is shutdown afterwards.\\n\\n```\\nI 2020-07-30T05:02:58.275180Z Bootstrap Broker-1 partitions [1/3]: partition 3 \\nI 2020-07-30T05:02:58.288302Z Bootstrap Broker-1 partitions [1/3]: partition 3 failed with unexpected exception. \\nI 2020-07-30T05:02:58.289941Z Closing Broker-1 partitions succeeded. Closed 0 steps in 0 ms. \\nI 2020-07-30T05:02:58.290305Z Bootstrap Broker-1 [11/12]: zeebe partitions failed with unexpected exception. \\nI 2020-07-30T05:02:58.291341Z Closing Broker-1 [1/10]: leader management request handler \\nD 2020-07-30T05:02:58.293136Z Closing Broker-1 [1/10]: leader management request handler closed in 2 ms \\nI 2020-07-30T05:02:58.293512Z Closing Broker-1 [2/10]: disk space monitor \\nD 2020-07-30T05:02:58.294140Z Closing Broker-1 [2/10]: disk space monitor closed in 1 ms \\nI 2020-07-30T05:02:58.294509Z Closing Broker-1 [3/10]: monitoring services \\nD 2020-07-30T05:02:58.295059Z Closing Broker-1 [3/10]: monitoring services closed in 1 ms \\nI 2020-07-30T05:02:58.295371Z Closing Broker-1 [4/10]: topology manager \\nD 2020-07-30T05:02:58.295964Z Closing Broker-1 [4/10]: topology manager closed in 0 ms \\nI 2020-07-30T05:02:58.296264Z Closing Broker-1 [5/10]: cluster services \\nD 2020-07-30T05:02:58.296612Z Closing Broker-1 [5/10]: cluster services closed in 0 ms \\nI 2020-07-30T05:02:58.296928Z Closing Broker-1 [6/10]: subscription api \\nD 2020-07-30T05:02:58.297450Z Closing Broker-1 [6/10]: subscription api closed in 0 ms \\nI 2020-07-30T05:02:58.297763Z Closing Broker-1 [7/10]: command api handler \\nD 2020-07-30T05:02:58.298951Z Closing Broker-1 [7/10]: command api handler closed in 0 ms \\nI 2020-07-30T05:02:58.299366Z Closing Broker-1 [8/10]: command api transport \\nI 2020-07-30T05:03:00.320886Z Stopped \\nD 2020-07-30T05:03:00.321691Z Closing Broker-1 [8/10]: command api transport closed in 2022 ms \\nI 2020-07-30T05:03:00.322220Z Closing Broker-1 [9/10]: membership and replication protocol \\nI 2020-07-30T05:03:00.323776Z RaftServer{raft-partition-partition-3} - Transitioning to INACTIVE \\nI 2020-07-30T05:03:00.324179Z RaftServer{raft-partition-partition-2} - Transitioning to INACTIVE \\nI 2020-07-30T05:03:00.324205Z RaftServer{raft-partition-partition-1} - Transitioning to INACTIVE \\nI 2020-07-30T05:03:00.343039Z Stopped \\nI 2020-07-30T05:03:00.344506Z Stopped \\nI 2020-07-30T05:03:00.345422Z Stopped \\nI 2020-07-30T05:03:00.346547Z Stopped \\nI 2020-07-30T05:03:00.347267Z 1 - Member deactivated: Member{id=1, address=zeebe-chaos-zeebe-1.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAQABAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlJAAAAemVlYmUtY2hhb3MtemVlYmUtMS56ZWViZS1jaGFvcy16ZWViZS56ZWViZS1jaGFvcy5zdmMuY2x1c3Rlci5sb2NhbDoyNjUwMQUAAAwAAA8AAAAwLjI1LjAtU05BUFNIT1Q=}} \\nI 2020-07-30T05:03:00.347747Z Stopped \\nI 2020-07-30T05:03:00.348326Z Left \\nI 2020-07-30T05:03:00.349083Z Stopped \\nI 2020-07-30T05:03:04.364059Z Stopped \\nI 2020-07-30T05:03:04.364994Z Stopped \\nD 2020-07-30T05:03:04.365955Z Closing Broker-1 [9/10]: membership and replication protocol closed in 4043 ms \\nI 2020-07-30T05:03:04.366433Z Closing Broker-1 [10/10]: actor scheduler \\nD 2020-07-30T05:03:04.366879Z Closing blocking task runner \\nD 2020-07-30T05:03:04.367217Z Closing actor thread ground \'Broker-1-zb-fs-workers\' \\nD 2020-07-30T05:03:04.368641Z Closing actor thread ground \'Broker-1-zb-fs-workers\': closed successfully \\nD 2020-07-30T05:03:04.369018Z Closing actor thread ground \'Broker-1-zb-actors\' \\nD 2020-07-30T05:03:04.369955Z Closing blocking task runner: closed successfully \\nD 2020-07-30T05:03:04.370112Z Closing actor thread ground \'Broker-1-zb-actors\': closed successfully \\nD 2020-07-30T05:03:04.371034Z Closing Broker-1 [10/10]: actor scheduler closed in 4 ms \\nI 2020-07-30T05:03:04.371414Z Closing Broker-1 succeeded. Closed 10 steps in 6080 ms. \\nE 2020-07-30T05:03:04.371769Z Failed to start broker 1! \\n```\\nThis is works without problems. I think this is good to know.\\n\\n\\n### Code\\n\\nTo deploy a benchmark without exporters we had to do a `helm template` and remove all exporter related env variables.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/16/big-multi-instance","metadata":{"permalink":"/zeebe-chaos/2020/07/16/big-multi-instance","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-07-16-big-multi-instance/index.md","source":"@site/blog/2020-07-16-big-multi-instance/index.md","title":"Big Multi Instance","description":"* investigate and fix automated chaos experiments - works again with 88c404f and cd8d685","date":"2020-07-16T00:00:00.000Z","formattedDate":"July 16, 2020","tags":[],"readingTime":2.82,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Big Multi Instance","date":"2020-07-16T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment without Exporters","permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters"},"nextItem":{"title":"Experiment with Timers and Huge Variables","permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables"}},"content":"* investigate and fix automated chaos experiments - works again with [88c404f](https://github.com/zeebe-io/zeebe-chaos/commit/88c404f97514d4a7a511ce9751085acdd1720cd9) and [cd8d685](https://github.com/zeebe-io/zeebe-chaos/commit/cd8d685b83eaa1ac9050ad3d16868389e1c0c36d)\\n * Closed some issues in the backlog\\n * Run a chaos experiment with bigger multi instance to reach `maxMessageSize`\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n We wanted to run a chaos experiment, which covers [#33](https://github.com/zeebe-io/zeebe-chaos/issues/33).\\n\\n### Expected\\n\\n If we reach `maxMessageSize` in a workflow instance, for example via variables we expect that an incident is created or at least an error event and we can see that in operate. Furthermore we expect that the partition processing is not stopped, which means we can still create new instances.\\n\\n### Actual\\n\\n The experiment uses parallel multiInstance service tasks, to create a lot of tasks which should be completed with big variables.\\n\\n ![multiInstance](multiInstance.png)\\n\\n On collecting the output the `maxMessageSize` should be reached and we expected either an incident or exception for this instance. This should not affect other workflow instance creations.\\n\\n ![overview](overview.png)\\n\\n In operate we can see that we have two running workflow instances, one was started after the first failed. Later we created multiple instance\'s in a loop, without issues. This means we are not breaking partition processing, otherwise we would see timeouts.\\n \\n The problem we have is that we are not able to see in Operate that the workflow instance is actually broken. We have no indication for that.\\n\\n ![broken-multi](broken-multi.png)\\n\\n The instances seem still to be in starting the multi instance, but actually they are already blacklisted. If we check the logs we can find the following:\\n\\n```\\nI 2020-07-16T13:05:20.630315Z Error event was committed, we continue with processing. \\nE 2020-07-16T13:05:26.699139Z Expected to write one or more follow up events for event \'LoggedEvent [type=0, version=0, streamId=2, position=567, key=4503599627370769, timestamp=1594904720629, sourceEventPosition=565]\' without errors, but exception was thrown. \\nE 2020-07-16T13:05:26.735966Z Expected to process event \'TypedEventImpl{metadata=RecordMetadata{recordType=EVENT, intentValue=255, intent=ELEMENT_ACTIVATED, requestStreamId=-2147483648, requestId=-1, protocolVersion=1, valueType=WORKFLOW_INSTANCE, rejectionType=NULL_VAL, rejectionReason=}, value={\\"bpmnProcessId\\":\\"chaos\\",\\"version\\":2,\\"workflowKey\\":2251799813685359,\\"workflowInstanceKey\\":4503599627370761,\\"elementId\\":\\"chaosTask\\",\\"flowScopeKey\\":4503599627370769,\\"bpmnElementType\\":\\"SERVICE_TASK\\",\\"parentWorkflowInstanceKey\\":-1,\\"parentElementInstanceKey\\":-1}}\' without errors, but exception occurred with message \'Expected to claim segment of size 8439432, but can\'t claim more than 4194304 bytes.\' . \\nW 2020-07-16T13:05:26.737532Z Blacklist workflow instance 4503599627370761, due to previous errors. \\nI 2020-07-16T13:05:26.738421Z Error record was written at 568, we will continue with processing if event was committed. Current commit position is 567. \\nI 2020-07-16T13:05:26.793523Z Error event was committed, we continue with processing.\\n\\n```\\n\\nI created a feature request for operate [OPE-1037](https://jira.camunda.com/browse/OPE-1037)\\n\\nIn general chaos experiment succeeded, since we not breaking processing and we are still able to process other instances, but we only see that the instance is blacklisted in the logs not in operate, which is a problem from my POV. Furthermore a bit unexpected was, that we already failed before, we are not able to activate jobs, since the size of the multi instance was already to big.\\n\\n### Code\\n\\n```csharp\\n    // create zeebe client\\n    var client = ZeebeClient.Builder()\\n        .UseLoggerFactory(new NLogLoggerFactory())\\n        .UseGatewayAddress(ZeebeUrl)\\n        .UseTransportEncryption()\\n        .UseAccessTokenSupplier(\\n            CamundaCloudTokenProvider.Builder()\\n                .UseAuthServer(AuthServer)\\n                .UseClientId(ClientId)\\n                .UseClientSecret(ClientSecret)\\n                .UseAudience(Audience)\\n                .Build())\\n        .Build();\\n\\n    var topology = await client.TopologyRequest().Send();\\n\\n    Logger.Info(topology.ToString);\\n    Console.WriteLine(topology);\\n    // deploy\\n    var deployResponse = await client.NewDeployCommand()\\n        .AddResourceFile(DemoProcessPath)\\n        .Send();\\n\\n    // create workflow instance\\n    var intArray = Enumerable.Range(0, 10_000).ToArray();\\n    var jsonObject = new {list = intArray};\\n    var jsonString = JsonConvert.SerializeObject(jsonObject);\\n\\n    await client.NewCreateWorkflowInstanceCommand()\\n                .BpmnProcessId(\\"chaos\\").LatestVersion()\\n                .Variables(jsonString)\\n                .Send();\\n\\n    // open job worker\\n    using (var signal = new EventWaitHandle(false, EventResetMode.AutoReset))\\n    {\\n        client.NewWorker()\\n              .JobType(JobType)\\n              .Handler(HandleJob)\\n              .MaxJobsActive(120)\\n              .Name(WorkerName)\\n              .AutoCompletion()\\n              .PollInterval(TimeSpan.FromMilliseconds(100))\\n              .Timeout(TimeSpan.FromSeconds(10))\\n              .PollingTimeout(TimeSpan.FromSeconds(30))\\n              .Open();\\n\\n        // blocks main thread, so that worker can run\\n        signal.WaitOne();\\n    }\\n\\nprivate static void HandleJob(IJobClient jobClient, IJob job)\\n{\\n    Logger.Debug(\\"Handle job {job}\\", job.Key);\\n\\n    var bigPayload = File.ReadAllText(PayloadPath);\\n    jobClient.NewCompleteJobCommand(job).Variables(bigPayload).Send();\\n}\\n  \\n```\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/09/timer-and-huge-variables","metadata":{"permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-07-09-timer-and-huge-variables/index.md","source":"@site/blog/2020-07-09-timer-and-huge-variables/index.md","title":"Experiment with Timers and Huge Variables","description":"* Failure documentation about RAFT","date":"2020-07-09T00:00:00.000Z","formattedDate":"July 9, 2020","tags":[],"readingTime":3.43,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment with Timers and Huge Variables","date":"2020-07-09T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Big Multi Instance","permalink":"/zeebe-chaos/2020/07/16/big-multi-instance"},"nextItem":{"title":"Extract K8 resources from namespace","permalink":"/zeebe-chaos/2020/07/02/extract-k8-resources"}},"content":"* Failure documentation about RAFT\\n * Added chaos day summaries to repo\\n * Run Chaos experiment with a lot of timers\\n * Run Chaos experiment with huge variables\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n### A Lot of Timers \\n\\nBased on the Hypothesis written here:  [#31](https://github.com/zeebe-io/zeebe-chaos/issues/31) we run an experiment with a stable load of 10 simple workflow instances per second (only start and end event) and 10 workflow instances with \\nmultiple timers. We wanted to explore what happens when we have a lot of timers running and especially what happens when the are triggered at once. We created the following workflow model, where timers are exponentially created.\\n\\n![timerProcess](timerProcess.png)\\n\\nThe experiments is based on the hypotheses we wrote here [#31](https://github.com/zeebe-io/zeebe-chaos/issues/31).\\n\\n#### Expectations \\n\\n * at some point we can\'t create new instances from out side, since backpressure will block that\\n * the metrics for processing records (events/commands) should be stable since there will be always new events/records\\n * the cluster itself should be stable\\n\\n\\n#### Observations\\n\\nDuring running the experiments we saw that indeed we were not able to create new instances at some point.\\nThe cluster kept stable and there was no leader change at all. Unexpected was the behavior of the processing record metrics, since it fluctuates a lot.\\n\\nFurthermore we reached really high processing records throughput, which we normally not see.\\n\\n![overall](overall.png)\\n\\nThe log appender backpressure seem to work, at some point it deferred around 1.3 million records.\\n\\nOn resource consumption side it seems that memory is growing, but we create also more timer might be the issue.\\n\\n![mem](mem.png)\\n\\nInteresting was that we saw a huge difference between processing and exporting.\\n\\n![exportvsprocess](exportvsprocess.png) \\n\\nThe issue we currently have is that we stop processing to trigger/evaluate due timers and write them to the log.\\nAfter we did that we will process a bunch of events again and then trigger/evaluate again. I think this should be decoupled to streamline the processing throughput. I created a new issue for that [#4931](https://github.com/zeebe-io/zeebe/issues/4931)\\n\\n### Huge Variables\\n\\nIn order to understand better what is the impact of huge variables I did a small experiment with a payload which was ~ 5 MB.\\n\\nI expected that this will not work, since this is larger then the maximum message size. I would expect an appropriate error message, but unfortunately I just got a cancel on the client side.\\n\\n\\nStarter Exception:\\n```\\n2020-07-09T12:27:20.945889183Z 12:27:20.945 [Thread-2] WARN  io.zeebe.ResponseChecker - Request failed\\n I \\n2020-07-09T12:27:20.945909759Z java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: CANCELLED: HTTP/2 error code: CANCEL\\n I \\n2020-07-09T12:27:20.945924206Z Received Rst Stream\\n I \\n2020-07-09T12:27:20.945929123Z \\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.945934351Z \\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.945939157Z \\tat io.zeebe.ResponseChecker.run(ResponseChecker.java:41) [app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945972595Z Caused by: io.grpc.StatusRuntimeException: CANCELLED: HTTP/2 error code: CANCEL\\n I \\n2020-07-09T12:27:20.945978663Z Received Rst Stream\\n I \\n2020-07-09T12:27:20.945983657Z \\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945988515Z \\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:449) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945993803Z \\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945998591Z \\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946003749Z \\tat me.dinowernli.grpc.prometheus.MonitoringClientCallListener.onClose(MonitoringClientCallListener.java:50) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946007316Z \\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:426) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946010626Z \\tat io.grpc.internal.ClientCallImpl.access$500(ClientCallImpl.java:66) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946013822Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:689) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946017026Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$900(ClientCallImpl.java:577) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946020414Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:751) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946025498Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:740) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946030508Z \\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946035265Z \\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946040Z \\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.946045833Z \\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.946050880Z \\tat java.lang.Thread.run(Unknown Source) ~[?:?]\\n I \\n```\\n\\n\\nInteresting is what happens on the Gateway:\\n\\n```\\n2020-07-09 14:27:33.929 CEST\\nStream Error\\n \\nExpand all | Collapse all{\\n insertId: \\"yoxfczvg9usifqd7d\\"  \\n jsonPayload: {\\n  context: {\u2026}   \\n  exception: \\"io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 10277\\n\\tat io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:147) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:596) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:239) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:422) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:174) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378) [netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438) [netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) [netty-codec-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) [netty-codec-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) [netty-codec-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792) [netty-transport-native-epoll-4.1.50.Final-linux-x86_64.jar:4.1.50.Final]\\n\\tat io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:387) [netty-transport-native-epoll-4.1.50.Final-linux-x86_64.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [netty-transport-native-epoll-4.1.50.Final-linux-x86_64.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat java.lang.Thread.run(Unknown Source) [?:?]\\n\\"   \\n  logger: \\"io.grpc.netty.NettyServerHandler\\"   \\n  message: \\"Stream Error\\"   \\n  serviceContext: {\u2026}   \\n  thread: \\"grpc-default-worker-ELG-1-1\\"   \\n }\\n\\n```\\n\\nCreated an issue for this: [#4928](https://github.com/zeebe-io/zeebe/issues/4928)\\n\\nI will probably continue with this next week.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/02/extract-k8-resources","metadata":{"permalink":"/zeebe-chaos/2020/07/02/extract-k8-resources","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-07-02-extract-k8-resources/index.md","source":"@site/blog/2020-07-02-extract-k8-resources/index.md","title":"Extract K8 resources from namespace","description":"* Research: Read about DiRT (Disaster Recovery Tests) @ google - gave me same new ideas for more game days","date":"2020-07-02T00:00:00.000Z","formattedDate":"July 2, 2020","tags":[],"readingTime":1.02,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Extract K8 resources from namespace","date":"2020-07-02T00:00:00.000Z","categories":["kubernetes"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment with Timers and Huge Variables","permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables"},"nextItem":{"title":"Gateway Network Partition","permalink":"/zeebe-chaos/2020/06/25/gateway-network-partition"}},"content":"* Research: Read about DiRT (Disaster Recovery Tests) @ google - gave me same new ideas for more game days\\n * Failure documentation about Log Appender\\n\\nUnfortunately I had no time today for new chaos experiment, but I spent with @pihme some time to investigate how we can run the cluster plans in our gke.\\nWe did a bit of progress. I\'m finally able to create cluster plans in the ultratest and can extract all resource definitions via command line.\\n\\n```shell\\nkubectl get pvc,configmap,service,deployment,statefulset,cronjob,storageclasses -o yaml --export | sed -e \'/resourceVersion: \\"[0-9]\\\\+\\"/d\' -e \'/uid: [a-z0-9-]\\\\+/d\' -e \'/selfLink: [a-z0-9A-Z/]\\\\+/d\' -e \'/status:/d\' -e \'/phase:/d\' -e \'/creationTimestamp:/d\' > s-cluster.yaml\\n```\\n\\nWe now need to find a way to successfully deploy it in our cluster - it haven\'t been successful yet. We thought about using kustomize to adjust some values they use.\\nMuch easier would be to just deploy the operator they use in our gke cloud and use the CRD to deploy the cluster plans. I think we need to investigate a bit more here what is the best approach. In the end I would like to run our chaos experiments against clusters which correspond to the real deployed ones.\\n\\n\x3c!--truncate--\x3e\\n\\n## Participants\\n\\n * @pihme\\n * @zelldon"},{"id":"/2020/06/25/gateway-network-partition","metadata":{"permalink":"/zeebe-chaos/2020/06/25/gateway-network-partition","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-06-25-gateway-network-partition/index.md","source":"@site/blog/2020-06-25-gateway-network-partition/index.md","title":"Gateway Network Partition","description":"* Documented failure cases for AsyncSnasphortDirector. Gave me some ideas where it might make sense to reinstall partition. Discussed a bit with @Deepthi","date":"2020-06-25T00:00:00.000Z","formattedDate":"June 25, 2020","tags":[],"readingTime":2.34,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway Network Partition","date":"2020-06-25T00:00:00.000Z","categories":["chaos_experiment","gateway"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Extract K8 resources from namespace","permalink":"/zeebe-chaos/2020/07/02/extract-k8-resources"},"nextItem":{"title":"Correlate Message after failover","permalink":"/zeebe-chaos/2020/06/18/correlate-message-after-failover"}},"content":"* Documented failure cases for AsyncSnasphortDirector. Gave me some ideas where it might make sense to reinstall partition. Discussed a bit with @Deepthi\\n * Still our automated chaos experiments are not running. I need some time for that, but I had no time for that today.\\n * Run a chaos experiment together with @pihme, where we do a network partition with the gateway.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment:\\n\\nActually we already have an network partition experiment with the standalone Gateway, where we completely isolate the gateway and take a look whether it comes back after the network partition. Today we wanted to explore how it behaves when only one node and the gateway has a network partition, so Broker 0 and Gateway can\'t talk to each other.\\n\\n### Expected during the experiment:\\n\\nthe topology stays the same, since gateway can ping indirectly (is discussable whether this is ideal or not)\\nwhen Broker 0 is leader for a partition then the processing for that partition stops but other partitions should not be affected\\nWe can somehow determine in the metrics that they can\'t connect to each other\\nAfter connecting again the affected partition should recover\\n\\n### Observations:\\n\\nAs expected we see no difference in the Topology. All commands which are send to that partition time out. Other partitions haven\'t been affected :+1: With the metrics we have we seen that: there is no progress in the partition, the partition is still healthy (which makes sense) and we see a lot of timeouts happening.\\n\\nUnfortunately we need multiple metrics to correlate somehow that it might be due to connectivity issues. I think we can improve here. For example it is not directly visible that one partition stopped processing. For that @Peter Ihme had a good idea and we will add a new panel, which directly shows the current record processing stats. I think this is also useful for exporting to directly see whether we have currently exporting problems. Check the attached image.\\n\\nWhat else is missing on the metrics side from my point of view:\\n\\n * a panel which shows me that all requests to a specific partition currently time out.\\n * metrics for the transport between gateway and broker to better analyze problems like that. Would be nice to have [#4487](https://github.com/zeebe-io/zeebe/issues/4487) \\n  * Liveness and Health stats of the Gateway in the metrics. I think this is currently not supported?\\n\\nAfter reconnecting the nodes we saw that the related partition started to process again. Interesting was that it seems that there piled some traffic up and after reconnecting we saw a burst against partition one (partition 2 was disconnected), but this caused no issues.\\n\\nI think was good and interesting experiment again and gave us a bit more insights what else we need.\\n\\n\\n![feedback](feedback.png)\\n\\n![reduce2](reduce2.png)\\n\\n## Participants\\n\\n * @pihme\\n * @zelldon"},{"id":"/2020/06/18/correlate-message-after-failover","metadata":{"permalink":"/zeebe-chaos/2020/06/18/correlate-message-after-failover","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-06-18-correlate-message-after-failover/index.md","source":"@site/blog/2020-06-18-correlate-message-after-failover/index.md","title":"Correlate Message after failover","description":"* Documented failure cases for engine and stream processor. I think almost all possible failure cases I can think of we already handle, except problems on reading, which I think can\'t be handled.","date":"2020-06-18T00:00:00.000Z","formattedDate":"June 18, 2020","tags":[],"readingTime":0.91,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Correlate Message after failover","date":"2020-06-18T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Gateway Network Partition","permalink":"/zeebe-chaos/2020/06/25/gateway-network-partition"},"nextItem":{"title":"High CPU load on Standalone Gateway","permalink":"/zeebe-chaos/2020/06/11/high-cpu-gateway"}},"content":"* Documented failure cases for engine and stream processor. I think almost all possible failure cases I can think of we already handle, except problems on reading, which I think can\'t be handled.\\n* Checked what the current issue is with the automated chaos experiments. It seems it is a infra problem. You can check the discussion in #infra. It might be affected due to [Infra-1292](https://jira.camunda.com/browse/INFRA-1292)\\n* Run a chaos experiment, where we correlate a message after fail over.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n* Start our normal setup and deploy a workflow with an intermediate message catch event.\\n* Publish a message and kill a random broker.\\n* Create a workflow instance and await the result.\\n\\nI did this experiment several times and it works without any problems, as far as I can tell. First I was wondering that the message was only correlated to one instance, but this seems to be expected [message-correlation.html#message-cardinality](https://docs.zeebe.io/reference/message-correlation/message-correlation.html#message-cardinality) So learned something new today about our messages :grin:.\\n\\nI prepared already an automatable chaos experiment for that. Have to fine tune it a bit.\\n\\nNo pictures today."},{"id":"/2020/06/11/high-cpu-gateway","metadata":{"permalink":"/zeebe-chaos/2020/06/11/high-cpu-gateway","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-06-11-high-cpu-gateway/index.md","source":"@site/blog/2020-06-11-high-cpu-gateway/index.md","title":"High CPU load on Standalone Gateway","description":"* Updated failure cases documentation for exporter based on review","date":"2020-06-11T00:00:00.000Z","formattedDate":"June 11, 2020","tags":[],"readingTime":1.985,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"High CPU load on Standalone Gateway","date":"2020-06-11T00:00:00.000Z","categories":["chaos_experiment","gateway"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Correlate Message after failover","permalink":"/zeebe-chaos/2020/06/18/correlate-message-after-failover"},"nextItem":{"title":"First Chaos Day!","permalink":"/zeebe-chaos/2020/06/04/first-chaos-day"}},"content":"* Updated failure cases documentation for exporter based on review\\n * Documented failure cases for ZeebeDB\\n * Wrote an chaostoolkit experiment based on the last manual Chaos experiment\\n * Run a chaos experiment with @Deepthi, where we put high CPU load on the standalone gateway https://github.com/zeebe-io/zeebe-chaos/issues/28\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment:\\n\\nWe did today an chaos experiment where we used our standard setup with a baseline load of 100 workflow instance and 6 workers, which can activate 120 jobs max.\\nOn our steady state we saw that we are able to start and complete 100 workflow instances in a second. One instance took 1 - 2.5 seconds.\\n\\nWe expected when we introduce stress on the standalone gateway CPU that the latency of the processing goes up and the throughput goes down, but there should be no cluster wide failures happening. We expected that after removing the stress the system should come back to normal and the baseline should be reached again.\\n\\n![/assets/2020-06-11/gw-stress-proc](gw-stress-proc.png)\\n![/assets/2020-06-11/gw-cpu](gw-cpu.png)\\n![/assets/2020-06-11/gw-stress-proc-latency](gw-stress-proc-latency.png)\\n\\nThe results look promising. We have seen no outage.\\nWe tested it twice and saw that the throughput goes down and latency up on stress, but comes back to normal after removing it.\\n\\n### What was unexpected or what we found out:\\n\\nUnexpected was that our Broker back pressure goes up, which means it drops requests during the stress time. This was not expected, since the latency between writing to dispatcher and processing the event should not change. We probably need to investigate this more. Current assumption is that the gateway sends requests in batches and this causes in higher spikes on the back pressure. We need more metrics on the transport module to verify that. There is already an open issue for that https://github.com/zeebe-io/zeebe/issues/4487 We might need to tackle this, before we can find out more here.\\n\\nWe found out that the standalone gateway is not resource limited, which caused that we used at some point 12 CPU cores. It seems there is also an open issue for that on the helm charts https://github.com/zeebe-io/zeebe-cluster-helm/issues/74\\n\\nWe want to improve on our current chaos toolkit test. We want to introduce failures and verify the steady state during the failure is happening, on rollback we should remove the failure again. We currently just verify that we can recover, but not the behavior during a failure, which might be also interesting.\\n\\n## Participants\\n\\n * @deepthidevaki\\n * @zelldon"},{"id":"/2020/06/04/first-chaos-day","metadata":{"permalink":"/zeebe-chaos/2020/06/04/first-chaos-day","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-06-04-first-chaos-day/index.md","source":"@site/blog/2020-06-04-first-chaos-day/index.md","title":"First Chaos Day!","description":"First Chaos day","date":"2020-06-04T00:00:00.000Z","formattedDate":"June 4, 2020","tags":[],"readingTime":1.095,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","imageURL":"https://github.com/zelldon.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"First Chaos Day!","date":"2020-06-04T00:00:00.000Z","categories":["chaos_experiment","broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"High CPU load on Standalone Gateway","permalink":"/zeebe-chaos/2020/06/11/high-cpu-gateway"}},"content":"First Chaos day :tada:\\n\\n * Documented failure cases for exporter (already some exist, it seemed) gave me a new idea for ZEP\\n * Introduced Peter to our Chaos Repository, discussed a bit about the hypothesis backlog, reopened the Chaos Trello board where we will organize ourselves\\n * Run a chaos experiment, where we put high CPU load on the Leader [#6](https://github.com/zeebe-io/zeebe-chaos/issues/6)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment:\\n\\n * We wanted to decrease the blast radius with only one partition, but we found an bug where this seemed not to be possible [#4664](https://github.com/zeebe-io/zeebe/issues/4664)\\n * We run the experiment with 2 partitions and put really high CPU load on the Leader (internally in the pod), we expected that this will not impact the complete cluster. That at most we have a leader change because the leader is not able to send heartbeats in time. After removing the cpu load we should be back on our throughput base line, where we start and complete around 70 - 80 workflow instances per second.\\n * The results where quite promising we had no leader change at all. The leader was able to send heartbeats in time and the backpressure did a good job and drop more requests. After reducing the cpu load we went back to our steady state. \\n\\n![img](result-chaos.png)\\n\\n## Participants\\n\\n * @pihme\\n * @zelldon"}]}')}}]);